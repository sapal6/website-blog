<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Satyabrata pal</title>
<link>https://www.satyabratapal.xyz/blog.html</link>
<atom:link href="https://www.satyabratapal.xyz/blog.xml" rel="self" type="application/rss+xml"/>
<description>This is Satyabrata pal's personal Blog.</description>
<generator>quarto-1.1.251</generator>
<lastBuildDate>Mon, 26 Sep 2022 00:00:00 GMT</lastBuildDate>
<item>
  <title>Chapter 1 - Deblur</title>
  <dc:creator>Satyabrata pal</dc:creator>
  <link>https://www.satyabratapal.xyz/posts/image-restoration-series/chapter1-deblur.html</link>
  <description><![CDATA[ 



<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Execute this notebook on kaggle by clicking <a href="https://www.kaggle.com/code/sapal6/chapter-1-deblur">here</a></p>
</div>
</div>
<p>Photography is one of my hobbies and a good percentage of my photography workflow is spent on correcting various artifacts in images. I always wondered if there is I could efficiently automate the process of image correction/restoration but I didn‚Äôt pay much attention to image restoration using deep learning. So, for the past couple of months I started playing around with different techniques in the image restoration side of deep learning in the hopes of creating a tool that would assist me in image restoration part of my photography workflow and whatever I have learnt till now, I am putting into a series of articles.</p>
<p>This is first in a series of such articles where we will try to build a image correction tool by implementing some cool generative imaging techniques using <a href="https://docs.fast.ai/">Fastai</a> and pytorch. Infact this entire series is inspired by the <a href="https://github.com/sapal6/course-v3/blob/master/nbs/dl1/lesson7-superres.ipynb">fastai course from the year 2018</a>.</p>
<div class="callout-note callout callout-style-simple no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
üìí Side Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Fastai is a deep learning library built on top of pytorch. Fastai provides powerful APIs to create a wide range of deep learning architectures and it‚Äôs layered API makes it easy to implement new architectures as well.</p>
</div>
</div>
<p>At the end of the article I will also post links to different resources from which I got to know a lot about GANs and related techniques.</p>
<p>At the begining of this notebook we will install all the required packages.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="op" style="color: #5E5E5E;">%</span>load_ext autoreload</span>
<span id="cb1-2"><span class="op" style="color: #5E5E5E;">%</span>autoreload <span class="dv" style="color: #AD0000;">2</span></span></code></pre></div>
</details>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="cf" style="color: #003B4F;">try</span>: <span class="im" style="color: #00769E;">import</span> fastkaggle</span>
<span id="cb2-2"><span class="cf" style="color: #003B4F;">except</span> ModuleNotFoundError:</span>
<span id="cb2-3">    <span class="op" style="color: #5E5E5E;">!</span>pip install <span class="op" style="color: #5E5E5E;">-</span>Uq fastkaggle</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv</code></pre>
</div>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="cf" style="color: #003B4F;">try</span>: <span class="im" style="color: #00769E;">import</span> fastai</span>
<span id="cb4-2"><span class="cf" style="color: #003B4F;">except</span> ModuleNotFoundError:</span>
<span id="cb4-3">    <span class="op" style="color: #5E5E5E;">!</span>pip install <span class="op" style="color: #5E5E5E;">-</span>Uq fastai</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv</code></pre>
</div>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="co" style="color: #5E5E5E;"># install fastkaggle if not available</span></span>
<span id="cb6-2"><span class="cf" style="color: #003B4F;">try</span>: <span class="im" style="color: #00769E;">import</span> fastaibreadcrumbs</span>
<span id="cb6-3"><span class="cf" style="color: #003B4F;">except</span> ModuleNotFoundError:</span>
<span id="cb6-4">    <span class="op" style="color: #5E5E5E;">!</span>pip install <span class="op" style="color: #5E5E5E;">-</span>Uq fastaibreadcrumbs</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv</code></pre>
</div>
</div>
<p>Then we will import the required modules.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="im" style="color: #00769E;">import</span> gc</span>
<span id="cb8-2"><span class="im" style="color: #00769E;">from</span> fastai.vision.<span class="bu" style="color: null;">all</span> <span class="im" style="color: #00769E;">import</span> <span class="op" style="color: #5E5E5E;">*</span></span>
<span id="cb8-3"><span class="im" style="color: #00769E;">from</span> fastai.vision.gan <span class="im" style="color: #00769E;">import</span> <span class="op" style="color: #5E5E5E;">*</span></span>
<span id="cb8-4"><span class="im" style="color: #00769E;">from</span> fastkaggle <span class="im" style="color: #00769E;">import</span> <span class="op" style="color: #5E5E5E;">*</span></span>
<span id="cb8-5"><span class="im" style="color: #00769E;">from</span> fastaibreadcrumbs.core <span class="im" style="color: #00769E;">import</span> <span class="op" style="color: #5E5E5E;">*</span></span></code></pre></div>
</details>
</div>
<section id="image-restoration" class="level3">
<h3 class="anchored" data-anchor-id="image-restoration">Image restoration</h3>
<p>Simply put Image restoration is the task of correcting imperfections in images. For example, you have taken some photographs of your pet and the photo, although it‚Äôs good but suffers from camera shake. Wouldn‚Äôt it be nice if you could remove some of the camera shake from the image so that you don‚Äôt have to discard the image altogether?</p>
<p>In this article we would try to achieve just that using deep learning.</p>
</section>
<section id="pre-requisites" class="level3">
<h3 class="anchored" data-anchor-id="pre-requisites">Pre-requisites</h3>
<p>This article and the other upcoming articles in this series needs some basic understanding of deep learning. I would recommend you to go through the <a href="https://course.fast.ai/">‚ÄúPractical Deep Learning‚Äù</a> course to get an understanding about deep learning.</p>
</section>
<section id="getting-the-data" class="level3">
<h3 class="anchored" data-anchor-id="getting-the-data">Getting the data</h3>
<p>I would need training data for my task but it so happens that my task is an unique one and I would need to collect my own data. One way collect data is to create them.</p>
<p>To create data I would need to collect some images first and then artificially introduce shakes and blurs into these images and to do this we will write some code to ‚Äúcrappify‚Äù data.</p>
<blockquote class="blockquote">
<p>I heard this term ‚Äúcrappify‚Äù and this technique of creating ‚Äúbad‚Äù data in <a href="https://youtu.be/9spwoDYwW_I">lesson-7 of Fastai‚Äôs 2019 course</a>.</p>
</blockquote>
<p>I have created a <a href="https://www.kaggle.com/datasets/sapal6/superresolution">dataset</a> of high resolution images that I collected from the free stock photography website <a href="https://www.pexels.com/">pexels.com</a>. During the training cycle of my model I combined my data with another <a href="https://www.kaggle.com/datasets/thaihoa1476050/df2k-ost">dataset</a> which also had some more high resolution images.</p>
</section>
<section id="downloading-the-data" class="level2">
<h2 class="anchored" data-anchor-id="downloading-the-data">Downloading the data</h2>
<p>I have created a tiny library <a href="https://sapal6.github.io/fastaibreadcrumbs/">‚Äúfastaibreadcrumbs‚Äù</a>. It provides some convinience functions that extends a few of the functionalities available in the <a href="https://fastai.github.io/fastkaggle/">‚Äúfastkaggle‚Äù</a> library.</p>
<p>‚Äúfastaibreadcrumbs‚Äù provides a <code>setup_data</code> using which you can pull any kaggle non-competition dataset onto your machine. Just provide the username from the kaggle dataset page and the dataset name.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>If you are trying to pull a competition dataset then fastkaggle has an equivalent function <code>setup_comp</code>.</p>
</div>
</div>
<p>We will pull the first dataset (the one I collected from pexels.com)</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">path1 <span class="op" style="color: #5E5E5E;">=</span> setup_data(<span class="st" style="color: #20794D;">'sapal6'</span>, <span class="st" style="color: #20794D;">'superresolution'</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'</code></pre>
</div>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">path1.ls()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<pre><code>(#11) [Path('superresolution/trains'),Path('superresolution/nature'),Path('superresolution/kids-playing'),Path('superresolution/fireworks'),Path('superresolution/busystreet'),Path('superresolution/dogs-running'),Path('superresolution/sports'),Path('superresolution/underwater'),Path('superresolution/dance'),Path('superresolution/wildlife')...]</code></pre>
</div>
</div>
<p>Then we will pull the other dataset from kaggle which I will combine with my dataset.</p>
<blockquote class="blockquote">
<p>Sometimes more data helps. While experimenting for this tutorial, I found that the amount of data that I had in my dataset was not enough and adding a few more samples expanded the variation in the input. That‚Äôs why I augmented my dataset with some external data (which in this case was another dataset from kaggle).</p>
</blockquote>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>If your model‚Äôs results are not at par then you should first try to bring in more training data (if it‚Äôs possible because in many domains more data is simply hard to come by). If addign more data doesn‚Äôt improve your model then you should look to other techniques.</p>
</div>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">path2 <span class="op" style="color: #5E5E5E;">=</span> setup_data(<span class="st" style="color: #20794D;">'thaihoa1476050'</span>, <span class="st" style="color: #20794D;">'df2k-ost'</span>)</span></code></pre></div>
</details>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">path2.ls()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<pre><code>(#2) [Path('df2k-ost/train'),Path('df2k-ost/test')]</code></pre>
</div>
</div>
<p>The next thing that I am going to do is to create a config sort of thing that would help me to store frequently used values.</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>In real world a better way is to create a config file in your project.</p>
</div>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">config <span class="op" style="color: #5E5E5E;">=</span> {<span class="st" style="color: #20794D;">'root'</span>:Path(<span class="st" style="color: #20794D;">"."</span>)}</span></code></pre></div>
</details>
</div>
</section>
<section id="crappifying" class="level2">
<h2 class="anchored" data-anchor-id="crappifying">Crappifying</h2>
<p>In order to get the target images which in our case are the images having ‚Äúcamera shake‚Äù in them, we need to simulate motion blur. ‚Äúfastaibreadcrumbs‚Äù provides <code>Crappifier</code> which takes in images and creates another version of the same image that has motion blur in it.</p>
<p>First let‚Äôs get the source and destination paths.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1">config[<span class="st" style="color: #20794D;">'path_hr1'</span>] <span class="op" style="color: #5E5E5E;">=</span> path1</span>
<span id="cb17-2">config[<span class="st" style="color: #20794D;">'path_hr2'</span>] <span class="op" style="color: #5E5E5E;">=</span> path2</span></code></pre></div>
</details>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1">config[<span class="st" style="color: #20794D;">'path_crappy'</span>] <span class="op" style="color: #5E5E5E;">=</span> Path(<span class="st" style="color: #20794D;">'crappy'</span>)</span></code></pre></div>
</details>
</div>
<p>fastai provides <code>get_image_files</code> which fetches all teh image files from a given path. We will use this get the images from our first dataset.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1">files_hr1 <span class="op" style="color: #5E5E5E;">=</span> get_image_files(config[<span class="st" style="color: #20794D;">'path_hr1'</span>])</span></code></pre></div>
</details>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1">files_hr1[:<span class="dv" style="color: #AD0000;">10</span>]</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<pre><code>(#10) [Path('superresolution/trains/0055.jpg'),Path('superresolution/trains/0002.jpg'),Path('superresolution/trains/0067.jpg'),Path('superresolution/trains/0022.jpg'),Path('superresolution/trains/0012.jpg'),Path('superresolution/trains/0019.jpg'),Path('superresolution/trains/0009.jpg'),Path('superresolution/trains/0051.jpg'),Path('superresolution/trains/0047.jpg'),Path('superresolution/trains/0072.jpg')]</code></pre>
</div>
</div>
<p>similarly we will get the images from teh second dataset path.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1">files_hr2 <span class="op" style="color: #5E5E5E;">=</span> get_image_files(config[<span class="st" style="color: #20794D;">'path_hr2'</span>])</span></code></pre></div>
</details>
</div>
<p>Let‚Äôs crappify a single image to check if things work.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1">Crappifier(config[<span class="st" style="color: #20794D;">"path_crappy"</span>])(files_hr1[<span class="dv" style="color: #AD0000;">0</span>])</span></code></pre></div>
</details>
</div>
<p>Let‚Äôs grab our crappified image.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1">files_crappy1 <span class="op" style="color: #5E5E5E;">=</span> get_image_files(config[<span class="st" style="color: #20794D;">'path_crappy'</span>])</span></code></pre></div>
</details>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1">files_crappy1[<span class="dv" style="color: #AD0000;">0</span>]</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<pre><code>Path('crappy/path_860/trains/0055.jpg')</code></pre>
</div>
</div>
<p>‚Äúfastaibreadcrumbs‚Äù provides the convinience function <code>show_plot</code> which let‚Äôs you display two images side by side.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>If you look into the source code of <code>show_plot</code> it‚Äôs only a few lines of code. Under the hood it‚Äôs only matplotlib functions. So, if you don‚Äôt want to use <code>show_plot</code> then you can create your own plotting function.</p>
</div>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1">show_plot(files_hr1[<span class="dv" style="color: #AD0000;">0</span>], files_crappy1[<span class="dv" style="color: #AD0000;">0</span>], <span class="dv" style="color: #AD0000;">1</span>, (<span class="dv" style="color: #AD0000;">10</span>, <span class="dv" style="color: #AD0000;">10</span>))</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/image-restoration-series/chapter1-deblur_files/figure-html/cell-20-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb28" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><span class="co" style="color: #5E5E5E;">#cleaning up the crappy directory.</span></span>
<span id="cb28-2">shutil.rmtree(config[<span class="st" style="color: #20794D;">'path_crappy'</span>])</span></code></pre></div>
</details>
</div>
<p>While feeding data into a neural network, all teh images needs to be in same size. So, let‚Äôs first check the sizes of our training images.</p>
<p>There is a neat trick which I learned from <a href="https://www.kaggle.com/code/jhoward/first-steps-road-to-the-top-part-1">Jeremy Howard‚Äôs notebook</a> to get the image size. I am using the same code here.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><span class="im" style="color: #00769E;">from</span> fastcore.parallel <span class="im" style="color: #00769E;">import</span> <span class="op" style="color: #5E5E5E;">*</span></span></code></pre></div>
</details>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb30" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><span class="kw" style="color: #003B4F;">def</span> f(o): <span class="cf" style="color: #003B4F;">return</span> PILImage.create(o).size</span>
<span id="cb30-2">sizes <span class="op" style="color: #5E5E5E;">=</span> parallel(f, files_hr1<span class="op" style="color: #5E5E5E;">+</span>files_hr2, n_workers<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">4</span>)</span>
<span id="cb30-3">pd.Series(sizes).value_counts()</span></code></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.9/dist-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>(1880, 1253)    4147
(867, 1300)     2608
(2040, 1356)    1435
(496, 368)       637
(1733, 1300)     623
                ... 
(464, 416)         1
(1803, 1300)       1
(1756, 1300)       1
(1235, 1300)       1
(2040, 1740)       1
Length: 2790, dtype: int64</code></pre>
</div>
</div>
<p>Now, there are a variety of image sizes and orientations but most of the images have there sizes in the range of a minimum of 800ish(shortest side) and 1300is(longest side). So, I will randomly pick a proportion which allows me to resize an image having teh longest side as 860. I could also have picked up a higher size but then the compute required would have been higher.</p>
<p>Fastai can resize images while loading data but all those computation would take time and the training speed might get affected on low power devices (for example my laptop which has a humble gpu). We may end up resizing the images to still lower sizes during the creation of dataloader but an initial resizing really helps to save some computing during the training time.</p>
<blockquote class="blockquote">
<p>These resizing tricks to save computing and speeding up training is what I got to know from <a href="https://www.kaggle.com/code/jhoward/first-steps-road-to-the-top-part-1">Jeremy Howard‚Äôs notebook</a> and from my own experiments that I did on my local machine and my experiments done on kaggle kernel. Your experience might vary depending upon the computing power that you have access to. So, again feel free to experiment.</p>
</blockquote>
<p>I will create a config for the path where I am goign to store my resized images.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1">config[<span class="st" style="color: #20794D;">'path_860'</span>] <span class="op" style="color: #5E5E5E;">=</span> Path(<span class="st" style="color: #20794D;">'path_860'</span>)</span></code></pre></div>
</details>
</div>
<p>fastai provides <code>resize_images</code> which resizes your source images to the desired size. The below function <code>resz_imgs</code> is just a wrapper on top of fastai‚Äôs <code>resize_images</code>. Pass on the source and destination paths and pass the maximum size that you want your images to be resized to. <code>resize_images</code> will then resize the images to the new proportions while keeping the longest side to the given <code>max_sz</code> and accordingly modify the shortest side.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb34" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><span class="kw" style="color: #003B4F;">def</span> resz_imgs(source: Path, dest: Path, max_sz: <span class="bu" style="color: null;">int</span>):</span>
<span id="cb34-2">    resize_images(source, dest<span class="op" style="color: #5E5E5E;">=</span>dest, max_size<span class="op" style="color: #5E5E5E;">=</span>max_sz, recurse<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span></code></pre></div>
</details>
</div>
<p>Let‚Äôs resize the first dataset.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb35" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><span class="co" style="color: #5E5E5E;">#for first set</span></span>
<span id="cb35-2">resz_imgs(config[<span class="st" style="color: #20794D;">'path_hr1'</span>], config[<span class="st" style="color: #20794D;">'path_860'</span>], max_sz<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">860</span>)</span></code></pre></div>
</details>
</div>
<p>Then the second dataset.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb36" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><span class="co" style="color: #5E5E5E;">#for second set</span></span>
<span id="cb36-2">resz_imgs(config[<span class="st" style="color: #20794D;">'path_hr2'</span>], config[<span class="st" style="color: #20794D;">'path_860'</span>], max_sz<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">860</span>)</span></code></pre></div>
</details>
</div>
<p>Next, we are going to crappify our resized images and we are going to do that in parallel. Do do things in parallel we will use <code>parallel</code> provided by <a href="https://fastcore.fast.ai/">fastcore</a>. It‚Äôs similar to python standard library‚Äôs parallel function but much more convenient.</p>
<div class="callout-note callout callout-style-simple no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
üìí Side Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>fastcore is a underrated python library which supercharges python.</p>
</div>
</div>
<p>For parallel crappification, I will use <code>crappify_imgs</code> provided by ‚Äúfastaibreadcrumbs‚Äù which let‚Äôs you provide the high resolution image path and the destination path, strength of the blur (as <code>sz</code>).</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb37" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1">crappify_imgs(config[<span class="st" style="color: #20794D;">'path_860'</span>], config[<span class="st" style="color: #20794D;">'path_crappy'</span>], n_workers<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">8</span>)</span></code></pre></div>
</details>
</div>
</section>
<section id="grabbing-the-data" class="level2">
<h2 class="anchored" data-anchor-id="grabbing-the-data">Grabbing the data</h2>
<p>Now that crappification is done, let‚Äôs grab our input and target data. We can do this by using fastai <code>Datablock</code> API. <code>Datablock</code> let‚Äôs you customize the way you want to grab your input and output.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb38" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1">files_crappy <span class="op" style="color: #5E5E5E;">=</span> get_image_files(config[<span class="st" style="color: #20794D;">'path_crappy'</span>])</span>
<span id="cb38-2">files_crappy[:<span class="dv" style="color: #AD0000;">4</span>]</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<pre><code>(#4) [Path('crappy/path_860/trains/0055.jpg'),Path('crappy/path_860/trains/0002.jpg'),Path('crappy/path_860/trains/0067.jpg'),Path('crappy/path_860/trains/0022.jpg')]</code></pre>
</div>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb40" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1">files_crappy[<span class="dv" style="color: #AD0000;">0</span>].relative_to(config[<span class="st" style="color: #20794D;">'path_crappy'</span>])</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<pre><code>Path('path_860/trains/0055.jpg')</code></pre>
</div>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb42" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1">dblock <span class="op" style="color: #5E5E5E;">=</span> DataBlock(blocks<span class="op" style="color: #5E5E5E;">=</span>(ImageBlock, ImageBlock),</span>
<span id="cb42-2">                       get_y <span class="op" style="color: #5E5E5E;">=</span> <span class="kw" style="color: #003B4F;">lambda</span> x: x.relative_to(config[<span class="st" style="color: #20794D;">'path_crappy'</span>]),</span>
<span id="cb42-3">                       splitter<span class="op" style="color: #5E5E5E;">=</span>RandomSplitter(),</span>
<span id="cb42-4">                       item_tfms<span class="op" style="color: #5E5E5E;">=</span>Resize(<span class="dv" style="color: #AD0000;">480</span>),</span>
<span id="cb42-5">                       batch_tfms<span class="op" style="color: #5E5E5E;">=</span>[<span class="op" style="color: #5E5E5E;">*</span>aug_transforms(max_zoom<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">2.</span>), Normalize.from_stats(<span class="op" style="color: #5E5E5E;">*</span>imagenet_stats)])</span></code></pre></div>
</details>
</div>
<p>In the first line we describe outr <code>blocks</code> which are the x and y. Here, our x and y both are images and thus we provide <code>Imageblock</code> as the blocks of choice. Then we use lambda function to tell fastai that we are expecting the crappy images as our y, whose filename match with the filename of the input images. After defining our labels (y) we tell fastai to randomly split our images into training and test data. Then we call the <code>Resize</code> since we want all our images to be of same size. Lastly we do some augmentations.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb43" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1">dls <span class="op" style="color: #5E5E5E;">=</span> dblock.dataloaders(files_crappy, bs<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">4</span>)</span></code></pre></div>
</details>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb44" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1">dls.show_batch()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/image-restoration-series/chapter1-deblur_files/figure-html/cell-33-output-1.png" class="img-fluid"></p>
</div>
</div>
<section id="feature-loss" class="level3">
<h3 class="anchored" data-anchor-id="feature-loss">Feature loss</h3>
<p>In the paper <a href="https://arxiv.org/abs/1603.08155">‚ÄúPerceptual Losses for Real-Time Style Transfer and Super-Resolution</a> the authors proposed a kind of loss known as perceptual loss which would compare different feature maps of the target image with the feature map of the generated image to see if teh generated image‚Äôs features are the same as the actual image. This can otherwise be known as ‚Äúfeature loss‚Äù as described in the <a href="https://youtu.be/9spwoDYwW_I">Fastai course 2019 lesson 7</a>.</p>
<p>The below doodle gives a very simple overview of the method used by the authors of the <a href="https://arxiv.org/abs/1603.08155">‚ÄúPerceptual Losses for Real-Time Style Transfer and Super-Resolution</a> paper</p>
<p align="center">
<img src="https://www.satyabratapal.xyz/posts/image-restoration-series/featureloss.png">
</p>
<p>In the proposed method, the hourglass shaped figure is a pre-cursor to ‚ÄúUnet‚Äù which is made up of a encoder for learning the features of an input image and a decoder which would reconstruct the image while upscaling the image.</p>
<blockquote class="blockquote">
<p>This is similar to super-resolution where an U-net is used to scale up the resolution of an image. Such a network can also be used for tasks like improving the quality of a crappy image.</p>
</blockquote>
<p>The colorful rectangle in the above figure is a pre-trained image model like vgg-16 which feeds on teh image generated by the U-net. While the generated image passes through the pre-trained image model, the feature map of the generated images are grabbed from the intermidiate layers of the vgg-16 model. The target image (actual data) is alos passed through this vgg-16 model it‚Äôs feature map is also extracted from teh intermidiate layers. After this is done, the feature map of the generated image is compared with the feature map of the target image. This is done by the feature loss function which is then used to re-train the U-net in order to make the generated model appear as close as possible to the actual thing.</p>
</section>
</section>
<section id="extracting-features" class="level2">
<h2 class="anchored" data-anchor-id="extracting-features">Extracting features</h2>
<p>If recall the diagram from previous section, you will notice that we need to extract the features from our input image as well as the target image which we can compare later. To extract the features, we will use a simple pre-trained network like ‚Äúvgg16‚Äù.</p>
<p>This below is a representation of the pre-trained network which we would be using to extract the features from our images. The sections in this image represents different layers of the network and the downward arrows represent the extraction of the features.</p>
<p align="center">
<img src="https://www.satyabratapal.xyz/posts/image-restoration-series/featuremap.png">
</p>
<p>The features are grabbed just before the grid size changes and the maxpooling layer in network is where the grid size change occurs.</p>
</section>
<section id="feature-loss-1" class="level2">
<h2 class="anchored" data-anchor-id="feature-loss-1">Feature Loss</h2>
<p>Taking all the previous things into account, we will use a pre-trained vgg16 to create a loss function that will help the network to compare the pixels of the target and the input image and check if the two images are the same.</p>
<p>‚Äúfastaibreadcrumbs‚Äù provides <code>calc_ft_loss</code> which does the heavy lifting of initiating a vgg16 pre-trained model and calculating the feature loss for you.</p>
<blockquote class="blockquote">
<p>If you wan to know more about what happens under the hood then I feel free to read through the <a href="https://github.com/sapal6/course-v3/blob/master/nbs/dl1/lesson7-superres.ipynb">fastai course(2018) notebook here</a> and watch the lecture <a href="https://youtu.be/9spwoDYwW_I">here</a></p>
</blockquote>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb45" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1">feat_loss <span class="op" style="color: #5E5E5E;">=</span> calc_ft_loss()</span></code></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and will be removed in 0.15. Please use keyword parameter(s) instead.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_BN_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Downloading: "https://download.pytorch.org/models/vgg16_bn-6c64b313.pth" to /root/.cache/torch/hub/checkpoints/vgg16_bn-6c64b313.pth</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"a724f833672d49efa642ae3c45abd106","version_major":2,"version_minor":0}
</script>
</div>
</div>
</section>
<section id="grabbing-data" class="level2">
<h2 class="anchored" data-anchor-id="grabbing-data">Grabbing data</h2>
<p>Like before we have to create a dataloader which would load adata onto the device. We can use fastai‚Äôs dataloader for this purpose but for convenience I will use the <code>get_unet_dls</code> from ‚Äúfastaibreadcrumbs‚Äù which is simply a wrapper over a fastai datablock with two Imageblocks (one Imageblock for input and one Imageblock for target) and a dataloader.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb47" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1">config</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<pre><code>{'root': Path('.'),
 'path_hr1': Path('superresolution'),
 'path_hr2': Path('df2k-ost'),
 'path_crappy': Path('crappy'),
 'path_860': Path('path_860')}</code></pre>
</div>
</div>
<p>‚Ä¶setting some parameters like weight decay, nymber of epochs etc‚Ä¶</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb49" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><span class="co" style="color: #5E5E5E;"># hyperparameters</span></span>
<span id="cb49-2">config[<span class="st" style="color: #20794D;">'epoch'</span>] <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">4</span></span>
<span id="cb49-3">config[<span class="st" style="color: #20794D;">'wd'</span>] <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">1e-3</span></span></code></pre></div>
</details>
</div>
<p>‚Ä¶then we will set our item transformations. These are the transformations which will be applied to each item when they are loaded. Here, we will reduce the size of images to a smaller size. We will use <code>Resize</code> for this. Then we will setup our ‚Äúbatch transformations‚Äù, these are the transformations which will be applied to the images once they are grouped into batches. In our case we will apply image augmentations to our batches which will do things like rotate the images, brighten up the images, darken the images, flip the images etc. After that we will setup the method by the way of which we want to grab our target images. Here, we will use a path relative to our crappy images because the directory structure and the image name of the crappy images matches with that of our input images.</p>
<blockquote class="blockquote">
<p>Training on a smaller image size will make our training faster and this will enabel use to iterate faster in case we want to try a lots of things to find the best settings fro our training. There is one more reason due to which we may want to start with a smaller image size. We are going to use something known as ‚Äúprogressive resizing‚Äù , whereby we start training with small images and then use the model trained on these smaller images as a pre-trained model for another training setup where we use slightly bigger images. This makes the trainign a lot faster then starting with big images from the get go. Remember that our goal is to make our training as fast as possible.</p>
</blockquote>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb50" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><span class="co" style="color: #5E5E5E;">#transformations</span></span>
<span id="cb50-2">item_tfms <span class="op" style="color: #5E5E5E;">=</span> Resize(<span class="dv" style="color: #AD0000;">128</span>)</span>
<span id="cb50-3">batch_tfms <span class="op" style="color: #5E5E5E;">=</span> [<span class="op" style="color: #5E5E5E;">*</span>aug_transforms(max_zoom<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">2.</span>), Normalize.from_stats(<span class="op" style="color: #5E5E5E;">*</span>imagenet_stats)]</span>
<span id="cb50-4">get_y <span class="op" style="color: #5E5E5E;">=</span> <span class="kw" style="color: #003B4F;">lambda</span> x: x.relative_to(config[<span class="st" style="color: #20794D;">'path_crappy'</span>])</span></code></pre></div>
</details>
</div>
<p>After all these are done, we will pass the following to the <code>get_unet_dls</code> function ‚Äì&gt;</p>
<ul>
<li>batch size</li>
<li>source i.e.&nbsp;the crappy file names</li>
<li><code>get_y</code> i.e.&nbsp;the way to fetch our targets</li>
<li><code>splitter</code> i.e.&nbsp;how to split our data into trainign and validation sets.</li>
<li>item_tfms i.e.&nbsp;item transformations.</li>
<li><code>batch_tfms</code> i.e.&nbsp;batch transformations.</li>
</ul>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb51" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1">dls<span class="op" style="color: #5E5E5E;">=</span> get_unet_dls(<span class="dv" style="color: #AD0000;">8</span>, source <span class="op" style="color: #5E5E5E;">=</span> files_crappy, get_y <span class="op" style="color: #5E5E5E;">=</span> get_y, </span>
<span id="cb51-2">                     splitter <span class="op" style="color: #5E5E5E;">=</span> RandomSplitter(), item_tfms <span class="op" style="color: #5E5E5E;">=</span> item_tfms,</span>
<span id="cb51-3">                     batch_tfms <span class="op" style="color: #5E5E5E;">=</span> batch_tfms)</span></code></pre></div>
</details>
</div>
<p>let‚Äôs see our samples.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb52" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1">dls.show_batch()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/image-restoration-series/chapter1-deblur_files/figure-html/cell-39-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="unet" class="level2">
<h2 class="anchored" data-anchor-id="unet">Unet</h2>
<p>Now that we have our feature loss and data ready, let‚Äôs create a unet.</p>
<p>Creating a basic unet is a one liner affair in fastai. Ofcourse, you can do all sorts of customization to your unet but I will use the minimalist way here and take advantage of the high level API in fastai.</p>
<p>We will use the unet_learner parameters recommended <a href="https://github.com/sapal6/course-v3/blob/master/nbs/dl1/lesson7-superres.ipynb">here</a>.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb53" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1">unet_learn <span class="op" style="color: #5E5E5E;">=</span> unet_learner(dls, models.resnet34, loss_func<span class="op" style="color: #5E5E5E;">=</span>feat_loss,</span>
<span id="cb53-2">                          metrics<span class="op" style="color: #5E5E5E;">=</span>LossMetrics(feat_loss.metric_names),</span>
<span id="cb53-3">                          blur<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, norm_type<span class="op" style="color: #5E5E5E;">=</span>NormType.Weight)</span>
<span id="cb53-4"></span>
<span id="cb53-5">gc.collect()<span class="op" style="color: #5E5E5E;">;</span></span></code></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Downloading: "https://download.pytorch.org/models/resnet34-b627a593.pth" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"f5fef45a64424fe3b60696acc3c67fc3","version_major":2,"version_minor":0}
</script>
</div>
</div>
<p>Notice that I have passed something like <code>LossMetrics(feat_loss.metric_names)</code> to the unet. This is our metric which will tell us how our unet is progressing. Where does this comes from?</p>
<p>Well if you look into the source code of the <code>Feature_loss</code>-</p>
<pre><code>class FeatureLoss(Module):
    """Class to calculate feature loss"""
    def __init__(self, m_feat, layer_ids, layer_wgts):
        self.m_feat = m_feat
        self.loss_features = [self.m_feat[i] for i in layer_ids]
        self.hooks = hook_outputs(self.loss_features, detach=False)
        self.wgts = layer_wgts
        self.metric_names = ['pixel',] + [f'feat_{i}' for i in range(len(layer_ids))
              ] + [f'gram_{i}' for i in range(len(layer_ids))]

    def make_features(self, x, clone=False):
        self.m_feat(x)
        return [(o.clone() if clone else o) for o in self.hooks.stored]
    
    def forward(self, input, target, reduction='mean'):
        out_feat = self.make_features(target, clone=True)
        in_feat = self.make_features(input)
        self.feat_losses = [pixel_loss(input,target,reduction=reduction)]
        self.feat_losses += [pixel_loss(f_in, f_out,reduction=reduction)*w
                             for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]
        self.feat_losses += [pixel_loss(gram_matrix(f_in), gram_matrix(f_out),reduction=reduction)*w**2 * 5e3
                             for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]
        if reduction=='none': 
            self.feat_losses = [f.mean(dim=[1,2,3]) for f in self.feat_losses[:4]] + [f.mean(dim=[1,2]) for f in self.feat_losses[4:]]
        for n,l in zip(self.metric_names, self.feat_losses): setattr(self, n, l)
        return sum(self.feat_losses)
    
    def __del__(self): self.hooks.remove()</code></pre>
<p>‚Ä¶notice that we have something called <code>metric_names</code> there. This thing is the collection of the pixel loss and gram loss which can then be used as a metric by us.</p>
<p>fastai provides the <code>lr_find</code> function which we will use to find a suitable learning rate.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb56" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1">lr <span class="op" style="color: #5E5E5E;">=</span> unet_learn.lr_find(suggest_funcs<span class="op" style="color: #5E5E5E;">=</span>(valley, slide))</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/image-restoration-series/chapter1-deblur_files/figure-html/cell-41-output-3.png" class="img-fluid"></p>
</div>
</div>
<p><code>lr_find</code> has a few different methods of finding the learning rate but the ‚Äúvalley‚Äù and ‚Äúslide‚Äù functions provide suitable learning rate most of the time.</p>
<p>The <code>lr_find</code> suggests very optimistic values of learning rates so that we don‚Äôt screw up our training but another thing that we can do is to look at the learning rate suggested by the ‚Äúvalley‚Äù and ‚Äúslide‚Äù functions and then take a value in between these two.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb57" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1">(lr.valley, lr.slide)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<pre><code>(0.0003311311302240938, 0.0006918309954926372)</code></pre>
</div>
</div>
<p>We will train the unet using the ‚Äúone cycle policy‚Äù. From 50,000 feet, the one cycle policy can be explained like this ‚ÄúA one cycle policy trains the model with large and oscillating (changing between different values) learning rate in order to make the training faster and more accurate‚Äù. A more detailed explanation can be found in this cool <a href="https://iconof.com/1cycle-learning-rate-policy/">article</a>.</p>
<p>There is one more trick that I am going to use in my unet_learner. I have experimented with quite a different versions of this notebook with variety of data (variations both in nterms of number of images and amount of data) and I have found that with data variations it becomes difficult for me to predict that a particular choice of ‚Äúepoch numbers‚Äù will work everytime. So, I will need to tell fastai that when we need to stop training if my loss is not improving and save my modell whenever a ‚Äúgood‚Äù enough loss state is reached. This can be done by the <a href="https://docs.fast.ai/callback.tracker.html"><code>EarlyStoppingCallback</code></a> and <a href="https://docs.fast.ai/callback.tracker.html#savemodelcallback"><code>SaveModelCallback</code></a>. The former stops the model trainign if the loss doesn‚Äôt improve further and the latter will save the best possible model. I have provided links to the official documentation of these two functions if you wan tot know more about them.</p>
<p>But hey! what‚Äôs a callback? Simply put a callback is a function that is passed as an argument to another function. It provides a way for the programmer to modify an existing piece of code without changing the structure of code. With callbacks you can modify the behaviour of your piece of code, say a function without changing the body of your current function. <a href="https://pythonexamples.org/python-callback-function/">Here</a> is an article that explains callbacks with examples.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb59" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1">unet_learn.fit_one_cycle(config[<span class="st" style="color: #20794D;">'epoch'</span>], (lr.valley<span class="op" style="color: #5E5E5E;">+</span>lr.slide)<span class="op" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">2</span>, pct_start<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.9</span>, wd<span class="op" style="color: #5E5E5E;">=</span>config[<span class="st" style="color: #20794D;">'wd'</span>], </span>
<span id="cb59-2">                         cbs<span class="op" style="color: #5E5E5E;">=</span>[EarlyStoppingCallback(patience<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>), SaveModelCallback(fname<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'model_128'</span>)])</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>pixel</th>
      <th>feat_0</th>
      <th>feat_1</th>
      <th>feat_2</th>
      <th>gram_0</th>
      <th>gram_1</th>
      <th>gram_2</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>2.209806</td>
      <td>2.456991</td>
      <td>0.185003</td>
      <td>0.207987</td>
      <td>0.262833</td>
      <td>0.114802</td>
      <td>0.530690</td>
      <td>0.872372</td>
      <td>0.283306</td>
      <td>06:00</td>
    </tr>
    <tr>
      <td>1</td>
      <td>2.042598</td>
      <td>2.254473</td>
      <td>0.171976</td>
      <td>0.196098</td>
      <td>0.245231</td>
      <td>0.106519</td>
      <td>0.463559</td>
      <td>0.806346</td>
      <td>0.264745</td>
      <td>06:03</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.886767</td>
      <td>2.064314</td>
      <td>0.150376</td>
      <td>0.181493</td>
      <td>0.225683</td>
      <td>0.098219</td>
      <td>0.433128</td>
      <td>0.729233</td>
      <td>0.246182</td>
      <td>06:01</td>
    </tr>
    <tr>
      <td>3</td>
      <td>1.694850</td>
      <td>1.856245</td>
      <td>0.135413</td>
      <td>0.170341</td>
      <td>0.208545</td>
      <td>0.090624</td>
      <td>0.365436</td>
      <td>0.657052</td>
      <td>0.228834</td>
      <td>06:04</td>
    </tr>
  </tbody>
</table>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Better model found at epoch 0 with valid_loss value: 2.45699143409729.
Better model found at epoch 1 with valid_loss value: 2.2544734477996826.
Better model found at epoch 2 with valid_loss value: 2.0643138885498047.
Better model found at epoch 3 with valid_loss value: 1.8562445640563965.</code></pre>
</div>
</div>
<p>we can view the predictions on the validation set using <code>show_results</code></p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb61" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1">unet_learn.show_results()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/image-restoration-series/chapter1-deblur_files/figure-html/cell-44-output-3.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="progressive-resizing" class="level2">
<h2 class="anchored" data-anchor-id="progressive-resizing">Progressive resizing</h2>
<p>Even with an image size of 128px our model is doing a good job of removing the motion blur and generating the blur free images. Now, let‚Äôs use this model as a pre-trained model fro our next training phase where we will increase the image size to 256px.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb62" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><span class="co" style="color: #5E5E5E;">#transformations</span></span>
<span id="cb62-2">item_tfms <span class="op" style="color: #5E5E5E;">=</span> Resize(<span class="dv" style="color: #AD0000;">256</span>)</span>
<span id="cb62-3">batch_tfms <span class="op" style="color: #5E5E5E;">=</span> [<span class="op" style="color: #5E5E5E;">*</span>aug_transforms(max_zoom<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">2.</span>), Normalize.from_stats(<span class="op" style="color: #5E5E5E;">*</span>imagenet_stats)]</span>
<span id="cb62-4">get_y <span class="op" style="color: #5E5E5E;">=</span> <span class="kw" style="color: #003B4F;">lambda</span> x: x.relative_to(config[<span class="st" style="color: #20794D;">'path_crappy'</span>])</span></code></pre></div>
</details>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb63" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1">dls<span class="op" style="color: #5E5E5E;">=</span> get_unet_dls(<span class="dv" style="color: #AD0000;">8</span>, source <span class="op" style="color: #5E5E5E;">=</span> files_crappy, get_y <span class="op" style="color: #5E5E5E;">=</span> get_y, </span>
<span id="cb63-2">                     splitter <span class="op" style="color: #5E5E5E;">=</span> RandomSplitter(), item_tfms <span class="op" style="color: #5E5E5E;">=</span> item_tfms,</span>
<span id="cb63-3">                     batch_tfms <span class="op" style="color: #5E5E5E;">=</span> batch_tfms)</span></code></pre></div>
</details>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb64" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1">dls.show_batch()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/image-restoration-series/chapter1-deblur_files/figure-html/cell-47-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>since we have re-initialized our unet_learner, we would need to assign the new dataloader (the one with 256px images) to the learner.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb65" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1">unet_learn.dls <span class="op" style="color: #5E5E5E;">=</span> dls</span></code></pre></div>
</details>
</div>
<p>Load the model that was trained on the 128px images.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb66" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1">unet_learn.load(<span class="st" style="color: #20794D;">'model_128'</span>)<span class="op" style="color: #5E5E5E;">;</span></span></code></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.9/dist-packages/fastai/learner.py:58: UserWarning: Saved filed doesn't contain an optimizer state.
  elif with_opt: warn("Saved filed doesn't contain an optimizer state.")</code></pre>
</div>
</div>
<p>Remember that our input image size have changed now and so has the parameters of our model. Due to this we will have to find new learning rate.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb68" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1">lr <span class="op" style="color: #5E5E5E;">=</span> unet_learn.lr_find(suggest_funcs<span class="op" style="color: #5E5E5E;">=</span>(valley, slide))</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/image-restoration-series/chapter1-deblur_files/figure-html/cell-50-output-3.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb69" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1">(lr.valley, lr.slide)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<pre><code>(9.120108734350652e-05, 0.0006918309954926372)</code></pre>
</div>
</div>
<p>Next, we will fine tune our model on the new set of data. With <code>fine_tune</code> we ask fastai to freeze the model, train it for a epoch, unfreeze it and then train for some more epochs.</p>
<p>In case you wan to understand more about <code>fine_tune</code>, then here is the <a href="https://github.com/fastai/fastai/blob/f2ab8ba78b63b2f4ebd64ea440b9886a2b9e7b6f/fastai/callback/schedule.py#L153">source code</a> and <a href="https://github.com/fastai/fastai/blob/f2ab8ba78b63b2f4ebd64ea440b9886a2b9e7b6f/fastai/callback/schedule.py#L153">here</a> is an old thread in the fastai forum where <code>fine_tune</code> was discussed.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb71" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1">unet_learn.fine_tune(<span class="dv" style="color: #AD0000;">5</span>, (lr.valley<span class="op" style="color: #5E5E5E;">+</span>lr.slide)<span class="op" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">2</span>, pct_start<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.9</span>, wd<span class="op" style="color: #5E5E5E;">=</span>config[<span class="st" style="color: #20794D;">'wd'</span>], </span>
<span id="cb71-2">                         cbs<span class="op" style="color: #5E5E5E;">=</span>[EarlyStoppingCallback(patience<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>), SaveModelCallback(fname<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'model_256'</span>)])</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>pixel</th>
      <th>feat_0</th>
      <th>feat_1</th>
      <th>feat_2</th>
      <th>gram_0</th>
      <th>gram_1</th>
      <th>gram_2</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.473990</td>
      <td>1.519641</td>
      <td>0.186348</td>
      <td>0.215888</td>
      <td>0.219089</td>
      <td>0.080457</td>
      <td>0.296886</td>
      <td>0.394380</td>
      <td>0.126594</td>
      <td>16:53</td>
    </tr>
  </tbody>
</table>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Better model found at epoch 0 with valid_loss value: 1.519641399383545.</code></pre>
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>pixel</th>
      <th>feat_0</th>
      <th>feat_1</th>
      <th>feat_2</th>
      <th>gram_0</th>
      <th>gram_1</th>
      <th>gram_2</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.412022</td>
      <td>1.459381</td>
      <td>0.178994</td>
      <td>0.212948</td>
      <td>0.214645</td>
      <td>0.078533</td>
      <td>0.275205</td>
      <td>0.375495</td>
      <td>0.123562</td>
      <td>17:30</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1.368803</td>
      <td>1.436245</td>
      <td>0.174595</td>
      <td>0.210241</td>
      <td>0.212634</td>
      <td>0.077483</td>
      <td>0.268589</td>
      <td>0.370479</td>
      <td>0.122224</td>
      <td>17:30</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.368343</td>
      <td>1.437706</td>
      <td>0.169373</td>
      <td>0.206416</td>
      <td>0.211114</td>
      <td>0.077285</td>
      <td>0.278587</td>
      <td>0.373089</td>
      <td>0.121841</td>
      <td>17:31</td>
    </tr>
    <tr>
      <td>3</td>
      <td>1.324890</td>
      <td>1.393757</td>
      <td>0.164672</td>
      <td>0.203151</td>
      <td>0.207758</td>
      <td>0.075826</td>
      <td>0.261442</td>
      <td>0.361697</td>
      <td>0.119211</td>
      <td>17:30</td>
    </tr>
    <tr>
      <td>4</td>
      <td>1.313353</td>
      <td>1.355986</td>
      <td>0.158461</td>
      <td>0.200111</td>
      <td>0.204974</td>
      <td>0.074636</td>
      <td>0.248391</td>
      <td>0.351761</td>
      <td>0.117653</td>
      <td>17:30</td>
    </tr>
  </tbody>
</table>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Better model found at epoch 0 with valid_loss value: 1.4593812227249146.
Better model found at epoch 1 with valid_loss value: 1.4362448453903198.
Better model found at epoch 3 with valid_loss value: 1.393756628036499.
Better model found at epoch 4 with valid_loss value: 1.3559863567352295.</code></pre>
</div>
</div>
<p>Let‚Äôs check the predictions on the validation set.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb74" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1">unet_learn.show_results()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/image-restoration-series/chapter1-deblur_files/figure-html/cell-53-output-3.png" class="img-fluid"></p>
</div>
</div>
<p>So, the results here have improved as compared to the 128px images. You see that‚Äôs how progressive resizing helps. Starting with smaller sized images makes training faster.</p>
<p>From this point onwards you can keep on increasing the image size and keep training until the performance of your model doesn‚Äôt improve much.</p>
</section>
<section id="observations-from-my-experiments" class="level2">
<h2 class="anchored" data-anchor-id="observations-from-my-experiments">Observations from my experiments</h2>
<p>I have actually done many different experiments on this and I have the followign observations ‚Äì&gt;</p>
<ul>
<li><p>When sufficient data is available, training for a 4-5 epochs during each stage of progressive re-sizing is sufficient to generate ‚Äúdecently blur free‚Äù images.</p></li>
<li><p>I have got decent results during training done on only 200 images but there were some noticeable artifacts in the generated images. An interesting thing I noticed was that the motion blur was removed.</p></li>
<li><p>In my experiments even when I didn‚Äôt continue the training after I had trained the model on 256px images, decent results were produced on bigger sized images during inference.</p></li>
</ul>
</section>
<section id="inference" class="level2">
<h2 class="anchored" data-anchor-id="inference">Inference</h2>
<p>Let‚Äôs test our model on unseen data. For this I have downloaded new images (which were not inlcuded in teh training data) from pexels.com.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb75" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1">config[<span class="st" style="color: #20794D;">'test_860'</span>] <span class="op" style="color: #5E5E5E;">=</span> Path(<span class="st" style="color: #20794D;">"test_860"</span>)</span>
<span id="cb75-2">config[<span class="st" style="color: #20794D;">'test_crappy'</span>] <span class="op" style="color: #5E5E5E;">=</span> Path(<span class="st" style="color: #20794D;">"test_crappy"</span>)</span></code></pre></div>
</details>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb76" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1">files_test <span class="op" style="color: #5E5E5E;">=</span> get_image_files(config[<span class="st" style="color: #20794D;">'test_860'</span>])</span></code></pre></div>
</details>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb77" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1">files_test</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<pre><code>(#2) [Path('test_860/sports/pexels-pixabay-248547.jpg'),Path('test_860/signpost/signpost.jpg')]</code></pre>
</div>
</div>
<p>‚Ä¶and crappified them like before.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb79" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1">crappify_imgs(config[<span class="st" style="color: #20794D;">'test_860'</span>], config[<span class="st" style="color: #20794D;">'test_crappy'</span>], sz<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">80</span>, n_workers<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">8</span>)</span></code></pre></div>
</details>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb80" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1">files_crappy <span class="op" style="color: #5E5E5E;">=</span> get_image_files(config[<span class="st" style="color: #20794D;">'test_crappy'</span>])</span></code></pre></div>
</details>
</div>
<p>We also need to grab our dataloader with the same type of transformations as before but this time we will take a 860px image because we are trying to generate ‚Äúfull size‚Äù blur free image.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb81" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><span class="co" style="color: #5E5E5E;">#transformations</span></span>
<span id="cb81-2">item_tfms <span class="op" style="color: #5E5E5E;">=</span> Resize(<span class="dv" style="color: #AD0000;">860</span>)</span>
<span id="cb81-3">batch_tfms <span class="op" style="color: #5E5E5E;">=</span> [<span class="op" style="color: #5E5E5E;">*</span>aug_transforms(max_zoom<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">2.</span>), Normalize.from_stats(<span class="op" style="color: #5E5E5E;">*</span>imagenet_stats)]</span>
<span id="cb81-4">get_y <span class="op" style="color: #5E5E5E;">=</span> <span class="kw" style="color: #003B4F;">lambda</span> x: x.relative_to(config[<span class="st" style="color: #20794D;">'test_crappy'</span>])</span></code></pre></div>
</details>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb82" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1">dls<span class="op" style="color: #5E5E5E;">=</span> get_unet_dls(<span class="dv" style="color: #AD0000;">2</span>, source <span class="op" style="color: #5E5E5E;">=</span> files_crappy, get_y <span class="op" style="color: #5E5E5E;">=</span> get_y, </span>
<span id="cb82-2">                     splitter <span class="op" style="color: #5E5E5E;">=</span> RandomSplitter(), item_tfms <span class="op" style="color: #5E5E5E;">=</span> item_tfms,</span>
<span id="cb82-3">                     batch_tfms <span class="op" style="color: #5E5E5E;">=</span> batch_tfms)</span></code></pre></div>
</details>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb83" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1">dls.show_batch()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/image-restoration-series/chapter1-deblur_files/figure-html/cell-61-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Create the Unet learner as before.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb84" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1">unet_learn <span class="op" style="color: #5E5E5E;">=</span> unet_learner(dls, models.resnet34, loss_func<span class="op" style="color: #5E5E5E;">=</span>F.l1_loss,</span>
<span id="cb84-2">                     blur<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, norm_type<span class="op" style="color: #5E5E5E;">=</span>NormType.Weight).load(config[<span class="st" style="color: #20794D;">'root'</span>]<span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'model_256'</span>)</span></code></pre></div>
</details>
</div>
<p>Before prediction we will drop any sort of augmentation from our data.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb85" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1">dl <span class="op" style="color: #5E5E5E;">=</span> dls.train.new(shuffle<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>, drop_last<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>, </span>
<span id="cb85-2">                       after_batch<span class="op" style="color: #5E5E5E;">=</span>[IntToFloatTensor, Normalize.from_stats(<span class="op" style="color: #5E5E5E;">*</span>imagenet_stats)])</span></code></pre></div>
</details>
</div>
<p>‚Äúfastaibreadcrumbs‚Äù provides <code>save_preds</code>, to which we pass the dataloader, learner and the path. This function will generate the predictionsa nd save them.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb86" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1">save_preds(dl, unet_learn, <span class="st" style="color: #20794D;">"gen_imgs"</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

</div>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb87" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1">preds <span class="op" style="color: #5E5E5E;">=</span> [Path(<span class="st" style="color: #20794D;">"gen_imgs"</span>)<span class="op" style="color: #5E5E5E;">/</span>fn.name <span class="cf" style="color: #003B4F;">for</span> fn <span class="kw" style="color: #003B4F;">in</span> files_crappy]</span></code></pre></div>
</details>
</div>
<p>‚Äúfastaibreadcrumbs‚Äù provides <code>compare_imgs</code> which takes in the path of the original images, path of crappy images, path of generated images and then compare them side by side.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb88" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1">compare_imgs(files_test, files_crappy, preds, (<span class="dv" style="color: #AD0000;">100</span>, <span class="dv" style="color: #AD0000;">100</span>))</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[[&lt;AxesSubplot:&gt; &lt;AxesSubplot:&gt; &lt;AxesSubplot:&gt;]
 [&lt;AxesSubplot:&gt; &lt;AxesSubplot:&gt; &lt;AxesSubplot:&gt;]]</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/image-restoration-series/chapter1-deblur_files/figure-html/cell-66-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Look! With a few epochs of training our model was able to recognize and remove motion blur from images. This is incredible as to how with some clever tricks (like progressive resizing) we can create such powerful deep learning models which runs fast on quite decent machines and with ‚Äúnot so crazy‚Äù amount of data. Another thing to note is that we had just used 256px images in our training but even then our model generalizes quite well when a bigger image is given to it. So, a couple of more epochswould produce even better results.</p>
<p>One quick thing to note is that you might notice the proportion difference between the original image and the generated images. This is becasue we had resized the images to 860px during training to make the inference faster. If you have access to GPUs with larger memory, then you can give the full size image a shot.</p>
<p>Now that we have created and trained a model to remove camera shake from our images, in next chapter we will prototype a Graphical tool that can be used as an interface to consume this model.</p>
</section>
<section id="reading-list" class="level2">
<h2 class="anchored" data-anchor-id="reading-list">Reading list</h2>
<ul>
<li><p><a href="https://github.com/sapal6/image-restoration">Code for this tutorial</a></p></li>
<li><p>Superresolution- Fastai - <a href="https://github.com/fastai/fastai/blob/master/dev_nbs/course/lesson7-superres.ipynb">Lesson Notebook</a>, <a href="https://www.youtube.com/watch?v=9spwoDYwW_I&amp;t=4243s">Lesson video</a></p></li>
<li><p><a href="https://course.fast.ai/">Fastai course 2022</a></p></li>
<li><p><a href="https://arxiv.org/abs/1603.08155">‚ÄúPerceptual Losses for Real-Time Style Transfer and Super-Resolution</a></p></li>
<li><p><a href="https://walkwithfastai.com/Super_Resolution">Walk with fastai</a></p></li>
</ul>


</section>

 ]]></description>
  <category>Deep Learning</category>
  <guid>https://www.satyabratapal.xyz/posts/image-restoration-series/chapter1-deblur.html</guid>
  <pubDate>Mon, 26 Sep 2022 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Can-we-use-neural-network-to-understand-data</title>
  <dc:creator>Satyabrata pal</dc:creator>
  <link>https://www.satyabratapal.xyz/posts/Can-we-use-neural-network-to-understand-data/Can-we-use-neural-network-to-understand-data.html</link>
  <description><![CDATA[ 



<ul>
<li>This notebook can also available in <a href="https://www.kaggle.com/sapal6/can-we-use-a-network-to-understand-data">kaggle</a></li>
<li>Get the code used in this note book on <a href="https://github.com/sapal6/kaggle_competitions/tree/main/happywhale">github</a>.</li>
<li>The data used can be found <a href="https://www.kaggle.com/rdizzl3/jpeg-happywhale-128x128">here</a></li>
</ul>
<section id="what-is-this" class="level2">
<h2 class="anchored" data-anchor-id="what-is-this">What is this ?</h2>
<ul>
<li>A starter notebook for folks new to machine learning.</li>
<li>A vanilla CNN created for <a href="https://www.kaggle.com/c/happy-whale-and-dolphin">whale species recognition</a> competition using fastai.</li>
<li>Aim is to observe the behavior of a simple CNN trained to recognize the whale species.</li>
<li>Here I am trying to show that you can use a simple CNN to know what areas of an image does the CNN ‚Äúsees‚Äù. You can use this to analyze what needs to be improved or engineered in the data to create a better model.</li>
</ul>
</section>
<section id="import-required-things." class="level2">
<h2 class="anchored" data-anchor-id="import-required-things.">Import required things.</h2>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-02-18T13:17:06.391942Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-02-18T13:17:06.391131Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-02-18T13:17:08.193261Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-02-18T13:17:08.192770Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2022-02-16T17:20:43.381136Z&quot;}" data-papermill="{&quot;duration&quot;:2.199272,&quot;end_time&quot;:&quot;2022-02-18T13:17:08.193391&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-02-18T13:17:05.994119&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">from</span> fastai.vision.<span class="bu" style="color: null;">all</span> <span class="im" style="color: #00769E;">import</span> <span class="op" style="color: #5E5E5E;">*</span></span></code></pre></div>
</details>
</div>
<section id="required-paths" class="level3">
<h3 class="anchored" data-anchor-id="required-paths">Required paths</h3>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-02-18T13:17:09.770028Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-02-18T13:17:09.768969Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-02-18T13:17:09.770727Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-02-18T13:17:09.771233Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2022-02-16T17:20:46.080951Z&quot;}" data-papermill="{&quot;duration&quot;:0.395792,&quot;end_time&quot;:&quot;2022-02-18T13:17:09.771366&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-02-18T13:17:09.375574&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">root_path <span class="op" style="color: #5E5E5E;">=</span> Path(<span class="st" style="color: #20794D;">"../input"</span>)</span>
<span id="cb2-2">train_csv_path <span class="op" style="color: #5E5E5E;">=</span> root_path<span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'happy-whale-and-dolphin/train.csv'</span></span>
<span id="cb2-3">train_img_path <span class="op" style="color: #5E5E5E;">=</span> root_path<span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">"jpeg-happywhale-128x128/train_images-128-128/train_images-128-128"</span></span></code></pre></div>
</details>
</div>
</section>
</section>
<section id="get-a-glimpse-of-the-files" class="level2">
<h2 class="anchored" data-anchor-id="get-a-glimpse-of-the-files">Get a glimpse of the files</h2>
<p><code>get_image_files</code> is a convenience function that goes through different folders and subfolders and gathers all the image file path.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-02-18T13:17:11.351775Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-02-18T13:17:11.350726Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-02-18T13:18:02.468327Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-02-18T13:18:02.468828Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2022-02-16T17:20:46.640338Z&quot;}" data-papermill="{&quot;duration&quot;:51.512361,&quot;end_time&quot;:&quot;2022-02-18T13:18:02.468976&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-02-18T13:17:10.956615&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">train_imgs <span class="op" style="color: #5E5E5E;">=</span> get_image_files(train_img_path)</span>
<span id="cb3-2">train_imgs[:<span class="dv" style="color: #AD0000;">10</span>]</span></code></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>(#10) [Path('../input/jpeg-happywhale-128x128/train_images-128-128/train_images-128-128/80b5373b87942b.jpg'),Path('../input/jpeg-happywhale-128x128/train_images-128-128/train_images-128-128/e113b51585c677.jpg'),Path('../input/jpeg-happywhale-128x128/train_images-128-128/train_images-128-128/94eb976e25416c.jpg'),Path('../input/jpeg-happywhale-128x128/train_images-128-128/train_images-128-128/19a45862ab99cd.jpg'),Path('../input/jpeg-happywhale-128x128/train_images-128-128/train_images-128-128/be9645065510e9.jpg'),Path('../input/jpeg-happywhale-128x128/train_images-128-128/train_images-128-128/76d25044120d3c.jpg'),Path('../input/jpeg-happywhale-128x128/train_images-128-128/train_images-128-128/0c64a705ba5d31.jpg'),Path('../input/jpeg-happywhale-128x128/train_images-128-128/train_images-128-128/c1fe278bdbd837.jpg'),Path('../input/jpeg-happywhale-128x128/train_images-128-128/train_images-128-128/6f94f30ac500a0.jpg'),Path('../input/jpeg-happywhale-128x128/train_images-128-128/train_images-128-128/15f295322e5b54.jpg')]</code></pre>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-02-18T13:18:03.265302Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-02-18T13:18:03.264390Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-02-18T13:18:03.288241Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-02-18T13:18:03.288707Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2022-02-16T17:21:29.887475Z&quot;}" data-papermill="{&quot;duration&quot;:0.430282,&quot;end_time&quot;:&quot;2022-02-18T13:18:03.288865&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-02-18T13:18:02.858583&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="5">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">Image.<span class="bu" style="color: null;">open</span>(train_imgs[<span class="dv" style="color: #AD0000;">1</span>])</span></code></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="5">
<p><img src="https://www.satyabratapal.xyz/posts/Can-we-use-neural-network-to-understand-data/Can-we-use-neural-network-to-understand-data_files/figure-html/cell-5-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>let‚Äôs see what is there in the csv file.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-02-18T13:18:04.866516Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-02-18T13:18:04.865688Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-02-18T13:18:04.950054Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-02-18T13:18:04.949019Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2022-02-16T17:21:29.913758Z&quot;}" data-papermill="{&quot;duration&quot;:0.475971,&quot;end_time&quot;:&quot;2022-02-18T13:18:04.950185&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-02-18T13:18:04.474214&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="6">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">train_path_df <span class="op" style="color: #5E5E5E;">=</span> pd.read_csv(train_csv_path)</span></code></pre></div>
</details>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-02-18T13:18:05.741462Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-02-18T13:18:05.740545Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-02-18T13:18:05.751734Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-02-18T13:18:05.752156Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2022-02-16T17:21:32.020457Z&quot;}" data-papermill="{&quot;duration&quot;:0.413408,&quot;end_time&quot;:&quot;2022-02-18T13:18:05.752298&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-02-18T13:18:05.338890&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="7">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">train_path_df.head(<span class="dv" style="color: #AD0000;">5</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="7">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>image</th>
      <th>species</th>
      <th>individual_id</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>00021adfb725ed.jpg</td>
      <td>melon_headed_whale</td>
      <td>cadddb1636b9</td>
    </tr>
    <tr>
      <th>1</th>
      <td>000562241d384d.jpg</td>
      <td>humpback_whale</td>
      <td>1a71fbb72250</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0007c33415ce37.jpg</td>
      <td>false_killer_whale</td>
      <td>60008f293a2b</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0007d9bca26a99.jpg</td>
      <td>bottlenose_dolphin</td>
      <td>4b00fe572063</td>
    </tr>
    <tr>
      <th>4</th>
      <td>00087baf5cef7a.jpg</td>
      <td>humpback_whale</td>
      <td>8e5253662392</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>If I can catch hold of the image filename from the dataframe and attach this to the <code>train_img_path</code> then I can get the full image path.</p>
<p>Replacing filnames with the filepath</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-02-18T13:18:08.154766Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-02-18T13:18:08.153919Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-02-18T13:18:08.582546Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-02-18T13:18:08.582019Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2022-02-16T17:21:34.609355Z&quot;}" data-papermill="{&quot;duration&quot;:0.848095,&quot;end_time&quot;:&quot;2022-02-18T13:18:08.582708&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-02-18T13:18:07.734613&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="8">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">train_path_df[<span class="st" style="color: #20794D;">'image'</span>] <span class="op" style="color: #5E5E5E;">=</span> train_path_df[<span class="st" style="color: #20794D;">'image'</span>].<span class="bu" style="color: null;">apply</span>(<span class="kw" style="color: #003B4F;">lambda</span> x:train_img_path<span class="op" style="color: #5E5E5E;">/</span>x)</span></code></pre></div>
</details>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-02-18T13:18:09.373415Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-02-18T13:18:09.372440Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-02-18T13:18:09.376258Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-02-18T13:18:09.376663Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2022-02-16T17:21:35.436696Z&quot;}" data-papermill="{&quot;duration&quot;:0.412572,&quot;end_time&quot;:&quot;2022-02-18T13:18:09.376855&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-02-18T13:18:08.964283&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="9">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">train_path_df.head(<span class="dv" style="color: #AD0000;">2</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="9">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>image</th>
      <th>species</th>
      <th>individual_id</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>../input/jpeg-happywhale-128x128/train_images-128-128/train_images-128-128/00021adfb725ed.jpg</td>
      <td>melon_headed_whale</td>
      <td>cadddb1636b9</td>
    </tr>
    <tr>
      <th>1</th>
      <td>../input/jpeg-happywhale-128x128/train_images-128-128/train_images-128-128/000562241d384d.jpg</td>
      <td>humpback_whale</td>
      <td>1a71fbb72250</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-02-18T13:18:10.429522Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-02-18T13:18:10.428976Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-02-18T13:18:10.445620Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-02-18T13:18:10.446057Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2022-02-16T17:21:36.153197Z&quot;}" data-papermill="{&quot;duration&quot;:0.457953,&quot;end_time&quot;:&quot;2022-02-18T13:18:10.446198&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-02-18T13:18:09.988245&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="10">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">Image.<span class="bu" style="color: null;">open</span>(train_path_df.image[<span class="dv" style="color: #AD0000;">0</span>])</span></code></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="10">
<p><img src="https://www.satyabratapal.xyz/posts/Can-we-use-neural-network-to-understand-data/Can-we-use-neural-network-to-understand-data_files/figure-html/cell-10-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="datablock" class="level2">
<h2 class="anchored" data-anchor-id="datablock">Datablock</h2>
<p>A <code>DataBlock</code> is like a blueprint that tells fastai‚Äì&gt; * What kind of data are we dealing with. Is it image, text etc. * What kind of labels we have. For example categorical labels, continuous labels etc. * From where to get the inputs. * From where to get the targets. * How to split the data into train and test. * The transforms that you want to do.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-02-18T13:18:12.046233Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-02-18T13:18:12.045237Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-02-18T13:18:12.047318Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-02-18T13:18:12.047806Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2022-02-16T17:21:38.208852Z&quot;}" data-papermill="{&quot;duration&quot;:0.409721,&quot;end_time&quot;:&quot;2022-02-18T13:18:12.047965&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-02-18T13:18:11.638244&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="11">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">data <span class="op" style="color: #5E5E5E;">=</span> DataBlock(blocks<span class="op" style="color: #5E5E5E;">=</span>(ImageBlock, CategoryBlock),</span>
<span id="cb11-2">                splitter<span class="op" style="color: #5E5E5E;">=</span>TrainTestSplitter(),</span>
<span id="cb11-3">                 get_x <span class="op" style="color: #5E5E5E;">=</span> ColReader(<span class="dv" style="color: #AD0000;">0</span>),</span>
<span id="cb11-4">                 get_y <span class="op" style="color: #5E5E5E;">=</span> ColReader(<span class="dv" style="color: #AD0000;">1</span>),</span>
<span id="cb11-5">                 item_tfms<span class="op" style="color: #5E5E5E;">=</span>Resize(<span class="dv" style="color: #AD0000;">224</span>),</span>
<span id="cb11-6">                 batch_tfms<span class="op" style="color: #5E5E5E;">=</span>aug_transforms(size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">128</span>))</span></code></pre></div>
</details>
</div>
<p>In the above code‚Äì&gt; * The <code>ImageBlock</code> tells fastai that we are dealing with image data. * <code>CategoryBlock</code> means our targets are categorical in nature. * The <code>get_x</code> and <code>get_y</code> tells fastai to get the x i.e.&nbsp;the image path from column 0 of dataframe and the target from column 1 of the dataframe. * <code>splitter</code> is the way in which train and test data are split. check <a href="https://docs.fast.ai/data.transforms.html#TrainTestSplitter">here</a> for more details. * The next two lines are not that important to understand now but they are needed when you are working on image augmentation. Refer to the <a href="https://github.com/fastai/fastbook">fastbook</a> to understand more about this.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-02-18T13:18:13.632915Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-02-18T13:18:13.632178Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-02-18T13:18:13.633836Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-02-18T13:18:13.633339Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2022-02-16T17:22:22.952736Z&quot;}" data-papermill="{&quot;duration&quot;:0.401244,&quot;end_time&quot;:&quot;2022-02-18T13:18:13.633952&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-02-18T13:18:13.232708&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="12">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="co" style="color: #5E5E5E;"># this is not necessary. I was just testing if my datablocks are correct or not</span></span>
<span id="cb12-2"><span class="co" style="color: #5E5E5E;">#data.summary(train_path_df)</span></span></code></pre></div>
</details>
</div>
</section>
<section id="dataloader" class="level2">
<h2 class="anchored" data-anchor-id="dataloader">Dataloader</h2>
<p>Think of a dataloader as a mechanism that picks up your images, does all the things that you had described in the datablock and then loads your data to the device(cpu or gpu).</p>
<p>At the minimum, you need to provide the source of data, the <code>train_path_df</code> in our case. Here we give the <code>bs</code> i.e.&nbsp;the batch size. For example if we say that the batch size is 8 then it means that the dataloader will load 8 images at a time onto the device.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-02-18T13:18:15.222984Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-02-18T13:18:15.222090Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-02-18T13:18:21.024557Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-02-18T13:18:21.025779Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2022-02-16T17:22:31.24258Z&quot;}" data-papermill="{&quot;duration&quot;:6.206063,&quot;end_time&quot;:&quot;2022-02-18T13:18:21.026019&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-02-18T13:18:14.819956&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="13">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">dls <span class="op" style="color: #5E5E5E;">=</span> data.dataloaders(train_path_df, bs<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">128</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>/root/.local/lib/python3.7/site-packages/torch/_tensor.py:1023: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release.
torch.linalg.solve has its arguments reversed and does not return the LU factorization.
To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack.
X = torch.solve(B, A).solution
should be replaced with
X = torch.linalg.solve(A, B) (Triggered internally at  /pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:760.)
  ret = func(*args, **kwargs)</code></pre>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-02-18T13:18:21.997075Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-02-18T13:18:21.996048Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-02-18T13:18:23.847491Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-02-18T13:18:23.847949Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2022-02-16T17:23:17.288009Z&quot;}" data-papermill="{&quot;duration&quot;:2.250795,&quot;end_time&quot;:&quot;2022-02-18T13:18:23.848095&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-02-18T13:18:21.597300&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="14">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1">dls.show_batch()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/Can-we-use-neural-network-to-understand-data/Can-we-use-neural-network-to-understand-data_files/figure-html/cell-14-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="the-model" class="level2">
<h2 class="anchored" data-anchor-id="the-model">The model</h2>
<p>Simple CNN to classify whale/dolphin species. The intuition is the following‚Äì&gt; * a model which knows to recognize whale/dolphin species should also be able to learn the features to distinguish the different species. * such a model can be queried to learn what all portions of an image is taken into account. For example, if somehow the background is tricking the model such that the model considers the background as an area of interest then such data needs to be further engineered.</p>
<ul>
<li>such model can then be used as a pre-trained model to recognize individuals .</li>
</ul>
</section>
<section id="creating-a-simple-cnn-learner" class="level2">
<h2 class="anchored" data-anchor-id="creating-a-simple-cnn-learner">Creating a simple CNN learner</h2>
<p><code>cnn_learner</code> creates a convolutional neural network for you. At the least you provide it the dataloader, a model architecture and a loss function.</p>
<p><code>models.resnet18</code> is a ‚Äúpre-trained‚Äù neural network. It‚Äôs a network that was already trained by some good folks (folks with huge computational hardware) on a very large number of images. This particular network knows how to recognize different images. So, we take this network and then ask fastai to train this network on our image data (whale, dolphins).</p>
<p>The intuition is that since the pre-trained network already knows how to distinguish between different images, we have to spend less time and computation to make it understand how to recognize different whales and dolphins. This is <strong>Transfer learning</strong></p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-02-18T13:18:26.292162Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-02-18T13:18:26.291408Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-02-18T13:21:49.863512Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-02-18T13:21:49.864542Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2022-02-16T17:23:34.216093Z&quot;}" data-papermill="{&quot;duration&quot;:203.989613,&quot;end_time&quot;:&quot;2022-02-18T13:21:49.864814&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-02-18T13:18:25.875201&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="15">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">learn <span class="op" style="color: #5E5E5E;">=</span> cnn_learner(dls, models.resnet18, loss_func<span class="op" style="color: #5E5E5E;">=</span>CrossEntropyLossFlat(), ps<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.25</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>Downloading: "https://download.pytorch.org/models/resnet18-f37072fd.pth" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"485ea4ba3257442d9c37ea0f1e26c7c4","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/root/.local/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)</code></pre>
</div>
</div>
<p>For now understand that <code>fine_tune</code> means ‚Äútake my network and make it look at each image a specified number of times. This‚Äùnumber of times‚Äù is the number that is provided as the first argument.</p>
<p>Let‚Äôs not dive into the second argument at this moment in time.</p>
<blockquote class="blockquote">
<p>For the curious mind, the second argument is known as the learning rate. It‚Äôs a hyperparameter of a neural network. More information can be found <a href="https://github.com/fastai/fastbook/blob/master/04_mnist_basics.ipynb">here</a></p>
</blockquote>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-02-18T13:21:51.509256Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-02-18T13:21:51.508325Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-02-18T13:30:40.163063Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-02-18T13:30:40.161578Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2022-02-16T17:23:37.479409Z&quot;}" data-papermill="{&quot;duration&quot;:529.059765,&quot;end_time&quot;:&quot;2022-02-18T13:30:40.163200&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-02-18T13:21:51.103435&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="16">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1">learn.fine_tune(<span class="dv" style="color: #AD0000;">2</span>, <span class="fl" style="color: #AD0000;">3e-3</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.905516</td>
      <td>0.673760</td>
      <td>03:58</td>
    </tr>
  </tbody>
</table>
</div>
<div class="cell-output cell-output-display">

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.422199</td>
      <td>0.337718</td>
      <td>02:24</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.231187</td>
      <td>0.232147</td>
      <td>02:24</td>
    </tr>
  </tbody>
</table>
</div>
</div>
<p>We can see what the model predicted vs the actual target.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-02-18T13:30:42.052118Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-02-18T13:30:42.051305Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-02-18T13:30:43.306546Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-02-18T13:30:43.307028Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2022-02-16T17:32:46.541576Z&quot;}" data-papermill="{&quot;duration&quot;:1.946826,&quot;end_time&quot;:&quot;2022-02-18T13:30:43.307189&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-02-18T13:30:41.360363&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="17">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1">learn.show_results()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/Can-we-use-neural-network-to-understand-data/Can-we-use-neural-network-to-understand-data_files/figure-html/cell-17-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-02-18T13:30:44.156842Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-02-18T13:30:44.156005Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-02-18T13:31:15.812230Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-02-18T13:31:15.812715Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2022-02-16T17:33:39.025923Z&quot;}" data-papermill="{&quot;duration&quot;:32.083541,&quot;end_time&quot;:&quot;2022-02-18T13:31:15.812874&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-02-18T13:30:43.729333&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="18">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1">interp <span class="op" style="color: #5E5E5E;">=</span> Interpretation.from_learner(learn)</span>
<span id="cb21-2">interp.plot_top_losses(<span class="dv" style="color: #AD0000;">3</span>, figsize<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">15</span>,<span class="dv" style="color: #AD0000;">10</span>))</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/Can-we-use-neural-network-to-understand-data/Can-we-use-neural-network-to-understand-data_files/figure-html/cell-18-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="why-the-neural-network-makes-the-decision" class="level2">
<h2 class="anchored" data-anchor-id="why-the-neural-network-makes-the-decision">Why the neural network makes the decision?</h2>
<p>We can use class activation maps (CAM) to see which regions of images are of interest to the neural network. Class activation map (CAM) was first introduced by Bolei Zhou et al.&nbsp;in <a href="https://arxiv.org/abs/1512.04150">‚ÄúLearning Deep Features for Discriminative Localization‚Äù</a>. Cam uses the output of the last convolutional layer with the predictions to give a heatmap of the regions of interest of the network in an image.</p>
<p>The intuition behind CAM is that if we do the dot product of the activations of the final layer with the final weights, for each location on our feature map then we can get the score of the feature that was used to make a decision.</p>
<p>Pytorch provides hooks to hook onto the any layer of a network. We can attach a hook to any layer of a neural network and it will be executed during the forward pass i.e.&nbsp;the moment when the output is computed or during the backward pass i.e.&nbsp;the moment when the weights are being re-adjusted.</p>
<p>I would highly recommend to read more about CAM in this chapter of <a href="https://github.com/fastai/fastbook/blob/master/18_CAM.ipynb">fastbook</a> before proceeding on with the code.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-02-18T13:31:17.514833Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-02-18T13:31:17.513187Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-02-18T13:31:17.527205Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-02-18T13:31:17.526776Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2022-02-16T17:35:19.246685Z&quot;}" data-papermill="{&quot;duration&quot;:0.439246,&quot;end_time&quot;:&quot;2022-02-18T13:31:17.527326&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-02-18T13:31:17.088080&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="19">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1">Image.<span class="bu" style="color: null;">open</span>(train_path_df.image[<span class="dv" style="color: #AD0000;">6</span>])</span></code></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="19">
<p><img src="https://www.satyabratapal.xyz/posts/Can-we-use-neural-network-to-understand-data/Can-we-use-neural-network-to-understand-data_files/figure-html/cell-19-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Let‚Äôs grab a batch.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-02-18T13:31:19.222811Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-02-18T13:31:19.221903Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-02-18T13:31:19.396487Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-02-18T13:31:19.397039Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2022-02-16T17:35:24.874665Z&quot;}" data-papermill="{&quot;duration&quot;:0.605277,&quot;end_time&quot;:&quot;2022-02-18T13:31:19.397211&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-02-18T13:31:18.791934&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="20">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1">img <span class="op" style="color: #5E5E5E;">=</span> PILImage.create(train_path_df.image[<span class="dv" style="color: #AD0000;">6</span>])</span>
<span id="cb23-2"><span class="co" style="color: #5E5E5E;"># grab a batch</span></span>
<span id="cb23-3">x, <span class="op" style="color: #5E5E5E;">=</span> first(dls.test_dl([img]))</span></code></pre></div>
</details>
</div>
<p>We hook into the network . A forward hook takes into three things ‚Äì&gt; the model , its input and it‚Äôs output. The fourth line in the below code is the hook function.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-02-18T13:31:21.074821Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-02-18T13:31:21.074036Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-02-18T13:31:21.076786Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-02-18T13:31:21.076344Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2022-02-16T17:35:26.165998Z&quot;}" data-papermill="{&quot;duration&quot;:0.430662,&quot;end_time&quot;:&quot;2022-02-18T13:31:21.076908&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-02-18T13:31:20.646246&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="21">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><span class="kw" style="color: #003B4F;">class</span> Hook():</span>
<span id="cb24-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, m):</span>
<span id="cb24-3">        <span class="va" style="color: #111111;">self</span>.hook <span class="op" style="color: #5E5E5E;">=</span> m.register_forward_hook(<span class="va" style="color: #111111;">self</span>.hook_func)   </span>
<span id="cb24-4">    <span class="kw" style="color: #003B4F;">def</span> hook_func(<span class="va" style="color: #111111;">self</span>, m, i, o): <span class="va" style="color: #111111;">self</span>.stored <span class="op" style="color: #5E5E5E;">=</span> o.detach().clone()</span>
<span id="cb24-5">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__enter__</span>(<span class="va" style="color: #111111;">self</span>, <span class="op" style="color: #5E5E5E;">*</span>args): <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span></span>
<span id="cb24-6">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__exit__</span>(<span class="va" style="color: #111111;">self</span>, <span class="op" style="color: #5E5E5E;">*</span>args): <span class="va" style="color: #111111;">self</span>.hook.remove()</span></code></pre></div>
</details>
</div>
<p>Now we do the dot product of our weight matrix with the activations.</p>
<p>The code below takes the model, hooks into the last layer of our model using our hook function and stores the activations in the <code>act</code> variable. Then we do the dot product of the stored activations with the weights using <code>torch.einsum</code>.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-02-18T13:31:23.030423Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-02-18T13:31:23.029503Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-02-18T13:31:23.112449Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-02-18T13:31:23.111974Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2022-02-16T17:35:34.630955Z&quot;}" data-papermill="{&quot;duration&quot;:0.504757,&quot;end_time&quot;:&quot;2022-02-18T13:31:23.112577&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-02-18T13:31:22.607820&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="22">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><span class="cf" style="color: #003B4F;">with</span> Hook(learn.model[<span class="dv" style="color: #AD0000;">0</span>]) <span class="im" style="color: #00769E;">as</span> hook:</span>
<span id="cb25-2">    <span class="cf" style="color: #003B4F;">with</span> torch.no_grad(): </span>
<span id="cb25-3">        output <span class="op" style="color: #5E5E5E;">=</span> learn.model.<span class="bu" style="color: null;">eval</span>()(x.cuda())</span>
<span id="cb25-4">        act <span class="op" style="color: #5E5E5E;">=</span> hook.stored[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb25-5">    cam_map <span class="op" style="color: #5E5E5E;">=</span> torch.einsum(<span class="st" style="color: #20794D;">'ck,kij-&gt;cij'</span>, learn.model[<span class="dv" style="color: #AD0000;">1</span>][<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>].weight, act)</span>
<span id="cb25-6">    cam_map.shape</span></code></pre></div>
</details>
</div>
<p>For each image in our batch, and for each class, we get a 7√ó7 feature map that tells us where the activations were higher and where they were lower. This will let us see which areas of the pictures influenced the model‚Äôs decision.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2022-02-18T13:31:24.790253Z&quot;,&quot;iopub.status.busy&quot;:&quot;2022-02-18T13:31:24.781084Z&quot;,&quot;iopub.status.idle&quot;:&quot;2022-02-18T13:31:24.943128Z&quot;,&quot;shell.execute_reply&quot;:&quot;2022-02-18T13:31:24.943714Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2022-02-16T17:35:35.969745Z&quot;}" data-papermill="{&quot;duration&quot;:0.591457,&quot;end_time&quot;:&quot;2022-02-18T13:31:24.943877&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2022-02-18T13:31:24.352420&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="23">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb26" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1">x_dec <span class="op" style="color: #5E5E5E;">=</span> TensorImage(dls.train.decode((x,))[<span class="dv" style="color: #AD0000;">0</span>][<span class="dv" style="color: #AD0000;">0</span>])</span>
<span id="cb26-2">_,ax <span class="op" style="color: #5E5E5E;">=</span> plt.subplots()</span>
<span id="cb26-3">x_dec.show(ctx<span class="op" style="color: #5E5E5E;">=</span>ax)</span>
<span id="cb26-4">ax.imshow(cam_map[<span class="dv" style="color: #AD0000;">1</span>].detach().cpu(), alpha<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.6</span>, extent<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">0</span>,<span class="dv" style="color: #AD0000;">128</span>,<span class="dv" style="color: #AD0000;">128</span>,<span class="dv" style="color: #AD0000;">0</span>),</span>
<span id="cb26-5">              interpolation<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'bilinear'</span>, cmap<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'magma'</span>)<span class="op" style="color: #5E5E5E;">;</span></span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/Can-we-use-neural-network-to-understand-data/Can-we-use-neural-network-to-understand-data_files/figure-html/cell-23-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>The bright areas in the activations are somehow spilling over to the ocean. In my opinion this should not be the case. The activations should be more around the of the dorsal fins and the surface of the fin. In layman terms the network should focus more on the object i.e.&nbsp;the fin features and not on the surrounding water.</p>
<p>An important takeaway from my perspective is that if we can restrict the region of interest to the whale fin and not the surrounding water then the network would be able to learn better.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Besides tasks like classifying images or detecting objects, a neural networks can be used for analyzing the data to device strategies for pre-processing, data engineering and data cleaning.</p>
<p>Here I have bounced off one idea about how a simple CNN can be used to understand the regions in an image that the neural network focuses to make it‚Äôs decisions and then analyzing these things we can observer if the network is focusing on areas of images that it needs to focus and if not, then we can think of the steps that we can take to help the network to focus to generalize better.</p>
<p>I have just scratched the surface of what‚Äôs possible here and there can be many more ways in which deep learning can be used to take better decisions on data. So, I would emphasize you to expand on this idea and think of other ways in which you can use simple networks to analyze the data before building an architecture for the actual task at hand.</p>


</section>

 ]]></description>
  <category>Deep Learning</category>
  <guid>https://www.satyabratapal.xyz/posts/Can-we-use-neural-network-to-understand-data/Can-we-use-neural-network-to-understand-data.html</guid>
  <pubDate>Wed, 23 Feb 2022 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Nbdev.jl-And-Literate-programming</title>
  <dc:creator>Satyabrata pal</dc:creator>
  <link>https://www.satyabratapal.xyz/posts/Nbdev.jl-And-Literate-programming/Nbdev.jl-And-Literate-programming.html</link>
  <description><![CDATA[ 



<section id="what-is-literate-programming" class="level2">
<h2 class="anchored" data-anchor-id="what-is-literate-programming">What is literate programming</h2>
<p>Litrerate programming is a term coined by <a href="https://en.wikipedia.org/wiki/Donald_Knuth">Donald_Knuth</a>. Under this programming paradigm, one would write human redeable code documentation writing code.</p>
</section>
<section id="why-this-makes-sense" class="level2">
<h2 class="anchored" data-anchor-id="why-this-makes-sense">Why this makes sense ?</h2>
<p>You all know that whatever instructions we give to the computer, it ‚Äúpercieves‚Äù it in the following way</p>
<p><img src="https://www.satyabratapal.xyz/posts/Nbdev.jl-And-Literate-programming/matrix.gif" height="300" width="300"></p>
<p>Oops! I got carried away.. It looks more like this</p>
<p><img src="https://www.satyabratapal.xyz/posts/Nbdev.jl-And-Literate-programming/binary.png"></p>
<p>So, for a computer it‚Äôs all binary number. So, what is this that we write when we say that we are writing a computer program ?</p>
<p><img src="https://www.satyabratapal.xyz/posts/Nbdev.jl-And-Literate-programming/program.png"></p>
<p>Some of you will say this is a programming language, but I ask why it‚Äôs called a ‚Äúlanguage‚Äù ? Well! if you ask me I would say that it‚Äôs a language and like every other language that exists, it‚Äôs sole purpose is to enable communication, share stories, share thoughts. Not to a machine but to another human, that‚Äôs why it‚Äôs called a programming ‚Äúlanguage‚Äù.</p>
<p>Ultimately this programming instructions get‚Äôs converted into machine code but if this piece of code which when I show to a fellow human then that person would know what instructions I want to give a computer. See! that‚Äôs communication‚Ä¶that‚Äôs what languages do.</p>
<p>So, in short we invented programming languages for communicating our thoughts to another person, our thoughts about what we want a machine to do and that‚Äôs why we want the programming or coding to be as clear and as human readable as possible.</p>
</section>
<section id="writing-code-containing-documentation-vs-writing-documentation-containing-code" class="level2">
<h2 class="anchored" data-anchor-id="writing-code-containing-documentation-vs-writing-documentation-containing-code">Writing code containing documentation vs writing documentation containing code</h2>
<p>Usually we write code the folowing way</p>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="kw" style="color: #003B4F;">def</span> funny_func(who_is_funny):</span>
<span id="cb1-2">    <span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"I am funny"</span>)</span></code></pre></div>
</details>
</div>
<p>Over here the function is self explanatory but to make it extra clear for your friend, you might include an elaborate comment like this</p>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="kw" style="color: #003B4F;">def</span> funny_func(who_is_funny):</span>
<span id="cb2-2">    <span class="co" style="color: #5E5E5E;">"""function to tell who is funny and who is not.</span><span class="ch" style="color: #20794D;">\</span></span>
<span id="cb2-3"><span class="co" style="color: #5E5E5E;">    Takes in an argument about the name and then prints a string"""</span></span>
<span id="cb2-4">    <span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"I am funny"</span>)</span></code></pre></div>
</details>
</div>
<p>You can insert as much elaborate documentation as you may wish. All is well till now.</p>
<p>But wait! what would happen when you have thousands of lines of code? Unsurprisengly there would be thousands of lines of documentation on top of that which makes it difficult for a clear communication of your thoughts via that piece of code. Not only this, in all possibilities, you will also end up writing a separate detailed documentation explaining your code.</p>
<p>This is the problem that literate programming tries to solve. Under literate programming paradigm you write documentation containing code like this.</p>
<p><img src="https://www.satyabratapal.xyz/posts/Nbdev.jl-And-Literate-programming/literate_program.png"></p>
<p>In the above example, you can see that the document contains an heading which is the name of the function followed by a description of what the function is all about then that is followed by the code itself.</p>
<p>If I were to show this code document to you then you could easily understand what the code is all about and at teh same time you would also appreciate the ‚Äúlengthy document free‚Äù code. This in short is what a literate programming style would look like.</p>
</section>
<section id="literate-programming-tool" class="level2">
<h2 class="anchored" data-anchor-id="literate-programming-tool">Literate programming tool</h2>
<p>To implement a literate programming style you would need special tools. These tools come under the umbrella of literate programming tools, which read through your document, separate out the code and the documentations and put the code in a source code file and the documentation in a text file.</p>
<p>While talking about literate programming tools, one can not ignore the role of ‚Äúnotebooks‚Äù in making it easier for developers to write literate programs. Notebooks are web based interactive computing platforms that let‚Äôs you write text and code in a web based editor and let‚Äôs you execute your code interactively.</p>
<p><img src="https://www.satyabratapal.xyz/posts/Nbdev.jl-And-Literate-programming/notebooks.png" class="img-fluid"></p>
<p>Two of the notable notebooks are <a href="https://jupyter.org/">Jupyter notebooks</a> and <a href="https://github.com/fonsp/Pluto.jl">Pluto.jl</a>. Both of these projects differ in their behaviour and the extent of languages they support but both of these projects have a common goal, that is to provide developers with a web based literate programming and exploratory platform. With these projects you can write your documentation and code in the same notebook, interactively execute your code and also share that across as a reproducible environment.</p>
<p>Due to the very nature of these notebooks projects, they provide a very good foundation of a literate programming environment and using this foundation, <a href="https://www.fast.ai/2019/12/02/nbdev/">Jeremy howard announced the creation of Nbdev</a>. At a very high level Nbdev is a system that let‚Äôs you write your python code and documentation in jupyter notebook and then generates source code files and the related documentation for your project among other things. However, this is a very lazy explanation of Nbdev. So, I suggest you to read the original launch <a href="https://www.fast.ai/2019/12/02/nbdev/">blog</a> and the <a href="https://nbdev.fast.ai/tutorial.html">documentation</a> to truly appreciate the capabilities of Nbdev.</p>
<p>The original Nbdev supports the creation of Python projects from a jupyter notebook and it does really superb job at that but there is another popular language ecosystem and notebook environment which the original nbdev doesn‚Äôt support. That language is <a href="https://julialang.org/">Julia</a> and the notebook environment is <a href="https://github.com/fonsp/Pluto.jl">Pluto.jl</a>.</p>
<p>Pluto is a reactive notebook environment and differs from jupyter notebook environment in many ways. I would suggest you to explore the project <a href="https://github.com/fonsp/Pluto.jl">Readme</a> to get an idea about teh awsomeness of Pluto.</p>
<p>Pluto is a great exploratory developement platform and really love using it for my Julia project. I love Julia and python equally üòâ and I am big fan of Nbdev (python version) and use it extensively when I am developing projects in python. I always wanted a literate programming tool similar to Nbdev for Pluto/julia and while searching through the Julia forums found this <a href="https://discourse.julialang.org/t/nbdev-for-julia/32450">discussion</a> ‚Äì&gt;</p>
<p><img src="https://www.satyabratapal.xyz/posts/Nbdev.jl-And-Literate-programming/nbdev-discussion.png" class="img-fluid"></p>
<p>Scrolling down the discussion thread I could see that lot many folks were also in search of something similar. This is when I decided to do something in this direction and started working on porting Nbdev to Julia.</p>
</section>
<section id="nbdev.jl" class="level2">
<h2 class="anchored" data-anchor-id="nbdev.jl">Nbdev.jl</h2>
<p>Last month I released the alpha version of <a href="https://sapal6.github.io/Nbdev.jl/">Nbdev.jl</a>. With Nbdev.jl I hope to work towards building a developer tool which would do justice to the awesome user experience that is provided by the original Nbdev.</p>
<p>This is just the begining and there are lots of things that I need to fix and many things which are yet to come to Nbdev.jl. However, if you would like to take Nbdev.jl for a spin then follow the getting started tutorial [here])(https://sapal6.github.io/Nbdev.jl/tutorial/).</p>
<p>If you want to take part in the twitter discussion then hope onto this thread ‚Äì&gt;</p>
<blockquote class="twitter-tweet blockquote">
<p lang="en" dir="ltr">
I am happy to announce that I have released the Alpha version of Nbdev.jl (port of <a href="https://twitter.com/fastdotai?ref_src=twsrc%5Etfw"><span class="citation" data-cites="fastdotai">@fastdotai</span></a> 's Nbdev to <a href="https://twitter.com/hashtag/julialang?src=hash&amp;ref_src=twsrc%5Etfw">#julialang</a> ) . Follow the tutorial - <a href="https://t.co/BwLlUPw9KQ">https://t.co/BwLlUPw9KQ</a> to start experimenting with Nbdev. <br><br>A short üßµ on my thoughts üëá.
</p>
‚Äî Satyabrata pal (<span class="citation" data-cites="TheCodingProjec">@TheCodingProjec</span>) <a href="https://twitter.com/TheCodingProjec/status/1475354805987991555?ref_src=twsrc%5Etfw">December 27, 2021</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</section>
<section id="further-reads" class="level2">
<h2 class="anchored" data-anchor-id="further-reads">Further reads</h2>
<ul>
<li>http://ross.net/funnelweb/tutorial/intro_what.html</li>
<li>https://www.fast.ai/2019/12/02/nbdev/</li>
<li>https://nbdev.fast.ai/tutorial.html</li>
</ul>


</section>

 ]]></description>
  <category>Programming</category>
  <guid>https://www.satyabratapal.xyz/posts/Nbdev.jl-And-Literate-programming/Nbdev.jl-And-Literate-programming.html</guid>
  <pubDate>Wed, 05 Jan 2022 00:00:00 GMT</pubDate>
  <media:content url="https://www.satyabratapal.xyz/posts/Nbdev.jl-And-Literate-programming/notebooks.png" medium="image" type="image/png" height="65" width="144"/>
</item>
<item>
  <title>Extending-Fastai-For-Custom-Task</title>
  <dc:creator>Satyabrata pal</dc:creator>
  <link>https://www.satyabratapal.xyz/posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html</link>
  <description><![CDATA[ 



<section id="what-is-this" class="level2">
<h2 class="anchored" data-anchor-id="what-is-this">What is this? ü§î</h2>
<p>Experiment to weave nnaudio, timm and fastai together</p>
</section>
<section id="what-i-have-tried-to-explore-here" class="level2">
<h2 class="anchored" data-anchor-id="what-i-have-tried-to-explore-here">What I have tried to explore here? üîç</h2>
<p>üëâ Extend Fastai for signal processing and time series:</p>
<pre><code>* To use nnAudio for faster processing than librosa or other signal/audio processing methods.
* To deal with time series data as images.
* To deal with cosmological data like gravitational waves.1</code></pre>
<p>üëâ Create custom Transform.</p>
<p>üëâ Create custom block.</p>
<p>üëâ Create a dataloader.</p>
<p>üëâ Create a custom model with models from the timm library.</p>
<p>üëâ Create custom learner.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:38:56.545640Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:38:56.466750Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:38:56.572797Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:38:56.573224Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:00:52.319686Z&quot;}" data-papermill="{&quot;duration&quot;:0.187521,&quot;end_time&quot;:&quot;2021-10-01T09:38:56.573475&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:38:56.385954&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="op" style="color: #5E5E5E;">%</span>reload_ext autoreload</span>
<span id="cb2-2"><span class="op" style="color: #5E5E5E;">%</span>autoreload <span class="dv" style="color: #AD0000;">2</span></span>
<span id="cb2-3"><span class="op" style="color: #5E5E5E;">%</span>matplotlib inline</span></code></pre></div>
</details>
</div>
</section>
<section id="installing-all-the-required-libraries" class="level2">
<h2 class="anchored" data-anchor-id="installing-all-the-required-libraries">Installing all the required libraries‚öôÔ∏è</h2>
<p>üëâ Spacy</p>
<p>üëâ Fastai</p>
<p>üëâ nnAudio</p>
<p>üëâ timm</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:38:56.891945Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:38:56.891149Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:38:56.915871Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:38:56.915429Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:00:52.453769Z&quot;}" data-papermill="{&quot;duration&quot;:0.105598,&quot;end_time&quot;:&quot;2021-10-01T09:38:56.915991&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:38:56.810393&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="co" style="color: #5E5E5E;">#!pip install spacy==3.1.1</span></span></code></pre></div>
</details>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:38:57.074554Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:38:57.073802Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:42:33.772598Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:42:33.772085Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:00:52.483065Z&quot;}" data-papermill="{&quot;duration&quot;:216.779078,&quot;end_time&quot;:&quot;2021-10-01T09:42:33.772766&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:38:56.993688&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="co" style="color: #5E5E5E;">#!pip3 install torch==1.9.0 torchvision==0.10.0</span></span>
<span id="cb4-2"><span class="op" style="color: #5E5E5E;">!</span>pip3 install torch<span class="op" style="color: #5E5E5E;">==</span><span class="fl" style="color: #AD0000;">1.9.0</span><span class="op" style="color: #5E5E5E;">+</span>cu111 torchvision<span class="op" style="color: #5E5E5E;">==</span><span class="fl" style="color: #AD0000;">0.10.0</span><span class="op" style="color: #5E5E5E;">+</span>cu111 torchaudio<span class="op" style="color: #5E5E5E;">==</span><span class="fl" style="color: #AD0000;">0.9.0</span> <span class="op" style="color: #5E5E5E;">-</span>f https:<span class="op" style="color: #5E5E5E;">//</span>download.pytorch.org<span class="op" style="color: #5E5E5E;">/</span>whl<span class="op" style="color: #5E5E5E;">/</span>torch_stable.html</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Looking in links: https://download.pytorch.org/whl/torch_stable.html
Collecting torch==1.9.0+cu111
  Downloading https://download.pytorch.org/whl/cu111/torch-1.9.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (2041.3 MB)
     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2041.3 MB 3.1 kB/s 
Collecting torchvision==0.10.0+cu111
  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.10.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (23.2 MB)
     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23.2 MB 28.8 MB/s 
Collecting torchaudio==0.9.0
  Downloading torchaudio-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)
     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.9 MB 607 kB/s 
Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.9.0+cu111) (3.7.4.3)
Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision==0.10.0+cu111) (1.19.5)
Requirement already satisfied: pillow&gt;=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision==0.10.0+cu111) (8.2.0)
Installing collected packages: torch, torchvision, torchaudio
  Attempting uninstall: torch
    Found existing installation: torch 1.7.1+cu110
    Uninstalling torch-1.7.1+cu110:
      Successfully uninstalled torch-1.7.1+cu110
  Attempting uninstall: torchvision
    Found existing installation: torchvision 0.8.2+cu110
    Uninstalling torchvision-0.8.2+cu110:
      Successfully uninstalled torchvision-0.8.2+cu110
  Attempting uninstall: torchaudio
    Found existing installation: torchaudio 0.7.2
    Uninstalling torchaudio-0.7.2:
      Successfully uninstalled torchaudio-0.7.2
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
torchtext 0.8.1 requires torch==1.7.1, but you have torch 1.9.0+cu111 which is incompatible.
fastai 2.2.7 requires torch&lt;1.8,&gt;=1.7.0, but you have torch 1.9.0+cu111 which is incompatible.
fastai 2.2.7 requires torchvision&lt;0.9,&gt;=0.8, but you have torchvision 0.10.0+cu111 which is incompatible.
Successfully installed torch-1.9.0+cu111 torchaudio-0.9.0 torchvision-0.10.0+cu111
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv</code></pre>
</div>
</div>
<p>::: {.cell _kg_hide-input=‚Äòtrue‚Äô _kg_hide-output=‚Äòtrue‚Äô execution=‚Äò{‚Äúiopub.execute_input‚Äù:‚Äú2021-10-01T09:42:35.698110Z‚Äù,‚Äúiopub.status.busy‚Äù:‚Äú2021-10-01T09:42:35.697293Z‚Äù,‚Äúiopub.status.idle‚Äù:‚Äú2021-10-01T09:42:46.806039Z‚Äù,‚Äúshell.execute_reply‚Äù:‚Äú2021-10-01T09:42:46.805491Z‚Äù,‚Äúshell.execute_reply.started‚Äù:‚Äú2021-10-01T09:04:05.909260Z‚Äù}‚Äô papermill=‚Äò{‚Äúduration‚Äù:12.087095,‚Äúend_time‚Äù:‚Äú2021-10-01T09:42:46.806171‚Äù,‚Äúexception‚Äù:false,‚Äústart_time‚Äù:‚Äú2021-10-01T09:42:34.719076‚Äù,‚Äústatus‚Äù:‚Äúcompleted‚Äù}‚Äô tags=‚Äò[]‚Äô execution_count=4}</p>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="co" style="color: #5E5E5E;">#!yes Y|conda install -c fastai fastai=2.5.2</span></span>
<span id="cb6-2"><span class="op" style="color: #5E5E5E;">!</span>pip3 install fastai<span class="op" style="color: #5E5E5E;">==</span><span class="fl" style="color: #AD0000;">2.5.2</span></span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Collecting fastai==2.5.2
  Downloading fastai-2.5.2-py3-none-any.whl (186 kB)
     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 186 kB 15 kB/s 
Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from fastai==2.5.2) (1.7.1)
Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from fastai==2.5.2) (21.0)
Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from fastai==2.5.2) (5.4.1)
Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from fastai==2.5.2) (3.4.3)
Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from fastai==2.5.2) (2.25.1)
Requirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (from fastai==2.5.2) (21.2.4)
Requirement already satisfied: torchvision&gt;=0.8.2 in /opt/conda/lib/python3.7/site-packages (from fastai==2.5.2) (0.10.0+cu111)
Requirement already satisfied: pillow&gt;6.0.0 in /opt/conda/lib/python3.7/site-packages (from fastai==2.5.2) (8.2.0)
Collecting fastdownload&lt;2,&gt;=0.0.5
  Downloading fastdownload-0.0.5-py3-none-any.whl (13 kB)
Requirement already satisfied: spacy&lt;4 in /opt/conda/lib/python3.7/site-packages (from fastai==2.5.2) (2.3.7)
Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from fastai==2.5.2) (0.23.2)
Requirement already satisfied: torch&lt;1.10,&gt;=1.7.0 in /opt/conda/lib/python3.7/site-packages (from fastai==2.5.2) (1.9.0+cu111)
Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from fastai==2.5.2) (1.2.5)
Requirement already satisfied: fastprogress&gt;=0.2.4 in /opt/conda/lib/python3.7/site-packages (from fastai==2.5.2) (1.0.0)
Requirement already satisfied: fastcore&lt;1.4,&gt;=1.3.8 in /opt/conda/lib/python3.7/site-packages (from fastai==2.5.2) (1.3.26)
Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from fastprogress&gt;=0.2.4-&gt;fastai==2.5.2) (1.19.5)
Requirement already satisfied: blis&lt;0.8.0,&gt;=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai==2.5.2) (0.7.4)
Requirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai==2.5.2) (1.0.5)
Requirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai==2.5.2) (3.0.5)
Requirement already satisfied: wasabi&lt;1.1.0,&gt;=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai==2.5.2) (0.8.2)
Requirement already satisfied: thinc&lt;7.5.0,&gt;=7.4.1 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai==2.5.2) (7.4.5)
Requirement already satisfied: plac&lt;1.2.0,&gt;=0.9.6 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai==2.5.2) (1.1.3)
Requirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai==2.5.2) (2.0.5)
Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai==2.5.2) (57.4.0)
Requirement already satisfied: catalogue&lt;1.1.0,&gt;=0.0.7 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai==2.5.2) (1.0.0)
Requirement already satisfied: tqdm&lt;5.0.0,&gt;=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai==2.5.2) (4.62.1)
Requirement already satisfied: srsly&lt;1.1.0,&gt;=1.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai==2.5.2) (1.0.5)
Requirement already satisfied: importlib-metadata&gt;=0.20 in /opt/conda/lib/python3.7/site-packages (from catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy&lt;4-&gt;fastai==2.5.2) (3.4.0)
Requirement already satisfied: zipp&gt;=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata&gt;=0.20-&gt;catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy&lt;4-&gt;fastai==2.5.2) (3.5.0)
Requirement already satisfied: typing-extensions&gt;=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata&gt;=0.20-&gt;catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy&lt;4-&gt;fastai==2.5.2) (3.7.4.3)
Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests-&gt;fastai==2.5.2) (1.26.6)
Requirement already satisfied: chardet&lt;5,&gt;=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests-&gt;fastai==2.5.2) (4.0.0)
Requirement already satisfied: certifi&gt;=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests-&gt;fastai==2.5.2) (2021.5.30)
Requirement already satisfied: idna&lt;3,&gt;=2.5 in /opt/conda/lib/python3.7/site-packages (from requests-&gt;fastai==2.5.2) (2.10)
Requirement already satisfied: pyparsing&gt;=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib-&gt;fastai==2.5.2) (2.4.7)
Requirement already satisfied: kiwisolver&gt;=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib-&gt;fastai==2.5.2) (1.3.1)
Requirement already satisfied: python-dateutil&gt;=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib-&gt;fastai==2.5.2) (2.8.0)
Requirement already satisfied: cycler&gt;=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib-&gt;fastai==2.5.2) (0.10.0)
Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from cycler&gt;=0.10-&gt;matplotlib-&gt;fastai==2.5.2) (1.15.0)
Requirement already satisfied: pytz&gt;=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas-&gt;fastai==2.5.2) (2021.1)
Requirement already satisfied: joblib&gt;=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn-&gt;fastai==2.5.2) (1.0.1)
Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn-&gt;fastai==2.5.2) (2.2.0)
Installing collected packages: fastdownload, fastai
  Attempting uninstall: fastai
    Found existing installation: fastai 2.2.7
    Uninstalling fastai-2.2.7:
      Successfully uninstalled fastai-2.2.7
Successfully installed fastai-2.5.2 fastdownload-0.0.5
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv</code></pre>
</div>
<p>:::</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:42:48.716907Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:42:48.716329Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:42:57.167868Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:42:57.166933Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:04:15.012505Z&quot;}" data-papermill="{&quot;duration&quot;:9.41224,&quot;end_time&quot;:&quot;2021-10-01T09:42:57.168010&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:42:47.755770&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="5">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="op" style="color: #5E5E5E;">!</span>pip3 install timm</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Collecting timm
  Downloading timm-0.4.12-py3-none-any.whl (376 kB)
     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 376 kB 607 kB/s 
Requirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from timm) (0.10.0+cu111)
Requirement already satisfied: torch&gt;=1.4 in /opt/conda/lib/python3.7/site-packages (from timm) (1.9.0+cu111)
Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch&gt;=1.4-&gt;timm) (3.7.4.3)
Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision-&gt;timm) (1.19.5)
Requirement already satisfied: pillow&gt;=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision-&gt;timm) (8.2.0)
Installing collected packages: timm
Successfully installed timm-0.4.12
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv</code></pre>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:42:59.055673Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:42:59.054872Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:43:06.308125Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:43:06.307562Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:04:22.811880Z&quot;}" data-papermill="{&quot;duration&quot;:8.200742,&quot;end_time&quot;:&quot;2021-10-01T09:43:06.308255&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:42:58.107513&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="6">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="op" style="color: #5E5E5E;">!</span>pip3 install nnaudio</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Collecting nnaudio
  Downloading nnAudio-0.2.6-py3-none-any.whl (30 kB)
Installing collected packages: nnaudio
Successfully installed nnaudio-0.2.6
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv</code></pre>
</div>
</div>
</section>
<section id="import-all-required-modules" class="level2">
<h2 class="anchored" data-anchor-id="import-all-required-modules">Import all required modulesüñ•Ô∏è</h2>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:43:10.071274Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:43:10.070406Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:43:12.901932Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:43:12.901421Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:04:30.265649Z&quot;}" data-papermill="{&quot;duration&quot;:3.776142,&quot;end_time&quot;:&quot;2021-10-01T09:43:12.902074&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:43:09.125932&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="7">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="co" style="color: #5E5E5E;">#export</span></span>
<span id="cb12-2"><span class="im" style="color: #00769E;">from</span> typing <span class="im" style="color: #00769E;">import</span> Tuple</span>
<span id="cb12-3"><span class="im" style="color: #00769E;">from</span> collections <span class="im" style="color: #00769E;">import</span> namedtuple</span>
<span id="cb12-4"><span class="im" style="color: #00769E;">from</span> nnAudio.Spectrogram <span class="im" style="color: #00769E;">import</span> CQT</span>
<span id="cb12-5"><span class="im" style="color: #00769E;">from</span> timm <span class="im" style="color: #00769E;">import</span> create_model, list_models</span>
<span id="cb12-6"><span class="im" style="color: #00769E;">from</span> pandas.core.frame <span class="im" style="color: #00769E;">import</span> DataFrame</span>
<span id="cb12-7"><span class="im" style="color: #00769E;">from</span> fastcore.foundation <span class="im" style="color: #00769E;">import</span> <span class="op" style="color: #5E5E5E;">*</span></span>
<span id="cb12-8"><span class="im" style="color: #00769E;">from</span> fastai.vision.<span class="bu" style="color: null;">all</span> <span class="im" style="color: #00769E;">import</span> <span class="op" style="color: #5E5E5E;">*</span></span>
<span id="cb12-9"><span class="im" style="color: #00769E;">from</span> fastai.torch_core <span class="im" style="color: #00769E;">import</span> show_image</span>
<span id="cb12-10"><span class="im" style="color: #00769E;">from</span> fastai.vision.learner <span class="im" style="color: #00769E;">import</span> _update_first_layer</span></code></pre></div>
</details>
</div>
</section>
<section id="get-the-files" class="level2">
<h2 class="anchored" data-anchor-id="get-the-files">Get the filesüèóÔ∏è</h2>
<p>I will try to grab all the numpy files inside train folder</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:43:16.624045Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:43:16.623223Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:43:16.665213Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:43:16.664711Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:04:33.139175Z&quot;}" data-papermill="{&quot;duration&quot;:0.973764,&quot;end_time&quot;:&quot;2021-10-01T09:43:16.665350&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:43:15.691586&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="8">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">path <span class="op" style="color: #5E5E5E;">=</span> Path(<span class="st" style="color: #20794D;">"../input"</span>)</span></code></pre></div>
</details>
</div>
<section id="get-labels" class="level3">
<h3 class="anchored" data-anchor-id="get-labels">Get labelsüèóÔ∏è</h3>
<p>Training labels are in the ‚Äòtraining_labels.csv‚Äô file.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:43:20.438161Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:43:20.437410Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:43:20.823245Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:43:20.822246Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:04:33.212365Z&quot;}" data-papermill="{&quot;duration&quot;:1.359852,&quot;end_time&quot;:&quot;2021-10-01T09:43:20.823386&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:43:19.463534&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="9">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">df <span class="op" style="color: #5E5E5E;">=</span> pd.read_csv(path<span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'g2net-gravitational-wave-detection/training_labels.csv'</span>)</span></code></pre></div>
</details>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:43:22.913372Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:43:22.912529Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:43:22.971088Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:43:22.971486Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:04:33.744909Z&quot;}" data-papermill="{&quot;duration&quot;:1.225638,&quot;end_time&quot;:&quot;2021-10-01T09:43:22.971625&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:43:21.745987&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="10">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1">df.head(<span class="dv" style="color: #AD0000;">1</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="10">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>id</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>00000e74ad</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
</section>
</section>
<section id="getfilespath-path-ext" class="level2">
<h2 class="anchored" data-anchor-id="getfilespath-path-ext">getfiles(path: Path, ext)</h2>
<p>Get numpy files in <code>path</code> recursively, only in <code>folders</code>, if specified.</p>
<blockquote class="blockquote">
<p>The ‚Äú#export‚Äù in the function below and all the rest of the functions/code are there to help me use nbdev to export the required code into a library later.</p>
</blockquote>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:43:26.746451Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:43:26.745612Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:43:26.779860Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:43:26.780227Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:04:33.811055Z&quot;}" data-papermill="{&quot;duration&quot;:1.006944,&quot;end_time&quot;:&quot;2021-10-01T09:43:26.780383&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:43:25.773439&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="11">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><span class="co" style="color: #5E5E5E;">#export</span></span>
<span id="cb16-2"><span class="kw" style="color: #003B4F;">def</span> getfiles(path: Path, ext) <span class="op" style="color: #5E5E5E;">-&gt;</span> L:</span>
<span id="cb16-3">    <span class="co" style="color: #5E5E5E;">"Get numpy files in `path` recursively, only in `folders`, if specified."</span></span>
<span id="cb16-4">    <span class="cf" style="color: #003B4F;">return</span> L(path.glob(<span class="ss" style="color: #20794D;">f'**/*.</span><span class="sc" style="color: #5E5E5E;">{</span>ext<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">'</span>))</span></code></pre></div>
</details>
</div>
<p>I am using the previous function to get all the files under the train folder.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:43:31.069529Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:43:31.068612Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:43:31.106901Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:43:31.106458Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:04:33.853413Z&quot;}" data-papermill="{&quot;duration&quot;:1.101897,&quot;end_time&quot;:&quot;2021-10-01T09:43:31.107030&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:43:30.005133&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="12">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1">train_path <span class="op" style="color: #5E5E5E;">=</span> path<span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'g2net-gravitational-wave-detection/train'</span></span></code></pre></div>
</details>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:43:32.988636Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:43:32.987842Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:45:35.388858Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:45:35.389479Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:04:33.892954Z&quot;}" data-papermill="{&quot;duration&quot;:123.362642,&quot;end_time&quot;:&quot;2021-10-01T09:45:35.389688&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:43:32.027046&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="13">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><span class="op" style="color: #5E5E5E;">%%</span>time</span>
<span id="cb18-2">train_files <span class="op" style="color: #5E5E5E;">=</span> getfiles(train_path, <span class="st" style="color: #20794D;">"npy"</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>CPU times: user 5.74 s, sys: 2.23 s, total: 7.97 s
Wall time: 2min 1s</code></pre>
</div>
</div>
<p>Just a quick test to see if we got the correct files.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:45:39.698395Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:45:39.697512Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:45:39.740570Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:45:39.740155Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:19.800983Z&quot;}" data-papermill="{&quot;duration&quot;:1.510079,&quot;end_time&quot;:&quot;2021-10-01T09:45:39.740726&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:45:38.230647&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="14">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1">train_files[:<span class="dv" style="color: #AD0000;">2</span>]</span></code></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>(#2) [Path('../input/g2net-gravitational-wave-detection/train/7/7/7/777d746e90.npy'),Path('../input/g2net-gravitational-wave-detection/train/7/7/7/777ecfbd65.npy')]</code></pre>
</div>
</div>
<p>Picking labels from the dataframe. We may need these labels later.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:45:43.623558Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:45:43.622726Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:45:43.674748Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:45:43.675157Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:19.845785Z&quot;}" data-papermill="{&quot;duration&quot;:0.988765,&quot;end_time&quot;:&quot;2021-10-01T09:45:43.675308&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:45:42.686543&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="15">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1">labels <span class="op" style="color: #5E5E5E;">=</span> df.target.to_list()</span></code></pre></div>
</details>
</div>
</section>
<section id="map-path-to-labels" class="level2">
<h2 class="anchored" data-anchor-id="map-path-to-labels">Map path to labelsüó∫Ô∏è</h2>
<p>To make things easier I will try to map the file paths to their respective labels and create a datafrane out of it.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:45:47.649911Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:45:47.649081Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:45:47.688794Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:45:47.688289Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:19.899274Z&quot;}" data-papermill="{&quot;duration&quot;:0.977258,&quot;end_time&quot;:&quot;2021-10-01T09:45:47.688967&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:45:46.711709&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="16">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><span class="co" style="color: #5E5E5E;">#export</span></span>
<span id="cb23-2"><span class="kw" style="color: #003B4F;">def</span> map_path_to_labels(data: L, cols: L<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span> ) <span class="op" style="color: #5E5E5E;">-&gt;</span> DataFrame:</span>
<span id="cb23-3">    <span class="co" style="color: #5E5E5E;">"""maps the files to their labels"""</span></span>
<span id="cb23-4">    <span class="cf" style="color: #003B4F;">if</span> cols <span class="kw" style="color: #003B4F;">is</span> <span class="va" style="color: #111111;">None</span>: <span class="cf" style="color: #003B4F;">raise</span> <span class="pp" style="color: #AD0000;">ValueError</span>(<span class="st" style="color: #20794D;">"You forgot to provide the columns"</span>)</span>
<span id="cb23-5">    data <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">dict</span>(<span class="bu" style="color: null;">zip</span>(cols, data))</span>
<span id="cb23-6">    <span class="cf" style="color: #003B4F;">return</span> pd.DataFrame.from_dict(data)</span></code></pre></div>
</details>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:45:49.614450Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:45:49.610414Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:45:51.727711Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:45:51.728094Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:19.940760Z&quot;}" data-papermill="{&quot;duration&quot;:3.079305,&quot;end_time&quot;:&quot;2021-10-01T09:45:51.728245&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:45:48.648940&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="17">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><span class="op" style="color: #5E5E5E;">%%</span>time</span>
<span id="cb24-2">df <span class="op" style="color: #5E5E5E;">=</span> map_path_to_labels([train_files, labels], cols<span class="op" style="color: #5E5E5E;">=</span>[<span class="st" style="color: #20794D;">"id"</span>, <span class="st" style="color: #20794D;">"target"</span>])</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>CPU times: user 2.07 s, sys: 0 ns, total: 2.07 s
Wall time: 2.07 s</code></pre>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:45:53.611382Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:45:53.610589Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:45:53.654868Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:45:53.655271Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:22.121645Z&quot;}" data-papermill="{&quot;duration&quot;:0.977578,&quot;end_time&quot;:&quot;2021-10-01T09:45:53.655495&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:45:52.677917&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="18">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb26" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1">df.head(<span class="dv" style="color: #AD0000;">1</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="18">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>id</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>../input/g2net-gravitational-wave-detection/train/7/7/7/777d746e90.npy</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
</section>
<section id="mapxydf" class="level2">
<h2 class="anchored" data-anchor-id="mapxydf">mapxy(df)</h2>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:45:57.616651Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:45:57.615772Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:45:57.654544Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:45:57.654140Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:22.174400Z&quot;}" data-papermill="{&quot;duration&quot;:1.095784,&quot;end_time&quot;:&quot;2021-10-01T09:45:57.654677&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:45:56.558893&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="19">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><span class="co" style="color: #5E5E5E;">#export</span></span>
<span id="cb27-2"><span class="kw" style="color: #003B4F;">def</span> mapxy(df: DataFrame):</span>
<span id="cb27-3">    <span class="co" style="color: #5E5E5E;">"""Create a dictionary of file path and the label from a dataframe"""</span></span>
<span id="cb27-4">    <span class="cf" style="color: #003B4F;">return</span> <span class="bu" style="color: null;">dict</span>(df.values)</span></code></pre></div>
</details>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:45:59.555896Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:45:59.555045Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:46:00.464911Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:46:00.465345Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:22.218681Z&quot;}" data-papermill="{&quot;duration&quot;:1.859472,&quot;end_time&quot;:&quot;2021-10-01T09:46:00.465494&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:45:58.606022&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="20">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb28" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><span class="op" style="color: #5E5E5E;">%%</span>time</span>
<span id="cb28-2">files <span class="op" style="color: #5E5E5E;">=</span> mapxy(df)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>CPU times: user 872 ms, sys: 0 ns, total: 872 ms
Wall time: 872 ms</code></pre>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:46:02.347883Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:46:02.347250Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:46:02.391725Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:46:02.392170Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:23.234763Z&quot;}" data-papermill="{&quot;duration&quot;:0.985177,&quot;end_time&quot;:&quot;2021-10-01T09:46:02.392332&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:46:01.407155&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="21">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb30" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><span class="op" style="color: #5E5E5E;">%%</span>time</span>
<span id="cb30-2">files[train_files[<span class="dv" style="color: #AD0000;">7</span>]]</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>CPU times: user 11 ¬µs, sys: 7 ¬µs, total: 18 ¬µs
Wall time: 21.5 ¬µs</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>1</code></pre>
</div>
</div>
</section>
<section id="get_labelf-path" class="level2">
<h2 class="anchored" data-anchor-id="get_labelf-path">get_label(f: Path)</h2>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:46:06.126187Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:46:06.124462Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:46:06.167916Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:46:06.168359Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:23.298090Z&quot;}" data-papermill="{&quot;duration&quot;:0.979043,&quot;end_time&quot;:&quot;2021-10-01T09:46:06.168512&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:46:05.189469&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="22">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><span class="kw" style="color: #003B4F;">def</span> get_label(f: Path):</span>
<span id="cb33-2">    <span class="co" style="color: #5E5E5E;">"""get the label belonging to a file"""</span></span>
<span id="cb33-3">    label <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">None</span></span>
<span id="cb33-4">    <span class="cf" style="color: #003B4F;">if</span> f <span class="kw" style="color: #003B4F;">in</span> df.values:</span>
<span id="cb33-5">        label <span class="op" style="color: #5E5E5E;">=</span> df[df[<span class="st" style="color: #20794D;">'id'</span>] <span class="op" style="color: #5E5E5E;">==</span> f][<span class="st" style="color: #20794D;">'target'</span>].values[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb33-6">    <span class="cf" style="color: #003B4F;">return</span> label</span></code></pre></div>
</details>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:46:08.282748Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:46:08.281790Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:46:09.163609Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:46:09.164278Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:23.360251Z&quot;}" data-papermill="{&quot;duration&quot;:2.064649,&quot;end_time&quot;:&quot;2021-10-01T09:46:09.164438&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:46:07.099789&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="23">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb34" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1">get_label(train_files[<span class="dv" style="color: #AD0000;">0</span>])</span></code></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>1</code></pre>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:46:11.055758Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:46:11.053882Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:46:11.628856Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:46:11.629395Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:24.362308Z&quot;}" data-papermill="{&quot;duration&quot;:1.514773,&quot;end_time&quot;:&quot;2021-10-01T09:46:11.629617&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:46:10.114844&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="24">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb36" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1">get_label(Path(<span class="st" style="color: #20794D;">"/ff/ff.npy"</span>))</span></code></pre></div>
</details>
</div>
</section>
<section id="q-transform-using-nnaudio" class="level2">
<h2 class="anchored" data-anchor-id="q-transform-using-nnaudio">Q transform using nnaudio‚öóÔ∏è</h2>
<p>We will design a function that would get the q transform of the time series on the fly using nnaudio. The result will be similar to converting the time series data into images.</p>
<p>Code taken from <a href="https://www.kaggle.com/yasufuminakama/g2net-efficientnet-b7-baseline-training">notebook</a> shared by <a href="https://www.kaggle.com/yasufuminakama">Y.Nakama</a></p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:46:15.798420Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:46:15.797586Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:46:15.839178Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:46:15.838741Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:24.876416Z&quot;}" data-papermill="{&quot;duration&quot;:0.974233,&quot;end_time&quot;:&quot;2021-10-01T09:46:15.839302&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:46:14.865069&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="25">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb37" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><span class="co" style="color: #5E5E5E;">#export</span></span>
<span id="cb37-2"><span class="kw" style="color: #003B4F;">def</span> qtfm():</span>
<span id="cb37-3">    <span class="co" style="color: #5E5E5E;">"""convert waves to images"""</span></span>
<span id="cb37-4">    cqt <span class="op" style="color: #5E5E5E;">=</span> CQT(sr<span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">2048</span>, fmin<span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">20</span>, fmax<span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">1024</span>, hop_length<span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">32</span>, bins_per_octave<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">8</span>,verbose<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span>
<span id="cb37-5">    <span class="cf" style="color: #003B4F;">return</span> cqt</span></code></pre></div>
</details>
</div>
<p><strong>NOTE</strong></p>
<p>Remember to set <code>verbose</code> False if you don‚Äôt want all the string output to be displayed everytime dataloader loads the data.</p>
<p>Quick test to see if this works.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:46:21.724126Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:46:21.723386Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:46:21.781980Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:46:21.781493Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:24.923367Z&quot;}" data-papermill="{&quot;duration&quot;:1.023866,&quot;end_time&quot;:&quot;2021-10-01T09:46:21.782103&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:46:20.758237&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="26">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb38" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1">waves <span class="op" style="color: #5E5E5E;">=</span> np.load(train_files[<span class="dv" style="color: #AD0000;">0</span>])</span>
<span id="cb38-2">waves <span class="op" style="color: #5E5E5E;">=</span> np.hstack(waves)</span>
<span id="cb38-3">waves <span class="op" style="color: #5E5E5E;">=</span> waves <span class="op" style="color: #5E5E5E;">/</span> np.<span class="bu" style="color: null;">max</span>(waves)</span>
<span id="cb38-4">waves <span class="op" style="color: #5E5E5E;">=</span> torch.from_numpy(waves).<span class="bu" style="color: null;">float</span>()</span></code></pre></div>
</details>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:46:23.638706Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:46:23.637241Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:46:23.716699Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:46:23.716076Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:24.981234Z&quot;}" data-papermill="{&quot;duration&quot;:1.012117,&quot;end_time&quot;:&quot;2021-10-01T09:46:23.716907&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:46:22.704790&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="27">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb39" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><span class="op" style="color: #5E5E5E;">%%</span>time</span>
<span id="cb39-2">qtfm()(waves)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>CPU times: user 20.5 ms, sys: 957 ¬µs, total: 21.5 ms
Wall time: 26.3 ms</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored
  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="27">
<pre><code>tensor([[[0.3868, 0.3814, 0.3670,  ..., 0.1262, 0.1258, 0.1258],
         [0.3012, 0.2917, 0.2653,  ..., 0.1504, 0.1565, 0.1585],
         [0.2445, 0.2453, 0.2446,  ..., 0.0945, 0.0961, 0.0965],
         ...,
         [0.0027, 0.0040, 0.0072,  ..., 0.0018, 0.0076, 0.0014],
         [0.0027, 0.0040, 0.0120,  ..., 0.0012, 0.0061, 0.0126],
         [0.0039, 0.0140, 0.0172,  ..., 0.0010, 0.0064, 0.0160]]])</code></pre>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:46:25.601583Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:46:25.596457Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:46:30.895253Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:46:30.895860Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:25.077077Z&quot;}" data-papermill="{&quot;duration&quot;:6.241553,&quot;end_time&quot;:&quot;2021-10-01T09:46:30.896058&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:46:24.654505&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="28">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb43" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><span class="op" style="color: #5E5E5E;">%%</span>time</span>
<span id="cb43-2"><span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">5</span>):</span>
<span id="cb43-3">    waves <span class="op" style="color: #5E5E5E;">=</span> np.load(df.iloc[i].<span class="bu" style="color: null;">id</span>)</span>
<span id="cb43-4">    waves <span class="op" style="color: #5E5E5E;">=</span> np.hstack(waves)</span>
<span id="cb43-5">    waves <span class="op" style="color: #5E5E5E;">=</span> waves <span class="op" style="color: #5E5E5E;">/</span> np.<span class="bu" style="color: null;">max</span>(waves)</span>
<span id="cb43-6">    waves <span class="op" style="color: #5E5E5E;">=</span> torch.from_numpy(waves).<span class="bu" style="color: null;">float</span>()</span>
<span id="cb43-7">    image <span class="op" style="color: #5E5E5E;">=</span> qtfm()(waves)</span>
<span id="cb43-8">    target <span class="op" style="color: #5E5E5E;">=</span> get_label(train_files[i])</span>
<span id="cb43-9">    plt.imshow(image[<span class="dv" style="color: #AD0000;">0</span>])</span>
<span id="cb43-10">    plt.title(<span class="ss" style="color: #20794D;">f"target: </span><span class="sc" style="color: #5E5E5E;">{</span>target<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span>
<span id="cb43-11">    plt.show()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task_files/figure-html/cell-29-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task_files/figure-html/cell-29-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task_files/figure-html/cell-29-output-3.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task_files/figure-html/cell-29-output-4.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task_files/figure-html/cell-29-output-5.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>CPU times: user 5.37 s, sys: 391 ms, total: 5.77 s
Wall time: 5.23 s</code></pre>
</div>
</div>
<p>Cool! so we are able to plot the images now. IT is fast too.</p>
</section>
<section id="creating-the-dataset" class="level2">
<h2 class="anchored" data-anchor-id="creating-the-dataset">Creating the datasetüñ´</h2>
<p>If you want to use fastai‚Äôs learner to train your model on the transfomed spectograms, you can do so by creating a custom Dataset in pytorch and then feeding that dataset with a dataloader to fastai‚Äôs learner. However, if you create a pipeline using fastai‚Äôs internals then you get to use some cool functionalities out-of-box. We will see that in a while.</p>
<p>All the code below are very heavily insipired by the original inspiration of this notebook (see the very first section), this <a href="https://ohmeow.com/posts/2020/04/11/finding-datablock-nirvana-part-1.html">post</a> by Wayde Gilliam and the fastai s<a href="https://docs.fast.ai/tutorial.siamese.html#Writing-your-custom-data-block">iamese tutorial</a>.</p>
</section>
<section id="spectogram" class="level2">
<h2 class="anchored" data-anchor-id="spectogram">Spectogram</h2>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:46:38.635390Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:46:38.634529Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:46:38.672205Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:46:38.671761Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:30.406874Z&quot;}" data-papermill="{&quot;duration&quot;:1.040374,&quot;end_time&quot;:&quot;2021-10-01T09:46:38.672322&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:46:37.631948&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="29">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb45" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><span class="co" style="color: #5E5E5E;">#export</span></span>
<span id="cb45-2"><span class="kw" style="color: #003B4F;">def</span> get_waves(f):</span>
<span id="cb45-3">    <span class="co" style="color: #5E5E5E;">"""read numpy file, stack the timeseries and convert those into a tensor"""</span></span>
<span id="cb45-4">    waves <span class="op" style="color: #5E5E5E;">=</span> np.load(f)</span>
<span id="cb45-5">    waves <span class="op" style="color: #5E5E5E;">=</span> np.hstack(waves)</span>
<span id="cb45-6">    waves <span class="op" style="color: #5E5E5E;">=</span> waves <span class="op" style="color: #5E5E5E;">/</span> np.<span class="bu" style="color: null;">max</span>(waves)</span>
<span id="cb45-7">    waves <span class="op" style="color: #5E5E5E;">=</span> torch.from_numpy(waves).<span class="bu" style="color: null;">float</span>()</span>
<span id="cb45-8">    <span class="cf" style="color: #003B4F;">return</span> waves</span>
<span id="cb45-9"></span>
<span id="cb45-10"><span class="kw" style="color: #003B4F;">def</span> create_spectrogram(x: Path):</span>
<span id="cb45-11">    <span class="co" style="color: #5E5E5E;">"""Create an AudioSpectrogram from a torch tensor"""</span></span>
<span id="cb45-12">    waves <span class="op" style="color: #5E5E5E;">=</span> get_waves(x)</span>
<span id="cb45-13">    <span class="cf" style="color: #003B4F;">return</span> qtfm()(waves)</span></code></pre></div>
</details>
</div>
</section>
<section id="nnaudioimagefastuple" class="level2">
<h2 class="anchored" data-anchor-id="nnaudioimagefastuple">NNAudioImage(fastuple)</h2>
<p>First of all, we are going to create an ‚ÄúImage type‚Äù for our transformed object (it‚Äôs the numpy data transformed into spectrogram).</p>
<p>We have to do this because our data is not an image data from get-go. Rather it‚Äôs a signal data which we are transforming into an Image. So, to tell fastai that this is a custom Image type which we are dealing with and ho we should be displaying it, we have to create an Image type.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:46:42.640974Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:46:42.640100Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:46:42.682156Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:46:42.682564Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:30.452342Z&quot;}" data-papermill="{&quot;duration&quot;:0.976227,&quot;end_time&quot;:&quot;2021-10-01T09:46:42.682724&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:46:41.706497&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="30">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb46" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><span class="co" style="color: #5E5E5E;">#export</span></span>
<span id="cb46-2"><span class="kw" style="color: #003B4F;">class</span> AudioImage(fastuple):</span>
<span id="cb46-3">    <span class="co" style="color: #5E5E5E;">"""Custom Image for nnAudio transformed signals"""</span></span>
<span id="cb46-4">    <span class="kw" style="color: #003B4F;">def</span> show(<span class="va" style="color: #111111;">self</span>, figsize<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>, ctx<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>, <span class="op" style="color: #5E5E5E;">**</span>kwargs):</span>
<span id="cb46-5">        <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">len</span>(<span class="va" style="color: #111111;">self</span>) <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="dv" style="color: #AD0000;">1</span>:</span>
<span id="cb46-6">            img,label <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span></span>
<span id="cb46-7">        <span class="cf" style="color: #003B4F;">else</span>:</span>
<span id="cb46-8">            img <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span></span>
<span id="cb46-9">            label <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">''</span></span>
<span id="cb46-10">    </span>
<span id="cb46-11">        <span class="cf" style="color: #003B4F;">if</span> figsize <span class="kw" style="color: #003B4F;">is</span> <span class="va" style="color: #111111;">None</span>: figsize<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">10</span>,<span class="dv" style="color: #AD0000;">10</span>)</span>
<span id="cb46-12">        <span class="cf" style="color: #003B4F;">return</span> show_image(img, </span>
<span id="cb46-13">                          title<span class="op" style="color: #5E5E5E;">=</span>label, figsize<span class="op" style="color: #5E5E5E;">=</span>figsize, ctx<span class="op" style="color: #5E5E5E;">=</span>ctx)</span></code></pre></div>
</details>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:46:44.745564Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:46:44.744856Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:46:44.806208Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:46:44.806594Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:30.498223Z&quot;}" data-papermill="{&quot;duration&quot;:1.171959,&quot;end_time&quot;:&quot;2021-10-01T09:46:44.806779&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:46:43.634820&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="31">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb47" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1">image <span class="op" style="color: #5E5E5E;">=</span> create_spectrogram(train_files[<span class="dv" style="color: #AD0000;">0</span>])</span></code></pre></div>
</details>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:46:47.017284Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:46:47.016481Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:46:47.057037Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:46:47.057452Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:30.555723Z&quot;}" data-papermill="{&quot;duration&quot;:0.986475,&quot;end_time&quot;:&quot;2021-10-01T09:46:47.057591&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:46:46.071116&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="32">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb48" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><span class="bu" style="color: null;">type</span>(image), image.shape</span></code></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="32">
<pre><code>(torch.Tensor, torch.Size([1, 46, 385]))</code></pre>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:46:48.987180Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:46:48.986310Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:46:49.029908Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:46:49.030272Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:30.599104Z&quot;}" data-papermill="{&quot;duration&quot;:1.022134,&quot;end_time&quot;:&quot;2021-10-01T09:46:49.030426&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:46:48.008292&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="33">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb50" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1">image</span></code></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="33">
<pre><code>tensor([[[0.3868, 0.3814, 0.3670,  ..., 0.1262, 0.1258, 0.1258],
         [0.3012, 0.2917, 0.2653,  ..., 0.1504, 0.1565, 0.1585],
         [0.2445, 0.2453, 0.2446,  ..., 0.0945, 0.0961, 0.0965],
         ...,
         [0.0027, 0.0040, 0.0072,  ..., 0.0018, 0.0076, 0.0014],
         [0.0027, 0.0040, 0.0120,  ..., 0.0012, 0.0061, 0.0126],
         [0.0039, 0.0140, 0.0172,  ..., 0.0010, 0.0064, 0.0160]]])</code></pre>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:46:50.926387Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:46:50.924709Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:46:52.005861Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:46:52.005428Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:30.643874Z&quot;}" data-papermill="{&quot;duration&quot;:2.041741,&quot;end_time&quot;:&quot;2021-10-01T09:46:52.006020&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:46:49.964279&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="34">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb52" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1">get_label(train_files[<span class="dv" style="color: #AD0000;">0</span>])</span></code></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="34">
<pre><code>1</code></pre>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:46:53.912432Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:46:53.911561Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:46:54.860807Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:46:54.862093Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:31.542498Z&quot;}" data-papermill="{&quot;duration&quot;:1.915439,&quot;end_time&quot;:&quot;2021-10-01T09:46:54.862354&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:46:52.946915&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="35">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb54" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1">s <span class="op" style="color: #5E5E5E;">=</span> AudioImage(image, get_label(train_files[<span class="dv" style="color: #AD0000;">0</span>]))</span>
<span id="cb54-2">s.show()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="35">
<pre><code>&lt;AxesSubplot:title={'center':'1'}&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task_files/figure-html/cell-36-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="nnaudiodataset" class="level2">
<h2 class="anchored" data-anchor-id="nnaudiodataset">NNAudioDataset</h2>
<p>You can create a Dataset in fastai by creating a custom Transform . Creating a <code>Transform</code> has come advantages as compared to a pytorch Dataset. For example, you don‚Äôt need to have a <code>len</code> component or a <code>get_item</code> component.</p>
<p>On a very high level a <code>Transform</code> has an <code>encodes</code>, <code>decodes</code> and <code>setup</code> methods. For our purpose having an <code>encodes</code> methods only would suffice. This is the place where we would be transforming the numpy data into spectograms.</p>
<p>To know more about <code>Tranforms</code> refer these ‚Äì&gt; * <a href="https://ohmeow.com/posts/2020/04/11/finding-datablock-nirvana-part-1.html">data block nirvana</a> * <a href="https://docs.fast.ai/tutorial.siamese.html#Writing-your-custom-data-block">Siamese tutorial</a> * <a href="https://github.com/fastai/fastbook/blob/master/11_midlevel_data.ipynb">Fastbook chapter-11</a> * <a href="https://docs.fast.ai/tutorial.albumentations.html">Albumentation tutorial</a></p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:46:58.692060Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:46:58.691044Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:46:58.741958Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:46:58.742403Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:32.539202Z&quot;}" data-papermill="{&quot;duration&quot;:0.997913,&quot;end_time&quot;:&quot;2021-10-01T09:46:58.742549&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:46:57.744636&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="36">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb56" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><span class="op" style="color: #5E5E5E;">%%</span>time</span>
<span id="cb56-2">vals <span class="op" style="color: #5E5E5E;">=</span> df.target.to_list()</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>CPU times: user 9.73 ms, sys: 0 ns, total: 9.73 ms
Wall time: 9.74 ms</code></pre>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:47:00.797005Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:47:00.795629Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:47:00.854097Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:47:00.854636Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:32.614154Z&quot;}" data-papermill="{&quot;duration&quot;:1.023585,&quot;end_time&quot;:&quot;2021-10-01T09:47:00.854837&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:46:59.831252&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="37">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb58" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><span class="op" style="color: #5E5E5E;">%%</span>time</span>
<span id="cb58-2">vocab,o2i <span class="op" style="color: #5E5E5E;">=</span> uniqueify(vals, sort<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, bidir<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>CPU times: user 12.7 ms, sys: 0 ns, total: 12.7 ms
Wall time: 12.8 ms</code></pre>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:47:02.943981Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:47:02.943083Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:47:03.830530Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:47:03.829980Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:32.671726Z&quot;}" data-papermill="{&quot;duration&quot;:2.044575,&quot;end_time&quot;:&quot;2021-10-01T09:47:03.830671&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:47:01.786096&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="38">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb60" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1">lbl2path <span class="op" style="color: #5E5E5E;">=</span> mapxy(df)</span></code></pre></div>
</details>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:47:05.897567Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:47:05.895956Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:47:05.946935Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:47:05.947312Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:33.587967Z&quot;}" data-papermill="{&quot;duration&quot;:1.035406,&quot;end_time&quot;:&quot;2021-10-01T09:47:05.947473&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:47:04.912067&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="39">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb61" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><span class="op" style="color: #5E5E5E;">%%</span>time</span>
<span id="cb61-2">lbl2path[train_files[<span class="dv" style="color: #AD0000;">5</span>]]</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>CPU times: user 13 ¬µs, sys: 5 ¬µs, total: 18 ¬µs
Wall time: 21.5 ¬µs</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="39">
<pre><code>1</code></pre>
</div>
</div>
<p>combining all above steps</p>
</section>
<section id="nnaudiotransformitemtransform" class="level2">
<h2 class="anchored" data-anchor-id="nnaudiotransformitemtransform">NNAudioTransform(ItemTransform)</h2>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:47:11.625588Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:47:11.624772Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:47:11.665630Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:47:11.665229Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:33.636217Z&quot;}" data-papermill="{&quot;duration&quot;:1.027127,&quot;end_time&quot;:&quot;2021-10-01T09:47:11.665767&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:47:10.638640&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="40">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb64" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><span class="co" style="color: #5E5E5E;">#export</span></span>
<span id="cb64-2"><span class="co" style="color: #5E5E5E;">#ItemTransform let's you work with tuple elements</span></span>
<span id="cb64-3"><span class="kw" style="color: #003B4F;">class</span> NNAudioTransform(ItemTransform):</span>
<span id="cb64-4">    <span class="co" style="color: #5E5E5E;">"""Custom Transform which uses nnAudio transforms</span></span>
<span id="cb64-5"><span class="co" style="color: #5E5E5E;">    to extract spectogram on the fly"""</span></span>
<span id="cb64-6">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, df: DataFrame, col: <span class="bu" style="color: null;">str</span> <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'target'</span>):</span>
<span id="cb64-7">        <span class="va" style="color: #111111;">self</span>.lbl2files <span class="op" style="color: #5E5E5E;">=</span> mapxy(df)</span>
<span id="cb64-8">        vals <span class="op" style="color: #5E5E5E;">=</span> df[col].to_list()</span>
<span id="cb64-9">        <span class="va" style="color: #111111;">self</span>.vocab,<span class="va" style="color: #111111;">self</span>.o2i <span class="op" style="color: #5E5E5E;">=</span> uniqueify(vals, sort<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, bidir<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb64-10">        </span>
<span id="cb64-11">    <span class="kw" style="color: #003B4F;">def</span> encodes(<span class="va" style="color: #111111;">self</span>, o): <span class="cf" style="color: #003B4F;">return</span> (create_spectrogram(o), <span class="va" style="color: #111111;">self</span>.o2i.get(<span class="va" style="color: #111111;">self</span>.lbl2files.get(o)))</span>
<span id="cb64-12">    <span class="kw" style="color: #003B4F;">def</span> decodes(<span class="va" style="color: #111111;">self</span>, x): <span class="cf" style="color: #003B4F;">return</span> AudioImage(x[<span class="dv" style="color: #AD0000;">0</span>],<span class="va" style="color: #111111;">self</span>.vocab[x[<span class="dv" style="color: #AD0000;">1</span>]])</span></code></pre></div>
</details>
</div>
<p>Let‚Äôs walk through the code.</p>
<p>If you inherit from the <code>Transform</code> class, the resulting transform is applied to the item as a whole but when you inherit from the <code>ItemTransform</code> class then the resulting transform is applied to each element of the input.</p>
<p>For example, if you have a transform that is inherited from the <code>Transform</code> class and you have an input which is a tuple <code>("a", 1)</code> then the transform would consider the tuple as a single element. But, when your transform is an <code>ItemTransform</code> then the transform is applied to ‚Äúa‚Äù as well as ‚Äú1‚Äù separately.</p>
<p>The <strong>init</strong> method sets up our <code>mapxy</code> method as a class property. It then converts the target column values into a list and creates a vocab of our targets and a dictionary mapping our targets to indices.</p>
<p>The encodes method is where the magic occurs. Here, we return a tuple with our spectogram and the label related to our input.</p>
<p>The decodes method returns an <code>AudioImage</code> type which knows how to show itself whenever a <code>show</code> method is invoked.</p>
<p>You might notice that I have used a dataframe to create a list of our inputs and a dictionary of our labels. This was an engineering choice which I made because creating a list of labels from the input list of filenames was too slow. Doing it this was by using a dataframe made things faster.</p>
<blockquote class="blockquote">
<p>In deep learning a majority chunk of the speed boost comes from good engineering practices rather than having the best SOTA architectures or a faster computer.</p>
</blockquote>
<p>We will also use a ‚Äòsplitter‚Äô which tells fastai the way we want to split our data. For now we will use <code>RandomSplitter</code> to do this job. Additionally we will also instantiate the <code>NNAudioTransform</code> object.</p>
<p>We will take a few samples only to make our experiment quicker.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:47:20.017965Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:47:20.017058Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:47:20.054366Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:47:20.053935Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:33.681518Z&quot;}" data-papermill="{&quot;duration&quot;:1.013842,&quot;end_time&quot;:&quot;2021-10-01T09:47:20.054489&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:47:19.040647&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="41">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb65" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1">subset_for_dsets <span class="op" style="color: #5E5E5E;">=</span> train_files[:<span class="dv" style="color: #AD0000;">20000</span>]</span></code></pre></div>
</details>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:47:21.970543Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:47:21.969564Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:47:22.882746Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:47:22.882201Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:33.723325Z&quot;}" data-papermill="{&quot;duration&quot;:1.893631,&quot;end_time&quot;:&quot;2021-10-01T09:47:22.882884&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:47:20.989253&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="42">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb66" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1">splits <span class="op" style="color: #5E5E5E;">=</span> TrainTestSplitter()(subset_for_dsets)</span>
<span id="cb66-2">tfm <span class="op" style="color: #5E5E5E;">=</span> NNAudioTransform(df)</span></code></pre></div>
</details>
</div>
<p>The <code>tfm</code> is a transform is would be applied to the input files to generate the spectogram. The second list has the transform which will be applied to our targets.</p>
<p>Next, we have to tell fastai to take our ‚Äòsample‚Äô and apply the transform and the splitter to it.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:47:28.804138Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:47:28.803292Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:47:28.945593Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:47:28.946246Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:34.952326Z&quot;}" data-papermill="{&quot;duration&quot;:1.10064,&quot;end_time&quot;:&quot;2021-10-01T09:47:28.946573&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:47:27.845933&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="43">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb67" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><span class="op" style="color: #5E5E5E;">%%</span>time</span>
<span id="cb67-2">tls <span class="op" style="color: #5E5E5E;">=</span> TfmdLists(subset_for_dsets, tfm, splits<span class="op" style="color: #5E5E5E;">=</span>splits)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>CPU times: user 85 ms, sys: 278 ¬µs, total: 85.3 ms
Wall time: 105 ms</code></pre>
</div>
</div>
<p><code>TfmdLists</code> is a low-level API which creates a pipeline for us. It creates a pipeline that takes in our samples‚Äì&gt;splits it ‚Äì&gt; applies our transform to the items.</p>
<p>More information on a <code>TfmdLists</code> can be found in this <a href="https://docs.fast.ai/tutorial.pets.html">tutorial</a> fromt he official documentation.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:47:32.725627Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:47:32.724819Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:47:32.770143Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:47:32.769672Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:35.047280Z&quot;}" data-papermill="{&quot;duration&quot;:0.98551,&quot;end_time&quot;:&quot;2021-10-01T09:47:32.770266&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:47:31.784756&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="44">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb69" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1">tls.vocab</span></code></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="44">
<pre><code>[0, 1]</code></pre>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:47:34.697652Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:47:34.696749Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:47:34.947680Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:47:34.948227Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:35.092640Z&quot;}" data-papermill="{&quot;duration&quot;:1.204399,&quot;end_time&quot;:&quot;2021-10-01T09:47:34.948399&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:47:33.744000&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="45">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb71" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1">show_at(tls.train, <span class="dv" style="color: #AD0000;">1</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="45">
<pre><code>&lt;AxesSubplot:title={'center':'0'}&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task_files/figure-html/cell-46-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="create-the-dataloader" class="level2">
<h2 class="anchored" data-anchor-id="create-the-dataloader">Create the Dataloaderüñ®Ô∏è</h2>
<p>We can use the <code>TfmdLists</code> to create a dataloader by calling <code>dataloaders()</code>. Here, we can‚Äôt apply <code>item_tfms</code> or <code>batch_tfms</code> but we can get the hooks to different point of the pipeline and can put our transforms there.</p>
<p>For example, once items are grabbed then that moment is known as ‚Äúafter_item‚Äù. We can use this hook to apply our transforms once items are grabbed.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:47:38.998868Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:47:38.997927Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:47:42.176502Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:47:42.175977Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:35.377106Z&quot;}" data-papermill="{&quot;duration&quot;:4.169865,&quot;end_time&quot;:&quot;2021-10-01T09:47:42.176642&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:47:38.006777&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="46">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb73" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1">dls <span class="op" style="color: #5E5E5E;">=</span> tls.dataloaders(after_item<span class="op" style="color: #5E5E5E;">=</span>[ToTensor()], after_batch<span class="op" style="color: #5E5E5E;">=</span>[IntToFloatTensor(), Normalize.from_stats(<span class="op" style="color: #5E5E5E;">*</span>imagenet_stats)])</span></code></pre></div>
</details>
</div>
<p>One more thing that we need to do is to make the <code>show_batch</code> method aware of the type of our Image. This can be easily done by using <code>typedispatch</code> to dispatch our <code>show_batch</code> (the one which we will override with our image type).</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:47:45.996932Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:47:45.996106Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:47:46.038582Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:47:46.038974Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:38.708753Z&quot;}" data-papermill="{&quot;duration&quot;:0.994521,&quot;end_time&quot;:&quot;2021-10-01T09:47:46.039118&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:47:45.044597&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="47">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb74" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><span class="co" style="color: #5E5E5E;">#export</span></span>
<span id="cb74-2"><span class="at" style="color: #657422;">@typedispatch</span></span>
<span id="cb74-3"><span class="kw" style="color: #003B4F;">def</span> show_batch(x:AudioImage, y, samples, ctxs<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>, max_n<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">6</span>, nrows<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>, ncols<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">3</span>, figsize<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>, <span class="op" style="color: #5E5E5E;">**</span>kwargs):</span>
<span id="cb74-4">    <span class="cf" style="color: #003B4F;">if</span> figsize <span class="kw" style="color: #003B4F;">is</span> <span class="va" style="color: #111111;">None</span>: figsize <span class="op" style="color: #5E5E5E;">=</span> (ncols<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">6</span>, max_n<span class="op" style="color: #5E5E5E;">//</span>ncols <span class="op" style="color: #5E5E5E;">*</span> <span class="dv" style="color: #AD0000;">3</span>)</span>
<span id="cb74-5">    <span class="cf" style="color: #003B4F;">if</span> ctxs <span class="kw" style="color: #003B4F;">is</span> <span class="va" style="color: #111111;">None</span>: ctxs <span class="op" style="color: #5E5E5E;">=</span> get_grid(<span class="bu" style="color: null;">min</span>(x[<span class="dv" style="color: #AD0000;">0</span>].shape[<span class="dv" style="color: #AD0000;">0</span>], max_n), nrows<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>, ncols<span class="op" style="color: #5E5E5E;">=</span>ncols, figsize<span class="op" style="color: #5E5E5E;">=</span>figsize)</span>
<span id="cb74-6">    <span class="cf" style="color: #003B4F;">for</span> i,ctx <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">enumerate</span>(ctxs):</span>
<span id="cb74-7">        AudioImage(x[<span class="dv" style="color: #AD0000;">0</span>][i], [<span class="st" style="color: #20794D;">'0'</span>,<span class="st" style="color: #20794D;">'1'</span>][x[<span class="dv" style="color: #AD0000;">1</span>][i].item()]).show(ctx<span class="op" style="color: #5E5E5E;">=</span>ctx)</span></code></pre></div>
</details>
</div>
<p><code>typedispatch</code> does something similar to <a href="https://en.wikipedia.org/wiki/Multiple_dispatch">multi-dispatch</a>. So, that whenever we call the <code>show_batch</code> on our image type then fastai will call our version of <code>show_batch</code> after recognizing our image type.</p>
<p>Here we go</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:47:52.664378Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:47:52.663533Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:47:55.106725Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:47:55.105982Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:38.756317Z&quot;}" data-papermill="{&quot;duration&quot;:3.402168,&quot;end_time&quot;:&quot;2021-10-01T09:47:55.106907&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:47:51.704739&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="48">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb75" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><span class="op" style="color: #5E5E5E;">%%</span>time</span>
<span id="cb75-2">dls.show_batch()</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>CPU times: user 1.05 s, sys: 15.8 ms, total: 1.07 s
Wall time: 1.62 s</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task_files/figure-html/cell-49-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task_files/figure-html/cell-49-output-3.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task_files/figure-html/cell-49-output-4.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task_files/figure-html/cell-49-output-5.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task_files/figure-html/cell-49-output-6.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task_files/figure-html/cell-49-output-7.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task_files/figure-html/cell-49-output-8.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task_files/figure-html/cell-49-output-9.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task_files/figure-html/cell-49-output-10.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="modularity" class="level2">
<h2 class="anchored" data-anchor-id="modularity">Modularity üß©</h2>
<p>The way that we have created the above transform works well for a specific type of task. There are somethings which could not be answered by the above transform.</p>
<ul>
<li>What is the categories are other than 0 and 1.</li>
<li>What if it‚Äôs a multicategory problem.</li>
<li>How to handle the lack of targets during inference?
<ul>
<li>This could be handled by having a <code>setups</code> method inside the transform and have it accept list of filenames. This could work well when data is small but for huge data mapping the labelling function to all the filenames in order to create a vocab and label maps would take lots of time. In short it doesn‚Äôt scale well.</li>
</ul></li>
</ul>
<p>So what do we do?</p>
<p>The solution is to create a custom datablock for our type of task which can then be plugged into a <code>Datablock</code> like this‚Äì&gt;</p>
<pre><code>DataBlock(blocks=(NNAudioBlock, MultiCategoryBlock),
                   splitter=ColSplitter(),
                   get_x=lambda x:pascal_source/"train"/f'{x[0]}',
                   get_y=lambda x:x[1].split(' '),
                   item_tfms=Resize(224),
                   batch_tfms=aug_transforms())</code></pre>
<p>Let‚Äôs create a type to represent our spectrogram</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:48:01.123652Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:48:01.122736Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:48:01.168097Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:48:01.167552Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:41.095486Z&quot;}" data-papermill="{&quot;duration&quot;:1.00829,&quot;end_time&quot;:&quot;2021-10-01T09:48:01.168234&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:48:00.159944&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="49">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb78" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><span class="kw" style="color: #003B4F;">class</span> Spectrogram(TensorImageBase):</span>
<span id="cb78-2">    <span class="co" style="color: #5E5E5E;">"""Type to represent a spectogram which knows show itself"""</span></span>
<span id="cb78-3">    <span class="at" style="color: #657422;">@classmethod</span></span>
<span id="cb78-4">    <span class="kw" style="color: #003B4F;">def</span> create(cls, o):</span>
<span id="cb78-5">        waves <span class="op" style="color: #5E5E5E;">=</span> get_waves(o)</span>
<span id="cb78-6">        <span class="cf" style="color: #003B4F;">return</span> cls(qtfm()(waves))</span>
<span id="cb78-7">    </span>
<span id="cb78-8">    <span class="kw" style="color: #003B4F;">def</span> show(<span class="va" style="color: #111111;">self</span>, figsize<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>, ctx<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>, <span class="op" style="color: #5E5E5E;">**</span>kwargs): </span>
<span id="cb78-9">        t <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span></span>
<span id="cb78-10">        <span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> <span class="bu" style="color: null;">isinstance</span>(t, Tensor): <span class="cf" style="color: #003B4F;">return</span> ctx</span>
<span id="cb78-11">        <span class="cf" style="color: #003B4F;">if</span> figsize <span class="kw" style="color: #003B4F;">is</span> <span class="va" style="color: #111111;">None</span>: figsize<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">10</span>,<span class="dv" style="color: #AD0000;">10</span>)</span>
<span id="cb78-12">        <span class="cf" style="color: #003B4F;">return</span> show_image(t, figsize<span class="op" style="color: #5E5E5E;">=</span>figsize, ctx<span class="op" style="color: #5E5E5E;">=</span>ctx)</span></code></pre></div>
</details>
</div>
<p>In the above class we use the functions <code>get_waves</code> and <code>qtfm()</code> defined in the previous sections to create a spectrogram. The <code>show</code> method is also similar to the <code>show</code> method which we had used in the previous section. The only difference is that in this show method we are not taking the label into account because the <code>Spectogram</code> is just a type of a file converted to a spectrogram.</p>
<p>but does it work? let‚Äôs test it.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:48:06.929053Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:48:06.928218Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:48:06.988978Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:48:06.989415Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:41.160097Z&quot;}" data-papermill="{&quot;duration&quot;:1.019016,&quot;end_time&quot;:&quot;2021-10-01T09:48:06.989594&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:48:05.970578&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="50">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb79" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1">spectrogram <span class="op" style="color: #5E5E5E;">=</span> Spectrogram.create(train_files[<span class="dv" style="color: #AD0000;">0</span>])</span>
<span id="cb79-2"><span class="bu" style="color: null;">type</span>(spectrogram)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="50">
<pre><code>__main__.Spectrogram</code></pre>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:48:09.184491Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:48:09.183642Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:48:09.323334Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:48:09.323985Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:41.224793Z&quot;}" data-papermill="{&quot;duration&quot;:1.359101,&quot;end_time&quot;:&quot;2021-10-01T09:48:09.324218&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:48:07.965117&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="51">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb81" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1">spectrogram.show()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="51">
<pre><code>&lt;AxesSubplot:&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task_files/figure-html/cell-52-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>Voila! it knows how to show itself.</p>
<p>Now, we can create a custom block for our data. A block is a set of default transforms which is supposed to be applied to your data in order to tell fastai about the type of your data.</p>
<p>In our custom block we will tell fastai how create a Spectrogram block and then apply <code>IntToFloatTensor</code> transform.</p>
<p>The source code an <code>ImageBlock</code> is like this‚Äì&gt;</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:48:15.143909Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:48:15.142886Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:48:15.234698Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:48:15.234238Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:41.373505Z&quot;}" data-papermill="{&quot;duration&quot;:1.06192,&quot;end_time&quot;:&quot;2021-10-01T09:48:15.234828&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:48:14.172908&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="52">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb83" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1">ImageBlock??</span></code></pre></div>
</details>
</div>
<p>We will use the source code for <code>ImageBlock</code> to create our custom block.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:48:19.136082Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:48:19.134707Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:48:19.177527Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:48:19.177130Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:41.489147Z&quot;}" data-papermill="{&quot;duration&quot;:1.008731,&quot;end_time&quot;:&quot;2021-10-01T09:48:19.177644&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:48:18.168913&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="53">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb84" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><span class="kw" style="color: #003B4F;">def</span> SpectrogramBlock(cls<span class="op" style="color: #5E5E5E;">=</span>Spectrogram) : </span>
<span id="cb84-2">    <span class="co" style="color: #5E5E5E;">"A `TransformBlock` for spectograms of `cls`"</span></span>
<span id="cb84-3">    <span class="cf" style="color: #003B4F;">return</span> TransformBlock(type_tfms<span class="op" style="color: #5E5E5E;">=</span>cls.create, batch_tfms<span class="op" style="color: #5E5E5E;">=</span>IntToFloatTensor)</span></code></pre></div>
</details>
</div>
<p>Now that we have our custom block ready, we can test if a DataBlock can now be created.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:48:23.254024Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:48:23.253090Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:48:23.297580Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:48:23.297170Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:41.530469Z&quot;}" data-papermill="{&quot;duration&quot;:1.027546,&quot;end_time&quot;:&quot;2021-10-01T09:48:23.297733&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:48:22.270187&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="54">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb85" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1">g2net <span class="op" style="color: #5E5E5E;">=</span> DataBlock(blocks<span class="op" style="color: #5E5E5E;">=</span>(SpectrogramBlock, CategoryBlock),</span>
<span id="cb85-2">                   splitter<span class="op" style="color: #5E5E5E;">=</span>RandomSplitter(),</span>
<span id="cb85-3">                   get_x<span class="op" style="color: #5E5E5E;">=</span>ColReader(<span class="dv" style="color: #AD0000;">0</span>),</span>
<span id="cb85-4">                   get_y<span class="op" style="color: #5E5E5E;">=</span>ColReader(<span class="dv" style="color: #AD0000;">1</span>),</span>
<span id="cb85-5">                   batch_tfms<span class="op" style="color: #5E5E5E;">=</span>aug_transforms())</span></code></pre></div>
</details>
</div>
<p>Next, we create the dataloader.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:48:27.660541Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:48:27.659646Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:48:28.070300Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:48:28.069837Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:41.574332Z&quot;}" data-papermill="{&quot;duration&quot;:1.397119,&quot;end_time&quot;:&quot;2021-10-01T09:48:28.070439&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:48:26.673320&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="55">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb86" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1">dls <span class="op" style="color: #5E5E5E;">=</span> g2net.dataloaders(df.iloc[:<span class="dv" style="color: #AD0000;">2000</span>])</span></code></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored
  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)</code></pre>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:48:29.990983Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:48:29.990074Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:48:32.467094Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:48:32.468308Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:41.884309Z&quot;}" data-papermill="{&quot;duration&quot;:3.443577,&quot;end_time&quot;:&quot;2021-10-01T09:48:32.468547&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:48:29.024970&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="56">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb88" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><span class="op" style="color: #5E5E5E;">%%</span>time</span>
<span id="cb88-2">dls.show_batch()</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>CPU times: user 1.19 s, sys: 5.58 ms, total: 1.2 s
Wall time: 1.7 s</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task_files/figure-html/cell-57-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task_files/figure-html/cell-57-output-3.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task_files/figure-html/cell-57-output-4.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task_files/figure-html/cell-57-output-5.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task_files/figure-html/cell-57-output-6.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task_files/figure-html/cell-57-output-7.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task_files/figure-html/cell-57-output-8.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task_files/figure-html/cell-57-output-9.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task_files/figure-html/cell-57-output-10.png" class="img-fluid"></p>
</div>
</div>
<p>Here we go. Now we have a custom block and we can create a DataBlock as well as dataloaders and then display it.</p>
</section>
<section id="make-your-own-model" class="level2">
<h2 class="anchored" data-anchor-id="make-your-own-model">Make your own modelüçï</h2>
<p>We are going to use the timm library as the source of our model. To weave it into fastai, we will create a custom fastai model.</p>
<p>All the code below is heavily inspired by‚Äì&gt;</p>
<ul>
<li><a href="https://www.kaggle.com/benihime91">Ayushman‚Äôs</a> <a href="https://www.kaggle.com/benihime91/fastai-timm-efficientnet-train-fold-0">notebook</a>.</li>
<li>fastai siamese <a href="https://docs.fast.ai/tutorial.siamese.html">tutorial</a>.</li>
</ul>
<p>We will also take into account the structure of fastai‚Äôs <code>create_cnn_model</code> class. The code for which is as follows</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:48:38.298017Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:48:38.297087Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:48:38.348364Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:48:38.349249Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:44.287464Z&quot;}" data-papermill="{&quot;duration&quot;:1.023296,&quot;end_time&quot;:&quot;2021-10-01T09:48:38.349404&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:48:37.326108&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="57">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb90" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1">create_cnn_model??</span></code></pre></div>
</details>
</div>
<p>Let‚Äôs build our own.</p>
<p>We will cut off the head of a timm pretrained model using <code>create_body</code> and take the encoder only as this would be the portion of the pretrained model which I would like to use. Then I will top it off with a custom fastai head using <code>create_head</code> that we would need to train on our target data.</p>
<p>To know more about this flow have a look into the fastai siamese <a href="https://docs.fast.ai/tutorial.siamese.html">tutorial</a>.</p>
<p>But first we will create our custom <code>create_body</code> and <code>create_head</code> functions. the reason for this is that fastai in it‚Äôs current state is not integrated with the timm library. So, creating custom versions of <code>create_body</code> and <code>create_head</code> makes the weaving of timm into fastai re-usable.</p>
<p>The insipration for this is the <a href="https://walkwithfastai.com/vision.external.timm#create_timm_body">post</a> in ‚Äòwalk with fastai‚Äô. Once again the code and the approach is based on this post.</p>
<blockquote class="blockquote">
<p>I am recreating this again here instead of using the ‚Äòwalk with fastai‚Äô library is to drill down into the concept and for my personal learning.</p>
</blockquote>
</section>
<section id="create_timm_bodyarch-n_in3-pretrainedtrue-cutnone" class="level2">
<h2 class="anchored" data-anchor-id="create_timm_bodyarch-n_in3-pretrainedtrue-cutnone">create_timm_body(arch, n_in=3, pretrained=True, cut=None)</h2>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:48:46.406982Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:48:46.406055Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:48:46.448766Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:48:46.448322Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:44.365319Z&quot;}" data-papermill="{&quot;duration&quot;:1.013044,&quot;end_time&quot;:&quot;2021-10-01T09:48:46.448901&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:48:45.435857&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="58">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb91" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1"><span class="co" style="color: #5E5E5E;">#export</span></span>
<span id="cb91-2"><span class="kw" style="color: #003B4F;">def</span> create_timm_body(arch, n_in<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">3</span>, pretrained<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, cut<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>):</span>
<span id="cb91-3">    <span class="co" style="color: #5E5E5E;">"Cut off the body of a typically pretrained timm library `arch` as determined by `cut`"</span></span>
<span id="cb91-4">    model <span class="op" style="color: #5E5E5E;">=</span> create_model(arch, pretrained<span class="op" style="color: #5E5E5E;">=</span>pretrained, num_classes<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">0</span>, in_chans<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>,global_pool<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">''</span>)</span>
<span id="cb91-5">    _update_first_layer(model, n_in, pretrained)</span>
<span id="cb91-6">    <span class="co" style="color: #5E5E5E;">#cut = ifnone(cut, cnn_config(arch)['cut'])</span></span>
<span id="cb91-7">    <span class="cf" style="color: #003B4F;">if</span> cut <span class="kw" style="color: #003B4F;">is</span> <span class="va" style="color: #111111;">None</span>:</span>
<span id="cb91-8">        ll <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">list</span>(<span class="bu" style="color: null;">enumerate</span>(model.children()))</span>
<span id="cb91-9">        cut <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">next</span>(i <span class="cf" style="color: #003B4F;">for</span> i,o <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">reversed</span>(ll) <span class="cf" style="color: #003B4F;">if</span> has_pool_type(o))</span>
<span id="cb91-10">    <span class="cf" style="color: #003B4F;">if</span>   <span class="bu" style="color: null;">isinstance</span>(cut, <span class="bu" style="color: null;">int</span>):      <span class="cf" style="color: #003B4F;">return</span> nn.Sequential(<span class="op" style="color: #5E5E5E;">*</span><span class="bu" style="color: null;">list</span>(model.children())[:cut])</span>
<span id="cb91-11">    <span class="cf" style="color: #003B4F;">elif</span> <span class="bu" style="color: null;">callable</span>(cut): <span class="cf" style="color: #003B4F;">return</span> cut(model)</span>
<span id="cb91-12">    <span class="cf" style="color: #003B4F;">else</span>:  <span class="cf" style="color: #003B4F;">raise</span> NamedError(<span class="st" style="color: #20794D;">"cut must be either integer or a function"</span>)</span></code></pre></div>
</details>
</div>
<p>Now that we have a way to create a body, we will use the code from <code>create_cnn_model</code> to build our custom <code>create_timm_model</code>.</p>
<p>The code for <code>create_timm_model</code> is as follows.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:48:50.354373Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:48:50.353547Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:48:50.403582Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:48:50.403123Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:44.414966Z&quot;}" data-papermill="{&quot;duration&quot;:1.02624,&quot;end_time&quot;:&quot;2021-10-01T09:48:50.403714&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:48:49.377474&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="59">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb92" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1">create_cnn_model??</span></code></pre></div>
</details>
</div>
</section>
<section id="create_timm_model" class="level2">
<h2 class="anchored" data-anchor-id="create_timm_model">create_timm_model</h2>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:48:54.502396Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:48:54.501529Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:48:54.543716Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:48:54.544160Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:44.474031Z&quot;}" data-papermill="{&quot;duration&quot;:1.018507,&quot;end_time&quot;:&quot;2021-10-01T09:48:54.544307&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:48:53.525800&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="60">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb93" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1">create_head?</span></code></pre></div>
</details>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:48:56.565422Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:48:56.561716Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:48:56.630040Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:48:56.628742Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:44.524051Z&quot;}" data-papermill="{&quot;duration&quot;:1.092977,&quot;end_time&quot;:&quot;2021-10-01T09:48:56.630211&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:48:55.537234&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="61">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb94" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><span class="co" style="color: #5E5E5E;">#export</span></span>
<span id="cb94-2"><span class="at" style="color: #657422;">@delegates</span>(create_head)</span>
<span id="cb94-3"><span class="kw" style="color: #003B4F;">def</span> create_timm_model(arch, n_out, pretrained<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, cut<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>, n_in<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">3</span>, init<span class="op" style="color: #5E5E5E;">=</span>nn.init.kaiming_normal_, custom_head<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>,</span>
<span id="cb94-4">                     concat_pool<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, in_chans<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>, <span class="op" style="color: #5E5E5E;">**</span>kwargs):</span>
<span id="cb94-5">    <span class="co" style="color: #5E5E5E;">"Create custom architecture from the timm library"</span></span>
<span id="cb94-6">    body <span class="op" style="color: #5E5E5E;">=</span> create_timm_body(arch, n_in, pretrained, <span class="va" style="color: #111111;">None</span>)</span>
<span id="cb94-7">    <span class="cf" style="color: #003B4F;">if</span> custom_head <span class="kw" style="color: #003B4F;">is</span> <span class="va" style="color: #111111;">None</span>:</span>
<span id="cb94-8">        nf <span class="op" style="color: #5E5E5E;">=</span> num_features_model(nn.Sequential(<span class="op" style="color: #5E5E5E;">*</span>body.children()))</span>
<span id="cb94-9">        head <span class="op" style="color: #5E5E5E;">=</span> create_head(nf, n_out, concat_pool<span class="op" style="color: #5E5E5E;">=</span>concat_pool, <span class="op" style="color: #5E5E5E;">**</span>kwargs)</span>
<span id="cb94-10">    <span class="cf" style="color: #003B4F;">else</span>: head <span class="op" style="color: #5E5E5E;">=</span> custom_head</span>
<span id="cb94-11">    model <span class="op" style="color: #5E5E5E;">=</span> nn.Sequential(body, head)</span>
<span id="cb94-12">    <span class="cf" style="color: #003B4F;">if</span> init <span class="kw" style="color: #003B4F;">is</span> <span class="kw" style="color: #003B4F;">not</span> <span class="va" style="color: #111111;">None</span>: apply_init(model[<span class="dv" style="color: #AD0000;">1</span>], init)</span>
<span id="cb94-13">    <span class="cf" style="color: #003B4F;">return</span> model</span></code></pre></div>
</details>
</div>
<p>The <code>@delegate</code> macro tells fastai to show the parameters of any <code>**kwargs</code> (which we would be using in the <code>create_body</code>) during function <a href="https://fastcore.fast.ai/meta.html#delegates">introspection</a>.</p>
<p>Let‚Äôs do a quick test to check if our custom model works.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:49:03.005064Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:49:03.004115Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:49:03.042351Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:49:03.041931Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:44.573818Z&quot;}" data-papermill="{&quot;duration&quot;:1.012878,&quot;end_time&quot;:&quot;2021-10-01T09:49:03.042472&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:49:02.029594&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="62">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb95" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><span class="co" style="color: #5E5E5E;"># num of classes</span></span>
<span id="cb95-2">n_out <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">2</span></span></code></pre></div>
</details>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:49:05.214552Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:49:05.213601Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:49:07.475507Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:49:07.474958Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:44.617871Z&quot;}" data-papermill="{&quot;duration&quot;:3.231905,&quot;end_time&quot;:&quot;2021-10-01T09:49:07.475633&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:49:04.243728&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="63">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb96" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb96-1">model <span class="op" style="color: #5E5E5E;">=</span> create_timm_model(<span class="st" style="color: #20794D;">"efficientnet_b3a"</span>, n_out)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>Downloading: "https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_b3_ra2-cf984f9c.pth" to /root/.cache/torch/hub/checkpoints/efficientnet_b3_ra2-cf984f9c.pth</code></pre>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:49:09.437591Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:49:09.436794Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:49:09.481688Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:49:09.482159Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:46.372602Z&quot;}" data-papermill="{&quot;duration&quot;:1.026279,&quot;end_time&quot;:&quot;2021-10-01T09:49:09.482384&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:49:08.456105&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="64">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb98" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1">L(model.children())</span></code></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="64">
<pre><code>(#2) [Sequential(
  (0): Conv2d(1, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): SiLU(inplace=True)
  (3): Sequential(
    (0): Sequential(
      (0): DepthwiseSeparableConv(
        (conv_dw): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)
        (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): SiLU(inplace=True)
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(40, 10, kernel_size=(1, 1), stride=(1, 1))
          (act1): SiLU(inplace=True)
          (conv_expand): Conv2d(10, 40, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (conv_pw): Conv2d(40, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
      (1): DepthwiseSeparableConv(
        (conv_dw): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)
        (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): SiLU(inplace=True)
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1))
          (act1): SiLU(inplace=True)
          (conv_expand): Conv2d(6, 24, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (conv_pw): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): Identity()
      )
    )
    (1): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): SiLU(inplace=True)
        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): SiLU(inplace=True)
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))
          (act1): SiLU(inplace=True)
          (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): SiLU(inplace=True)
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): SiLU(inplace=True)
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))
          (act1): SiLU(inplace=True)
          (conv_expand): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): SiLU(inplace=True)
        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): SiLU(inplace=True)
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))
          (act1): SiLU(inplace=True)
          (conv_expand): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): SiLU(inplace=True)
        (conv_dw): Conv2d(192, 192, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=192, bias=False)
        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): SiLU(inplace=True)
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))
          (act1): SiLU(inplace=True)
          (conv_expand): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): SiLU(inplace=True)
        (conv_dw): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): SiLU(inplace=True)
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))
          (act1): SiLU(inplace=True)
          (conv_expand): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): SiLU(inplace=True)
        (conv_dw): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): SiLU(inplace=True)
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))
          (act1): SiLU(inplace=True)
          (conv_expand): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): SiLU(inplace=True)
        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): SiLU(inplace=True)
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))
          (act1): SiLU(inplace=True)
          (conv_expand): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (conv_pwl): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): SiLU(inplace=True)
        (conv_dw): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
        (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): SiLU(inplace=True)
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))
          (act1): SiLU(inplace=True)
          (conv_expand): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (conv_pwl): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): SiLU(inplace=True)
        (conv_dw): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
        (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): SiLU(inplace=True)
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))
          (act1): SiLU(inplace=True)
          (conv_expand): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (conv_pwl): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): SiLU(inplace=True)
        (conv_dw): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
        (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): SiLU(inplace=True)
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))
          (act1): SiLU(inplace=True)
          (conv_expand): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (conv_pwl): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): InvertedResidual(
        (conv_pw): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): SiLU(inplace=True)
        (conv_dw): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
        (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): SiLU(inplace=True)
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))
          (act1): SiLU(inplace=True)
          (conv_expand): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (conv_pwl): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): SiLU(inplace=True)
        (conv_dw): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
        (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): SiLU(inplace=True)
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))
          (act1): SiLU(inplace=True)
          (conv_expand): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (conv_pwl): Conv2d(576, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): SiLU(inplace=True)
        (conv_dw): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): SiLU(inplace=True)
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))
          (act1): SiLU(inplace=True)
          (conv_expand): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): SiLU(inplace=True)
        (conv_dw): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): SiLU(inplace=True)
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))
          (act1): SiLU(inplace=True)
          (conv_expand): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): SiLU(inplace=True)
        (conv_dw): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): SiLU(inplace=True)
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))
          (act1): SiLU(inplace=True)
          (conv_expand): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): SiLU(inplace=True)
        (conv_dw): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): SiLU(inplace=True)
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))
          (act1): SiLU(inplace=True)
          (conv_expand): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): SiLU(inplace=True)
        (conv_dw): Conv2d(816, 816, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=816, bias=False)
        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): SiLU(inplace=True)
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))
          (act1): SiLU(inplace=True)
          (conv_expand): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (conv_pwl): Conv2d(816, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): SiLU(inplace=True)
        (conv_dw): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)
        (bn2): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): SiLU(inplace=True)
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))
          (act1): SiLU(inplace=True)
          (conv_expand): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (conv_pwl): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): InvertedResidual(
        (conv_pw): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): SiLU(inplace=True)
        (conv_dw): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)
        (bn2): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): SiLU(inplace=True)
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))
          (act1): SiLU(inplace=True)
          (conv_expand): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (conv_pwl): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): InvertedResidual(
        (conv_pw): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): SiLU(inplace=True)
        (conv_dw): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)
        (bn2): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): SiLU(inplace=True)
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))
          (act1): SiLU(inplace=True)
          (conv_expand): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (conv_pwl): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): InvertedResidual(
        (conv_pw): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): SiLU(inplace=True)
        (conv_dw): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)
        (bn2): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): SiLU(inplace=True)
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))
          (act1): SiLU(inplace=True)
          (conv_expand): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (conv_pwl): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (5): InvertedResidual(
        (conv_pw): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): SiLU(inplace=True)
        (conv_dw): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)
        (bn2): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): SiLU(inplace=True)
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))
          (act1): SiLU(inplace=True)
          (conv_expand): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (conv_pwl): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): InvertedResidual(
        (conv_pw): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): SiLU(inplace=True)
        (conv_dw): Conv2d(1392, 1392, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1392, bias=False)
        (bn2): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): SiLU(inplace=True)
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))
          (act1): SiLU(inplace=True)
          (conv_expand): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (conv_pwl): Conv2d(1392, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): InvertedResidual(
        (conv_pw): Conv2d(384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(2304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): SiLU(inplace=True)
        (conv_dw): Conv2d(2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)
        (bn2): BatchNorm2d(2304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act2): SiLU(inplace=True)
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(2304, 96, kernel_size=(1, 1), stride=(1, 1))
          (act1): SiLU(inplace=True)
          (conv_expand): Conv2d(96, 2304, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
        (conv_pwl): Conv2d(2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (4): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (5): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (6): SiLU(inplace=True)
),Sequential(
  (0): AdaptiveConcatPool2d(
    (ap): AdaptiveAvgPool2d(output_size=1)
    (mp): AdaptiveMaxPool2d(output_size=1)
  )
  (1): Flatten(full=False)
  (2): BatchNorm1d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): Dropout(p=0.25, inplace=False)
  (4): Linear(in_features=3072, out_features=512, bias=False)
  (5): ReLU(inplace=True)
  (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): Dropout(p=0.5, inplace=False)
  (8): Linear(in_features=512, out_features=2, bias=False)
)]</code></pre>
</div>
</div>
<p>cool! so it works.</p>
<p>Now, we will build a learner which would enable us to do transfer learning with timm models. Once again we will port <code>cnn_learner</code> for our use and like before let‚Äôs quickly take a look into the <code>cnn_learner</code> code</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:49:13.475720Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:49:13.474788Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:49:13.531717Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:49:13.532394Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:46.420119Z&quot;}" data-papermill="{&quot;duration&quot;:1.033343,&quot;end_time&quot;:&quot;2021-10-01T09:49:13.532551&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:49:12.499208&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="65">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb100" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1">cnn_learner??</span></code></pre></div>
</details>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:49:15.746011Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:49:15.745073Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:49:15.791554Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:49:15.791089Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:46.483258Z&quot;}" data-papermill="{&quot;duration&quot;:1.159368,&quot;end_time&quot;:&quot;2021-10-01T09:49:15.791688&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:49:14.632320&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="66">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb101" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb101-1"><span class="co" style="color: #5E5E5E;">#export</span></span>
<span id="cb101-2"><span class="at" style="color: #657422;">@delegates</span>(create_timm_model)</span>
<span id="cb101-3"><span class="kw" style="color: #003B4F;">def</span> timm_learner(dls, arch, n_out<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>, pretrained<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>,</span>
<span id="cb101-4">                <span class="co" style="color: #5E5E5E;"># learner args</span></span>
<span id="cb101-5">                loss_func<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>, opt_func<span class="op" style="color: #5E5E5E;">=</span>Adam, lr<span class="op" style="color: #5E5E5E;">=</span>defaults.lr, splitter<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>, cbs<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>, metrics<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>, path<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>,</span>
<span id="cb101-6">                model_dir<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'models'</span>, wd<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>, wd_bn_bias<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>, train_bn<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, moms<span class="op" style="color: #5E5E5E;">=</span>(<span class="fl" style="color: #AD0000;">0.95</span>,<span class="fl" style="color: #AD0000;">0.85</span>,<span class="fl" style="color: #AD0000;">0.95</span>),</span>
<span id="cb101-7">                <span class="co" style="color: #5E5E5E;"># other model args</span></span>
<span id="cb101-8">                <span class="op" style="color: #5E5E5E;">**</span>kwargs):</span>
<span id="cb101-9">    <span class="co" style="color: #5E5E5E;">"Build a convnet style learner from `dls` and `timm arch`"</span></span>
<span id="cb101-10"></span>
<span id="cb101-11">    kwargs <span class="op" style="color: #5E5E5E;">=</span> {<span class="op" style="color: #5E5E5E;">**</span>kwargs}</span>
<span id="cb101-12">    <span class="cf" style="color: #003B4F;">if</span> n_out <span class="kw" style="color: #003B4F;">is</span> <span class="va" style="color: #111111;">None</span>: n_out <span class="op" style="color: #5E5E5E;">=</span> get_c(dls)</span>
<span id="cb101-13">    <span class="cf" style="color: #003B4F;">assert</span> n_out, <span class="st" style="color: #20794D;">"`n_out` is not defined, and could not be inferred from data, set `dls.c` or pass `n_out`"</span></span>
<span id="cb101-14">    model <span class="op" style="color: #5E5E5E;">=</span> create_timm_model(arch, n_out, default_split, pretrained, <span class="op" style="color: #5E5E5E;">**</span>kwargs)</span>
<span id="cb101-15"></span>
<span id="cb101-16">    learn <span class="op" style="color: #5E5E5E;">=</span> Learner(dls<span class="op" style="color: #5E5E5E;">=</span>dls, model<span class="op" style="color: #5E5E5E;">=</span>model, loss_func<span class="op" style="color: #5E5E5E;">=</span>loss_func, opt_func<span class="op" style="color: #5E5E5E;">=</span>opt_func, lr<span class="op" style="color: #5E5E5E;">=</span>lr, splitter<span class="op" style="color: #5E5E5E;">=</span>default_split, cbs<span class="op" style="color: #5E5E5E;">=</span>cbs,</span>
<span id="cb101-17">                   metrics<span class="op" style="color: #5E5E5E;">=</span>metrics, path<span class="op" style="color: #5E5E5E;">=</span>path, model_dir<span class="op" style="color: #5E5E5E;">=</span>model_dir, wd<span class="op" style="color: #5E5E5E;">=</span>wd, wd_bn_bias<span class="op" style="color: #5E5E5E;">=</span>wd_bn_bias, train_bn<span class="op" style="color: #5E5E5E;">=</span>train_bn,</span>
<span id="cb101-18">                   moms<span class="op" style="color: #5E5E5E;">=</span>moms)</span>
<span id="cb101-19">    <span class="cf" style="color: #003B4F;">if</span> pretrained: learn.freeze()</span>
<span id="cb101-20">    <span class="co" style="color: #5E5E5E;"># keep track of args for loggers</span></span>
<span id="cb101-21">    store_attr(<span class="st" style="color: #20794D;">'arch,n_out,pretrained'</span>, <span class="va" style="color: #111111;">self</span><span class="op" style="color: #5E5E5E;">=</span>learn, <span class="op" style="color: #5E5E5E;">**</span>kwargs)</span>
<span id="cb101-22">    <span class="cf" style="color: #003B4F;">return</span> learn</span></code></pre></div>
</details>
</div>
<p>Here we go. We have managed to get a port of the learner code which looks the part. Does it work?</p>
<p>Let me find out.</p>
<blockquote class="blockquote">
<p>To find the list of models available in the timm library use <code>list_models</code></p>
</blockquote>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:49:19.842787Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:49:19.841843Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:49:19.887208Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:49:19.887628Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:06:46.526726Z&quot;}" data-papermill="{&quot;duration&quot;:1.078385,&quot;end_time&quot;:&quot;2021-10-01T09:49:19.887810&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:49:18.809425&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="67">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb102" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb102-1">list_models(<span class="st" style="color: #20794D;">"efficient*"</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="67">
<pre><code>['efficientnet_b0',
 'efficientnet_b1',
 'efficientnet_b1_pruned',
 'efficientnet_b2',
 'efficientnet_b2_pruned',
 'efficientnet_b2a',
 'efficientnet_b3',
 'efficientnet_b3_pruned',
 'efficientnet_b3a',
 'efficientnet_b4',
 'efficientnet_b5',
 'efficientnet_b6',
 'efficientnet_b7',
 'efficientnet_b8',
 'efficientnet_cc_b0_4e',
 'efficientnet_cc_b0_8e',
 'efficientnet_cc_b1_8e',
 'efficientnet_el',
 'efficientnet_el_pruned',
 'efficientnet_em',
 'efficientnet_es',
 'efficientnet_es_pruned',
 'efficientnet_l2',
 'efficientnet_lite0',
 'efficientnet_lite1',
 'efficientnet_lite2',
 'efficientnet_lite3',
 'efficientnet_lite4',
 'efficientnetv2_l',
 'efficientnetv2_m',
 'efficientnetv2_rw_m',
 'efficientnetv2_rw_s',
 'efficientnetv2_s']</code></pre>
</div>
</div>
</section>
<section id="the-learner" class="level2">
<h2 class="anchored" data-anchor-id="the-learner">The learnerüë©‚Äçüè´</h2>
<p>Now that we have the model in place, we can go ahead and create the learner the usual way. We have kept the batch size to default.</p>
<p>There is one little thing that I would like to do before creating a learner. I will create a helper function which can help me to get the suggested learning rate quickly.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:49:23.820183Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:49:23.818710Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:49:23.865467Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:49:23.866151Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:13:42.734505Z&quot;}" data-papermill="{&quot;duration&quot;:1.024225,&quot;end_time&quot;:&quot;2021-10-01T09:49:23.866315&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:49:22.842090&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="68">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb104" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb104-1"><span class="co" style="color: #5E5E5E;">#export</span></span>
<span id="cb104-2"><span class="kw" style="color: #003B4F;">def</span> show_me_lrs(learn, num_it:<span class="bu" style="color: null;">int</span><span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">10</span>):</span>
<span id="cb104-3">    Suggested_lrs <span class="op" style="color: #5E5E5E;">=</span> namedtuple(<span class="st" style="color: #20794D;">'Suggested_lrs'</span>, [<span class="st" style="color: #20794D;">"min"</span>, <span class="st" style="color: #20794D;">"steep"</span>,</span>
<span id="cb104-4">                                            <span class="st" style="color: #20794D;">"valley"</span>, <span class="st" style="color: #20794D;">"slide"</span>])</span>
<span id="cb104-5">    lrs <span class="op" style="color: #5E5E5E;">=</span> learn.lr_find(suggest_funcs<span class="op" style="color: #5E5E5E;">=</span>(minimum, steep,valley, slide))</span>
<span id="cb104-6">    suggested_lrs <span class="op" style="color: #5E5E5E;">=</span> Suggested_lrs(lrs[<span class="dv" style="color: #AD0000;">0</span>], lrs[<span class="dv" style="color: #AD0000;">1</span>], lrs[<span class="dv" style="color: #AD0000;">2</span>], lrs[<span class="dv" style="color: #AD0000;">3</span>])</span>
<span id="cb104-7">    </span>
<span id="cb104-8">    <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Minimum/10:</span><span class="ch" style="color: #20794D;">\t</span><span class="sc" style="color: #5E5E5E;">{</span>lrs[<span class="dv" style="color: #AD0000;">0</span>]<span class="sc" style="color: #5E5E5E;">:.2e}</span><span class="ch" style="color: #20794D;">\</span></span>
<span id="cb104-9"><span class="ss" style="color: #20794D;">          </span><span class="ch" style="color: #20794D;">\n</span><span class="ss" style="color: #20794D;">Steepest point:</span><span class="ch" style="color: #20794D;">\t</span><span class="sc" style="color: #5E5E5E;">{</span>lrs[<span class="dv" style="color: #AD0000;">1</span>]<span class="sc" style="color: #5E5E5E;">:.2e}</span><span class="ch" style="color: #20794D;">\</span></span>
<span id="cb104-10"><span class="ss" style="color: #20794D;">          </span><span class="ch" style="color: #20794D;">\n</span><span class="ss" style="color: #20794D;">Longest valley:</span><span class="ch" style="color: #20794D;">\t</span><span class="sc" style="color: #5E5E5E;">{</span>lrs[<span class="dv" style="color: #AD0000;">2</span>]<span class="sc" style="color: #5E5E5E;">:.2e}</span><span class="ch" style="color: #20794D;">\</span></span>
<span id="cb104-11"><span class="ss" style="color: #20794D;">          </span><span class="ch" style="color: #20794D;">\n</span><span class="ss" style="color: #20794D;">Slide interval:</span><span class="ch" style="color: #20794D;">\t</span><span class="sc" style="color: #5E5E5E;">{</span>lrs[<span class="dv" style="color: #AD0000;">3</span>]<span class="sc" style="color: #5E5E5E;">:.2e}</span><span class="ss" style="color: #20794D;">"</span>)</span>
<span id="cb104-12">    </span>
<span id="cb104-13">    <span class="cf" style="color: #003B4F;">return</span> suggested_lrs</span></code></pre></div>
</details>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:49:26.041900Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:49:26.041030Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:49:27.325462Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:49:27.324889Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:29:42.716691Z&quot;}" data-papermill="{&quot;duration&quot;:2.493525,&quot;end_time&quot;:&quot;2021-10-01T09:49:27.325598&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:49:24.832073&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="69">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb105" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb105-1">learn <span class="op" style="color: #5E5E5E;">=</span> timm_learner(dls, <span class="st" style="color: #20794D;">'efficientnet_b7'</span>, loss_func<span class="op" style="color: #5E5E5E;">=</span>CrossEntropyLossFlat(), metrics<span class="op" style="color: #5E5E5E;">=</span>[RocAucBinary(axis<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">0</span>)], n_out<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>).to_fp16()</span></code></pre></div>
</details>
</div>
<p>Fit one epoch to see how it behaves</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:49:31.910302Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:49:31.909522Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:50:09.834473Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:50:09.834898Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:29:45.448439Z&quot;}" data-papermill="{&quot;duration&quot;:39.042964,&quot;end_time&quot;:&quot;2021-10-01T09:50:09.835073&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:49:30.792109&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="70">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb106" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb106-1">learn.fit_one_cycle(<span class="dv" style="color: #AD0000;">1</span>, <span class="fl" style="color: #AD0000;">3e-3</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>roc_auc_score</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.216163</td>
      <td>0.700841</td>
      <td>0.482039</td>
      <td>00:37</td>
    </tr>
  </tbody>
</table>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:50:11.774071Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:50:11.773279Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:50:12.888447Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:50:12.888894Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:30:40.513725Z&quot;}" data-papermill="{&quot;duration&quot;:2.088702,&quot;end_time&quot;:&quot;2021-10-01T09:50:12.889045&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:50:10.800343&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="71">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb107" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb107-1"><span class="co" style="color: #5E5E5E;">#to recover gpu ram</span></span>
<span id="cb107-2">learn.save(<span class="st" style="color: #20794D;">'epoch1'</span>)</span>
<span id="cb107-3">learn.load(<span class="st" style="color: #20794D;">'epoch1'</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="71">
<pre><code>&lt;fastai.learner.Learner at 0x7fdc87c14890&gt;</code></pre>
</div>
</div>
<p>Using the learning rate finder to get the learning rate</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:50:16.930497Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:50:16.929101Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:50:17.421231Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:50:17.421651Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:30:42.109656Z&quot;}" data-papermill="{&quot;duration&quot;:1.485204,&quot;end_time&quot;:&quot;2021-10-01T09:50:17.421821&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:50:15.936617&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="72">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb109" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb109-1"><span class="im" style="color: #00769E;">import</span> gc<span class="op" style="color: #5E5E5E;">;</span> gc.collect()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="72">
<pre><code>66411</code></pre>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:50:19.366001Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:50:19.365185Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:52:04.055735Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:52:04.056146Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:30:44.227019Z&quot;}" data-papermill="{&quot;duration&quot;:105.679627,&quot;end_time&quot;:&quot;2021-10-01T09:52:04.056307&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:50:18.376680&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="73">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb111" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb111-1">suggested_lrs <span class="op" style="color: #5E5E5E;">=</span> show_me_lrs(learn)</span>
<span id="cb111-2"><span class="co" style="color: #5E5E5E;">#learn.lr_find(suggest_funcs=(minimum, steep,valley, slide))</span></span></code></pre></div>
</details>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-stderr">
<pre><code>/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored
  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)
/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored
  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)
/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored
  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)
/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored
  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)
/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored
  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)
/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored
  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)
/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored
  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)
/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored
  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)
/opt/conda/lib/python3.7/site-packages/fastai/callback/schedule.py:269: UserWarning: color is redundantly defined by the 'color' keyword argument and the fmt string "ro" (-&gt; color='r'). The keyword argument will take precedence.
  ax.plot(val, idx, 'ro', label=nm, c=color)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Minimum/10: 1.00e-06          
Steepest point: 1.10e-06          
Longest valley: 6.92e-06          
Slide interval: 4.37e-03</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task_files/figure-html/cell-74-output-4.png" class="img-fluid"></p>
</div>
</div>
<p>I will use the slide algorithm here to get the optimal learning rate.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:52:07.942511Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:52:07.941631Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:53:58.985818Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:53:58.986212Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:32:50.302349Z&quot;}" data-papermill="{&quot;duration&quot;:112.01864,&quot;end_time&quot;:&quot;2021-10-01T09:53:58.986378&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:52:06.967738&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="74">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb114" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb114-1">learn.unfreeze()</span>
<span id="cb114-2">learn.fit_one_cycle(<span class="dv" style="color: #AD0000;">3</span>, suggested_lrs.slide)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>roc_auc_score</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.114650</td>
      <td>1.825437</td>
      <td>0.465078</td>
      <td>00:36</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1.011949</td>
      <td>0.750606</td>
      <td>0.491870</td>
      <td>00:37</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.917761</td>
      <td>0.708508</td>
      <td>0.516685</td>
      <td>00:37</td>
    </tr>
  </tbody>
</table>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored
  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)
/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored
  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)
/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored
  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)
/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored
  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)
/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored
  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)
/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored
  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)
/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored
  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)
/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored
  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)
/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored
  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)
/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored
  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)
/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored
  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)
/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored
  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)</code></pre>
</div>
</div>
<p>Ok! The performance is not that great but the goal of this exercise was not to have a SOTA model but rather to learn how to create a custom code base by using Fastai internals.</p>
<p>However, with proper data augmentation and more data the performance can be much better.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:54:03.169154Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:54:03.168294Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:54:03.799126Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:54:03.798615Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:36:18.064035Z&quot;}" data-papermill="{&quot;duration&quot;:1.635374,&quot;end_time&quot;:&quot;2021-10-01T09:54:03.799254&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:54:02.163880&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="75">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb116" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb116-1">learn.export(<span class="st" style="color: #20794D;">"./final"</span>)</span></code></pre></div>
</details>
</div>
</section>
<section id="inference" class="level2">
<h2 class="anchored" data-anchor-id="inference">Inferenceüßê</h2>
<p>For inference you will need to use the previous dataloader to create a test dataloader by passing the test files to it.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:54:07.873786Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:54:07.872798Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:55:14.281967Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:55:14.282586Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:36:21.814635Z&quot;}" data-papermill="{&quot;duration&quot;:67.415251,&quot;end_time&quot;:&quot;2021-10-01T09:55:14.282791&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:54:06.867540&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="76">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb117" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb117-1"><span class="op" style="color: #5E5E5E;">%%</span>time</span>
<span id="cb117-2">test_path <span class="op" style="color: #5E5E5E;">=</span> path<span class="op" style="color: #5E5E5E;">/</span><span class="st" style="color: #20794D;">'g2net-gravitational-wave-detection/test'</span></span>
<span id="cb117-3">test_files <span class="op" style="color: #5E5E5E;">=</span> getfiles(test_path, <span class="st" style="color: #20794D;">"npy"</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>CPU times: user 2.65 s, sys: 1.22 s, total: 3.86 s
Wall time: 1min 5s</code></pre>
</div>
</div>
</section>
<section id="inference-1" class="level2">
<h2 class="anchored" data-anchor-id="inference-1">Inference</h2>
<p>For inference we first load the learner</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:55:20.476564Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:55:20.475569Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:55:20.807110Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:55:20.806553Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:37:05.914115Z&quot;}" data-papermill="{&quot;duration&quot;:1.339238,&quot;end_time&quot;:&quot;2021-10-01T09:55:20.807243&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:55:19.468005&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="77">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb119" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb119-1">learn <span class="op" style="color: #5E5E5E;">=</span> load_learner(<span class="st" style="color: #20794D;">'./final'</span>, cpu<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span></code></pre></div>
</details>
</div>
<p>Create a test dataloader. This will take in the test files and apply the transforms that we had created during trainign timebut on the inference data and give you a dataloader.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:55:24.710407Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:55:24.708948Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:55:24.753638Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:55:24.753230Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:37:24.696766Z&quot;}" data-papermill="{&quot;duration&quot;:1.043913,&quot;end_time&quot;:&quot;2021-10-01T09:55:24.753777&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:55:23.709864&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="78">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb120" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb120-1">test_dls <span class="op" style="color: #5E5E5E;">=</span> learn.dls.test_dl(test_files[:<span class="dv" style="color: #AD0000;">100</span>])</span></code></pre></div>
</details>
</div>
<p>check the batch</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:55:28.886711Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:55:28.885779Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:55:31.805234Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:55:31.805920Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:37:28.939545Z&quot;}" data-papermill="{&quot;duration&quot;:4.125493,&quot;end_time&quot;:&quot;2021-10-01T09:55:31.806190&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:55:27.680697&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="79">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb121" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb121-1">test_dls.show_batch()</span></code></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored
  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task_files/figure-html/cell-80-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task_files/figure-html/cell-80-output-3.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task_files/figure-html/cell-80-output-4.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task_files/figure-html/cell-80-output-5.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task_files/figure-html/cell-80-output-6.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task_files/figure-html/cell-80-output-7.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task_files/figure-html/cell-80-output-8.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task_files/figure-html/cell-80-output-9.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://www.satyabratapal.xyz/posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task_files/figure-html/cell-80-output-10.png" class="img-fluid"></p>
</div>
</div>
<p>Use <code>get_preds</code> to get predictions in batches.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:55:35.809331Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:55:35.808474Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:55:39.479409Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:55:39.480339Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:37:35.135658Z&quot;}" data-papermill="{&quot;duration&quot;:4.669632,&quot;end_time&quot;:&quot;2021-10-01T09:55:39.480575&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:55:34.810943&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="80">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb123" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb123-1">preds <span class="op" style="color: #5E5E5E;">=</span> learn.get_preds(dl<span class="op" style="color: #5E5E5E;">=</span>test_dls)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">

</div>
</div>
<p>Have a look at your predictions.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2021-10-01T09:55:43.678763Z&quot;,&quot;iopub.status.busy&quot;:&quot;2021-10-01T09:55:43.677940Z&quot;,&quot;iopub.status.idle&quot;:&quot;2021-10-01T09:55:43.724131Z&quot;,&quot;shell.execute_reply&quot;:&quot;2021-10-01T09:55:43.723314Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2021-10-01T09:37:44.269234Z&quot;}" data-papermill="{&quot;duration&quot;:1.037782,&quot;end_time&quot;:&quot;2021-10-01T09:55:43.724262&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2021-10-01T09:55:42.686480&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="81">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb124" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb124-1">preds</span></code></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="81">
<pre><code>(tensor([[0.5247, 0.4753],
         [0.5247, 0.4753],
         [0.5248, 0.4752],
         [0.5247, 0.4753],
         [0.5252, 0.4748],
         [0.5249, 0.4751],
         [0.4635, 0.5365],
         [0.5260, 0.4740],
         [0.5246, 0.4754],
         [0.5247, 0.4753],
         [0.4650, 0.5350],
         [0.5247, 0.4753],
         [0.5270, 0.4730],
         [0.5247, 0.4753],
         [0.5247, 0.4753],
         [0.4974, 0.5026],
         [0.5246, 0.4754],
         [0.5246, 0.4754],
         [0.5247, 0.4753],
         [0.4956, 0.5044],
         [0.4867, 0.5133],
         [0.4694, 0.5306],
         [0.5264, 0.4736],
         [0.5246, 0.4754],
         [0.5247, 0.4753],
         [0.5247, 0.4753],
         [0.4893, 0.5107],
         [0.4874, 0.5126],
         [0.4887, 0.5113],
         [0.5246, 0.4754],
         [0.5247, 0.4753],
         [0.5246, 0.4754],
         [0.5248, 0.4752],
         [0.5247, 0.4753],
         [0.4840, 0.5160],
         [0.5248, 0.4752],
         [0.5247, 0.4753],
         [0.4933, 0.5067],
         [0.5247, 0.4753],
         [0.5247, 0.4753],
         [0.4694, 0.5306],
         [0.4978, 0.5022],
         [0.4897, 0.5103],
         [0.5247, 0.4753],
         [0.4885, 0.5115],
         [0.4894, 0.5106],
         [0.5246, 0.4754],
         [0.5247, 0.4753],
         [0.4868, 0.5132],
         [0.5247, 0.4753],
         [0.4903, 0.5097],
         [0.5247, 0.4753],
         [0.5247, 0.4753],
         [0.4875, 0.5125],
         [0.5250, 0.4750],
         [0.4724, 0.5276],
         [0.4901, 0.5099],
         [0.5250, 0.4750],
         [0.5247, 0.4753],
         [0.4883, 0.5117],
         [0.4836, 0.5164],
         [0.4875, 0.5125],
         [0.5246, 0.4754],
         [0.4853, 0.5147],
         [0.4876, 0.5124],
         [0.5247, 0.4753],
         [0.4884, 0.5116],
         [0.4890, 0.5110],
         [0.5247, 0.4753],
         [0.4846, 0.5154],
         [0.5247, 0.4753],
         [0.5246, 0.4754],
         [0.5247, 0.4753],
         [0.4892, 0.5108],
         [0.4853, 0.5147],
         [0.4899, 0.5101],
         [0.4841, 0.5159],
         [0.5247, 0.4753],
         [0.4905, 0.5095],
         [0.4673, 0.5327],
         [0.5246, 0.4754],
         [0.5677, 0.4323],
         [0.4856, 0.5144],
         [0.5247, 0.4753],
         [0.4878, 0.5122],
         [0.5247, 0.4753],
         [0.5259, 0.4741],
         [0.4909, 0.5091],
         [0.5004, 0.4996],
         [0.4859, 0.5141],
         [0.5247, 0.4753],
         [0.5247, 0.4753],
         [0.5247, 0.4753],
         [0.5246, 0.4754],
         [0.5246, 0.4754],
         [0.5171, 0.4829],
         [0.5247, 0.4753],
         [0.5275, 0.4725],
         [0.4877, 0.5123],
         [0.4890, 0.5110]]),
 None)</code></pre>
</div>
</div>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>I prepared this post as part of my experimentation for the <a href="https://www.kaggle.com/c/g2net-gravitational-wave-detection">g2net-gravitational-wave-detection</a> competition. My goal for preparing this notebook was to design an end-to-end flow to learn about extending fastai for a custom new task and how to extend the library to work well with other libraries.</p>
<p>It took quite a long time to get my head around the low-level and mid level API in fastai.</p>
<p>Part of the reason being that I couldn‚Äôt spend much time on this competition and the other part was that there are very few resources available at this moment which provide good detail about creating custom bits using fastai‚Äôs mid-level and low-level APIs.</p>
<p>I would like to say that the effort that it took to complete this post was worth it and I came to know how powerful the modular structure of fastai is.</p>
<p>I would like to create an extension library using the code that I have developed for this post but at this moment I can‚Äôt say how soon I would be able to do it and when but stay tuned as I would keep posting my progress on this.</p>


</section>

 ]]></description>
  <category>Deep Learning</category>
  <guid>https://www.satyabratapal.xyz/posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html</guid>
  <pubDate>Tue, 12 Oct 2021 00:00:00 GMT</pubDate>
</item>
<item>
  <title>How To Workout For Longevity</title>
  <link>https://www.satyabratapal.xyz/posts/How-to-workout-for-longevity/How-to-workout-for-longevity.html</link>
  <description><![CDATA[ 



<p>This article contains my notes to summerize the article <a href="https://www.t-nation.com/training/news-bodybuilding-longevity-workout/">‚ÄúThe Eternal Warrior Plan‚Äù</a> by Christian Thibaudeau.</p>
<!--more-->
<p>I have been a long time reader of <a href="https://www.t-nation.com/">T-nation</a> especially articles by <a href="https://www.t-nation.com/author/christian-thibaudeau/">Christian Thibaudeau</a>. This man is so full of knowledge that‚Äôs practicle for experts, beginner, pros and all the rest.</p>
<p>During the month of October there was an awsome article named <a href="https://www.t-nation.com/training/news-bodybuilding-longevity-workout/">‚ÄúThe Eternal Warrior Plan‚Äù</a> wherein Christian shared some very insightful thoughts about how to train for longetivity and in this article I have tried to summerize a few points and I hope that this summary would help you to get a gist of the original article in case you are in a hurry and you want to distill the article down to the essential points.</p>
<section id="what-the-research-says" class="level2">
<h2 class="anchored" data-anchor-id="what-the-research-says">What the research says?üë©‚Äçüî¨</h2>
<ul>
<li><p>Carb surplus and Caloric surplus will favor muscle growth, but they can reduce longevity. Carb deficit and caloric deficit can increase longevity, but it‚Äôll make it harder to build muscle.</p></li>
<li><p>Resistance training favours muscle growth and endurance training slows cellular ageing.</p></li>
<li><p>Higher body mass increases ageing. Lower body mass slows down cellular age.</p></li>
</ul>
</section>
<section id="how-to-use-this-knowledge-to-build-muscularity-strength-longevity" class="level2">
<h2 class="anchored" data-anchor-id="how-to-use-this-knowledge-to-build-muscularity-strength-longevity">How to use this knowledge to build MUSCULARITY, STRENGTH, LONGEVITY? üèãÔ∏è‚Äç‚ôÇÔ∏è</h2>
<ul>
<li><p>Get lean and stay lean. Geeting lean makes you look more muscular and a lean body doesn‚Äôt put strain on your cardiovascular system becasue your body now has to carry around less mass.</p></li>
<li><p>Train you cardiovascular system. This is important to carry all the muscles that you would build.</p></li>
<li><p>Work towards increasing your work capacity. Unless you are a pro aiming for Mr.&nbsp;olympia you are better off in increasing your capacity to do more exercises in less amount of time.</p></li>
<li><p>Walk more. Walking is a great, low strain form of cardio and is good for your mind and body.</p></li>
<li><p>Give loaded carries priority. Farmer‚Äôs walk, suitcass walk etc. challenges your entire musculature as well as your cardiovascular syste. This also transfers to your real life like carrying a heavy bag of grocery even when you are 60+ years of age.</p></li>
<li><p>Don‚Äôt chase the idea of getting ‚Äúas big as possible‚Äù. This mindset won‚Äôt serve you well. To quote from the original article.</p></li>
</ul>
<blockquote class="blockquote">
<p>How many 280-pound, 70-year olds do you see walking around?</p>
</blockquote>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>I have tried to distill some of the bigger points in this article from the original article. However, I strongly recommend you to read the original article as there are many more thing (include a bonus workout template) which I didn‚Äôt cover here. However, this article is kind of a quick note for me and you which we can use to design our next workout template or just keep these things in mind if the goal is to keep lifting as long as possible.</p>
</section>
<section id="further-reading" class="level2">
<h2 class="anchored" data-anchor-id="further-reading">Further reading</h2>
<p>Read the original article here üëâ <em><a href="https://www.t-nation.com/training/news-bodybuilding-longevity-workout/">The Eternal Warrior Plan</a></em></p>


</section>

 ]]></description>
  <category>Fitness</category>
  <guid>https://www.satyabratapal.xyz/posts/How-to-workout-for-longevity/How-to-workout-for-longevity.html</guid>
  <pubDate>Tue, 12 Oct 2021 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Chapter 2 - MVP</title>
  <dc:creator>Satyabrata pal</dc:creator>
  <link>https://www.satyabratapal.xyz/posts/image-restoration-series/chapter2-mvp.html</link>
  <description><![CDATA[ 



<p><a href="https://www.satyabratapal.xyz/posts/image-restoration-series/chapter1-deblur.html">Last time</a> we trained a model that was able to remove motion blur from our images. In this chapter we are going to create a very basic application through which we can showcase how our users can use the model on their images.</p>
<section id="why-we-need-an-app" class="level2">
<h2 class="anchored" data-anchor-id="why-we-need-an-app">Why we need an app ?</h2>
<p>You may be thinking that why we need an app? Well! your target user won‚Äôt be firing up your jupyter notebook or your code everytime they need to use your model, right?</p>
<p>The user needs a visual medium through which they can consume the predictions of your model. That‚Äôs why we are goign to need an app.</p>
</section>
<section id="why-build-an-mvp" class="level2">
<h2 class="anchored" data-anchor-id="why-build-an-mvp">Why build an MVP ?</h2>
<p>During the initial stages of developing an idea into a usable product, you need to focus on speed of iteration. You need to be able to iterate quickly through many different ways to discover the right thing.</p>
<p>This is because of this reason that during the initial stage you don‚Äôt want to go all out while building a user facing interface. You don‚Äôt need a fancy GUI or you don‚Äôt need to worry about the hardcore software engineering stages. A prototype application will do fine. So, you need a minimal viable product (MVP) to present your idea to the world and to test if your idea resonates well with the need of the user.</p>
<p>You don‚Äôt need to be an expert in web designing to build a quick prototype of an application. A little bit of creativity and an open source tool like <a href="https://gradio.app/">Gradio</a> is all you need.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Gradio is an open source library to quickly build web apps for your machine learning model using python.</p>
</div>
</div>
</section>
<section id="the-design" class="level2">
<h2 class="anchored" data-anchor-id="the-design">The design</h2>
<p>A good interface provides a good user experience. A state-of-the-art model with an user interface with below average usage experience will not provide any value to the user.</p>
<p>So, a design which provides the required ease of usage is a must even during the prototype stage. Of course, the design of the prototype can be kept simple but some basic user experience elements like ease of use should be taken into consideration.</p>
<p>So, the first thing that we are going to do is to start with a basic design.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>In a real life project you may not need to create designs yourself as bigger projects/organization usually have separate experts, but in some cases you may need to wear the hat of the designer as well.</p>
</div>
</div>
<p>Below is a hand-drawn design of the prototype UI.</p>
<p><img src="https://www.satyabratapal.xyz/posts/image-restoration-series/ui_design.png" class="img-fluid"></p>
<p>Basically it has a browse button to upload your images (which needs deblurring) and a preview space with a comparison view of the original image and the deblurred image. I would also want to have a button to save the deblurred image.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>In a real project you may wan to use tools like ‚Äúvision‚Äù to create UI designs before presenting that to the stakeholders (customers, teams etc.)</p>
</div>
</div>
</section>
<section id="importing-the-libraries" class="level2">
<h2 class="anchored" data-anchor-id="importing-the-libraries">Importing the libraries</h2>
<p>First let‚Äôs import the required libraries.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb1-1">try<span class="op" style="color: #5E5E5E;">:</span> <span class="im" style="color: #00769E;">import</span> <span class="bu" style="color: null;">gradio </span>as gr</span>
<span id="cb1-2">except ModuleNotFoundError<span class="op" style="color: #5E5E5E;">:</span></span>
<span id="cb1-3">    !pip install <span class="op" style="color: #5E5E5E;">-</span>Uq gradio</span></code></pre></div>
</details>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb2-1"><span class="co" style="color: #5E5E5E;"># install fastkaggle if not available</span></span>
<span id="cb2-2">try<span class="op" style="color: #5E5E5E;">:</span> from fastaibreadcrumbs.core <span class="im" style="color: #00769E;">import</span> <span class="bu" style="color: null;">*</span></span>
<span id="cb2-3">except ModuleNotFoundError<span class="op" style="color: #5E5E5E;">:</span></span>
<span id="cb2-4">    !pip install <span class="op" style="color: #5E5E5E;">-</span>Uq fastaibreadcrumbs</span></code></pre></div>
</details>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb3-1"><span class="im" style="color: #00769E;">import</span> <span class="bu" style="color: null;">gc</span></span>
<span id="cb3-2">from fastai.vision.all <span class="im" style="color: #00769E;">import</span> <span class="bu" style="color: null;">*</span></span>
<span id="cb3-3">from fastai.vision.gan <span class="im" style="color: #00769E;">import</span> <span class="bu" style="color: null;">*</span></span>
<span id="cb3-4">from fastkaggle <span class="im" style="color: #00769E;">import</span> <span class="bu" style="color: null;">*</span></span>
<span id="cb3-5">from fastaibreadcrumbs.core <span class="im" style="color: #00769E;">import</span> <span class="bu" style="color: null;">*</span></span></code></pre></div>
</details>
</div>
</section>
<section id="getting-the-prediciton" class="level2">
<h2 class="anchored" data-anchor-id="getting-the-prediciton">Getting the prediciton</h2>
<p>We will use the model that we had trained in <a href="https://www.satyabratapal.xyz/posts/image-restoration-series/chapter1-deblur.html">chapter1</a>. We will use the same dataloaders and learners which we used in the previous schapter.</p>
<p>As is the usual drill, I will create a config dictionary.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb4-1">config <span class="op" style="color: #5E5E5E;">=</span> {<span class="ch" style="color: #20794D;">'path_crappy'</span><span class="op" style="color: #5E5E5E;">:</span> <span class="fu" style="color: #4758AB;">Path</span>(<span class="ch" style="color: #20794D;">'crappy'</span>)}</span></code></pre></div>
</details>
</div>
<p>I have created a quick function below which contains all my transformations and dataloaders from chapter1. If refer the previous <a href="https://www.satyabratapal.xyz/posts/image-restoration-series/chapter1-deblur.html">chapter</a>, you will notice that all the code used is the same. It‚Äôs just that here I have combined those into a single function.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb5-1">def <span class="fu" style="color: #4758AB;">get_dls</span>(sz<span class="op" style="color: #5E5E5E;">:</span>int,bs<span class="op" style="color: #5E5E5E;">:</span>int, src)<span class="op" style="color: #5E5E5E;">:</span></span>
<span id="cb5-2">    item_tfms <span class="op" style="color: #5E5E5E;">=</span> <span class="fu" style="color: #4758AB;">Resize</span>(sz)</span>
<span id="cb5-3">    batch_tfms <span class="op" style="color: #5E5E5E;">=</span> [<span class="fu" style="color: #4758AB;">*aug_transforms</span>(max_zoom<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">2</span>.), Normalize.<span class="fu" style="color: #4758AB;">from_stats</span>(<span class="op" style="color: #5E5E5E;">*</span>imagenet_stats)]</span>
<span id="cb5-4">    get_y <span class="op" style="color: #5E5E5E;">=</span> lambda x<span class="op" style="color: #5E5E5E;">:</span> x.<span class="fu" style="color: #4758AB;">relative_to</span>(config[<span class="ch" style="color: #20794D;">'path_crappy'</span>])</span>
<span id="cb5-5">    files_crappy <span class="op" style="color: #5E5E5E;">=</span> <span class="fu" style="color: #4758AB;">get_image_files</span>(src)</span>
<span id="cb5-6">    </span>
<span id="cb5-7">    dls<span class="op" style="color: #5E5E5E;">=</span> <span class="fu" style="color: #4758AB;">get_unet_dls</span>(bs, source <span class="op" style="color: #5E5E5E;">=</span> files_crappy, get_y <span class="op" style="color: #5E5E5E;">=</span> get_y, </span>
<span id="cb5-8">                     splitter <span class="op" style="color: #5E5E5E;">=</span> <span class="fu" style="color: #4758AB;">RandomSplitter</span>(), item_tfms <span class="op" style="color: #5E5E5E;">=</span> item_tfms,</span>
<span id="cb5-9">                     batch_tfms <span class="op" style="color: #5E5E5E;">=</span> batch_tfms)</span>
<span id="cb5-10">    </span>
<span id="cb5-11">    <span class="cf" style="color: #003B4F;">return</span> dls</span></code></pre></div>
</details>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>For line 4 you would need to make sure that your target images(the non-crappified images), needs to be in a path that is relative to the path where the crappified images are stored. Now if you are coming to this chpater from chapter1 then you would be having the crappified and non-crappified images in relative paths.</p>
</div>
</div>
<p>I have another function here which creates a unet learner and loads the model that we trained in the previous chapter.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb6-1">def <span class="fu" style="color: #4758AB;">get_inf_model</span>(dls, model<span class="op" style="color: #5E5E5E;">:</span>str)<span class="op" style="color: #5E5E5E;">:</span></span>
<span id="cb6-2">    unet_learn <span class="op" style="color: #5E5E5E;">=</span> <span class="fu" style="color: #4758AB;">unet_learner</span>(dls, models.resnet34, loss_func<span class="op" style="color: #5E5E5E;">=</span>F.l1_loss,</span>
<span id="cb6-3">                     blur<span class="op" style="color: #5E5E5E;">=</span>True, norm_type<span class="op" style="color: #5E5E5E;">=</span>NormType.Weight).<span class="fu" style="color: #4758AB;">load</span>(model)</span>
<span id="cb6-4">    </span>
<span id="cb6-5">    <span class="cf" style="color: #003B4F;">return</span> unet_learn</span></code></pre></div>
</details>
</div>
<p>Now, I will create the dataloaders. I will pass an image size of 860 to my <code>get_dls</code> functions and a batch size of 8. It‚Äôs not necessary to keep the batch size same to what you had kept during the trainign time. You can change the batch size. while training the model (in chapter1) we used a final image size of 256px. Here, during inference time I would like to ‚Äúdebluurify‚Äù an image bigger than that. Although, I can have the original ‚Äúbig‚Äù size of an image but somehow it crashes the jupyter kernel as soon as prediction is performed by the model. It might be due to limited gpu memory. So, a size of 860px works good enough for experiment.</p>
<p>Feel free to play around with bigger image size and let me know your findings by posting it on twitter (<span class="citation" data-cites="thecodingprojec">@thecodingprojec</span>).</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb7-1">dls <span class="op" style="color: #5E5E5E;">=</span> <span class="fu" style="color: #4758AB;">get_dls</span>(<span class="fl" style="color: #AD0000;">860</span>, <span class="fl" style="color: #AD0000;">8</span>, config[<span class="ch" style="color: #20794D;">'path_crappy'</span>])</span></code></pre></div>
</details>
</div>
<p>Next, let‚Äôs create the learner by passing the dataloaders and the model trained in chapter 1.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>You will need to pass the path where you have stored the crappy files which you had used to train your model.</p>
</div>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb8-1">learner <span class="op" style="color: #5E5E5E;">=</span> <span class="fu" style="color: #4758AB;">get_inf_model</span>(dls, <span class="ch" style="color: #20794D;">'./model_256'</span>)</span></code></pre></div>
</details>
</div>
<p>I have created a function to get the prediction and save that onto the disk.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Make sure to have your final model in the ‚Äúmodels‚Äù directory and pass the same path to the <code>get_inf_model</code> above.</p>
</div>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb9-1">def <span class="fu" style="color: #4758AB;">save_pred</span>(path<span class="op" style="color: #5E5E5E;">:</span>str, dest<span class="op" style="color: #5E5E5E;">:</span>str,learner)<span class="op" style="color: #5E5E5E;">:</span></span>
<span id="cb9-2">    path <span class="op" style="color: #5E5E5E;">=</span> <span class="fu" style="color: #4758AB;">Path</span>(path)</span>
<span id="cb9-3">    dest <span class="op" style="color: #5E5E5E;">=</span> <span class="fu" style="color: #4758AB;">Path</span>(dest)</span>
<span id="cb9-4">    preds <span class="op" style="color: #5E5E5E;">=</span> learner.<span class="fu" style="color: #4758AB;">predict</span>(path)</span>
<span id="cb9-5">    arr <span class="op" style="color: #5E5E5E;">=</span> preds[<span class="fl" style="color: #AD0000;">0</span>].<span class="fu" style="color: #4758AB;">numpy</span>().<span class="fu" style="color: #4758AB;">transpose</span>(<span class="fl" style="color: #AD0000;">1</span>,<span class="fl" style="color: #AD0000;">2</span>,<span class="fl" style="color: #AD0000;">0</span>).<span class="fu" style="color: #4758AB;">astype</span>(np.uint8)</span>
<span id="cb9-6">    dest.<span class="fu" style="color: #4758AB;">mkdir</span>(parents<span class="op" style="color: #5E5E5E;">=</span>True, exist_ok<span class="op" style="color: #5E5E5E;">=</span>True)</span>
<span id="cb9-7">    Image.<span class="fu" style="color: #4758AB;">fromarray</span>(arr).<span class="fu" style="color: #4758AB;">save</span>(dest<span class="op" style="color: #5E5E5E;">/</span>path.name)</span>
<span id="cb9-8">    <span class="cf" style="color: #003B4F;">return</span> dest<span class="op" style="color: #5E5E5E;">/</span>path.name</span></code></pre></div>
</details>
</div>
<p>The <code>save_pred</code> takes in the source image path, the destionation path and the learner. In line 4 the source image is passed onto the learner and the prediciton is stored in <code>pred</code>. <code>pred</code> is actually a tuple of three things out of which we need only the first item in this tuple i.e.&nbsp;the generated image.</p>
</section>
<section id="the-ui" class="level2">
<h2 class="anchored" data-anchor-id="the-ui">The UI</h2>
<p>First of all we need a function which would trigger the prediction for us, save it to the disk and then return the path. <code>save_pred</code> from the previous section would take care of generating and saving the prediction. We will wrap this in another function which would then return the path of the generated image.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb10-1">def <span class="fu" style="color: #4758AB;">display_result</span>(path)<span class="op" style="color: #5E5E5E;">:</span></span>
<span id="cb10-2">    dest <span class="op" style="color: #5E5E5E;">=</span> <span class="fu" style="color: #4758AB;">save_pred</span>(path,<span class="st" style="color: #20794D;">"gen_imgs"</span>,learner)</span>
<span id="cb10-3">    <span class="cf" style="color: #003B4F;">return</span> dest</span></code></pre></div>
</details>
</div>
<p>Next, we use gradio code to create our Ui components.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb11-1">with gr.<span class="fu" style="color: #4758AB;">Blocks</span>() as demo<span class="op" style="color: #5E5E5E;">:</span></span>
<span id="cb11-2">    with gr.<span class="fu" style="color: #4758AB;">Row</span>()<span class="op" style="color: #5E5E5E;">:</span></span>
<span id="cb11-3">        image_input <span class="op" style="color: #5E5E5E;">=</span> gr.<span class="fu" style="color: #4758AB;">Image</span>(<span class="kw" style="color: #003B4F;">type</span><span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"filepath"</span>)</span>
<span id="cb11-4">        image_output <span class="op" style="color: #5E5E5E;">=</span> gr.<span class="fu" style="color: #4758AB;">Image</span>()</span>
<span id="cb11-5">    deblur_btn <span class="op" style="color: #5E5E5E;">=</span> gr.<span class="fu" style="color: #4758AB;">Button</span>(<span class="st" style="color: #20794D;">"Deblurrify"</span>)</span>
<span id="cb11-6">    deblur_btn.<span class="fu" style="color: #4758AB;">click</span>(fn<span class="op" style="color: #5E5E5E;">=</span>display_result,</span>
<span id="cb11-7">                     inputs<span class="op" style="color: #5E5E5E;">=</span>image_input, outputs<span class="op" style="color: #5E5E5E;">=</span>image_output)</span>
<span id="cb11-8"></span>
<span id="cb11-9">demo.<span class="fu" style="color: #4758AB;">launch</span>()</span></code></pre></div>
</details>
</div>
<p><img src="https://www.satyabratapal.xyz/posts/image-restoration-series/GUI.png" class="img-fluid"></p>
<p>In line 1 we ask gradio to initiate something known as ‚ÄúBlocks‚Äù. This is an API in gradio which let‚Äôs you have more flexibility while creating UI elements. In line 2 we ask gradio to create rows in which we would want to place our UI elements and then in line 3 and 4 we create Image boxes. One image box for our input image and one image box for displaying our output. After this in line 5 we create a button. In line 6 we tie in everything together. Whenever user clicks on the button, <code>display_result</code> function is called with the image from the first image box (defined in line 3) as input and then the generated image will be displayed in the image box define in line 4.</p>
<p>Now, go ahead and test the UI by dragging on an image of you choice (which has motion blur) and then clicking on the ‚ÄúDeblurrify‚Äù button.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>What we have created here is a very early stage prototype application. You will notice that the UI that we created is exactly not same as the design that we imagined but that‚Äôs okay in current context. We will iterate on it later on to bring it close to the design.</p>
<p>You will also notice that the app code and the supporting code is not suitable enough to be hosted somewhere. For example, we still need to re-create the dataloaders and learner before prediction. This is cumbersome as we would need to re-create the same directory structure of our input data and target data wherever we want to host of app. Also, moving the training data around whenever we want to host our app is not a good way to do things.</p>
<p>At the current state since we are testing out our idea, the current state of our code and app is good enough. In the next chapter we would explore some more things like testing a few edge cases to find out where our model fails, optimize our training code further so that trying out different experiments becomes more easier.</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li><p>Chapter 1 can be found <a href="https://www.satyabratapal.xyz/posts/image-restoration-series/chapter1-deblur.html">here</a>.</p></li>
<li><p>Code for this chapter can be found <a href="https://github.com/sapal6/image-restoration.git">here</a></p></li>
</ul>


</section>

 ]]></description>
  <category>Deep Learning</category>
  <guid>https://www.satyabratapal.xyz/posts/image-restoration-series/chapter2-mvp.html</guid>
  <pubDate>Thu, 27 Oct 2022 11:40:03 GMT</pubDate>
  <media:content url="https://www.satyabratapal.xyz/posts/image-restoration-series/ui_design.png" medium="image" type="image/png" height="121" width="144"/>
</item>
</channel>
</rss>
