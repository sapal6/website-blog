[
  {
    "objectID": "posts/blog.html",
    "href": "posts/blog.html",
    "title": "Satyabrata pal",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html",
    "href": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html",
    "title": "Extending-Fastai-For-Custom-Task",
    "section": "",
    "text": "Experiment to weave nnaudio, timm and fastai together"
  },
  {
    "objectID": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#what-i-have-tried-to-explore-here",
    "href": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#what-i-have-tried-to-explore-here",
    "title": "Extending-Fastai-For-Custom-Task",
    "section": "What I have tried to explore here? 🔍",
    "text": "What I have tried to explore here? 🔍\n👉 Extend Fastai for signal processing and time series:\n* To use nnAudio for faster processing than librosa or other signal/audio processing methods.\n* To deal with time series data as images.\n* To deal with cosmological data like gravitational waves.1\n👉 Create custom Transform.\n👉 Create custom block.\n👉 Create a dataloader.\n👉 Create a custom model with models from the timm library.\n👉 Create custom learner.\n\n\nCode\n%reload_ext autoreload\n%autoreload 2\n%matplotlib inline"
  },
  {
    "objectID": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#installing-all-the-required-libraries",
    "href": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#installing-all-the-required-libraries",
    "title": "Extending-Fastai-For-Custom-Task",
    "section": "Installing all the required libraries⚙️",
    "text": "Installing all the required libraries⚙️\n👉 Spacy\n👉 Fastai\n👉 nnAudio\n👉 timm\n\n\nCode\n#!pip install spacy==3.1.1\n\n\n\n\nCode\n#!pip3 install torch==1.9.0 torchvision==0.10.0\n!pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\n\n\nLooking in links: https://download.pytorch.org/whl/torch_stable.html\nCollecting torch==1.9.0+cu111\n  Downloading https://download.pytorch.org/whl/cu111/torch-1.9.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (2041.3 MB)\n     |████████████████████████████████| 2041.3 MB 3.1 kB/s \nCollecting torchvision==0.10.0+cu111\n  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.10.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (23.2 MB)\n     |████████████████████████████████| 23.2 MB 28.8 MB/s \nCollecting torchaudio==0.9.0\n  Downloading torchaudio-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n     |████████████████████████████████| 1.9 MB 607 kB/s \nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.9.0+cu111) (3.7.4.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision==0.10.0+cu111) (1.19.5)\nRequirement already satisfied: pillow>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision==0.10.0+cu111) (8.2.0)\nInstalling collected packages: torch, torchvision, torchaudio\n  Attempting uninstall: torch\n    Found existing installation: torch 1.7.1+cu110\n    Uninstalling torch-1.7.1+cu110:\n      Successfully uninstalled torch-1.7.1+cu110\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.8.2+cu110\n    Uninstalling torchvision-0.8.2+cu110:\n      Successfully uninstalled torchvision-0.8.2+cu110\n  Attempting uninstall: torchaudio\n    Found existing installation: torchaudio 0.7.2\n    Uninstalling torchaudio-0.7.2:\n      Successfully uninstalled torchaudio-0.7.2\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchtext 0.8.1 requires torch==1.7.1, but you have torch 1.9.0+cu111 which is incompatible.\nfastai 2.2.7 requires torch<1.8,>=1.7.0, but you have torch 1.9.0+cu111 which is incompatible.\nfastai 2.2.7 requires torchvision<0.9,>=0.8, but you have torchvision 0.10.0+cu111 which is incompatible.\nSuccessfully installed torch-1.9.0+cu111 torchaudio-0.9.0 torchvision-0.10.0+cu111\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n\n\n::: {.cell _kg_hide-input=‘true’ _kg_hide-output=‘true’ execution=‘{“iopub.execute_input”:“2021-10-01T09:42:35.698110Z”,“iopub.status.busy”:“2021-10-01T09:42:35.697293Z”,“iopub.status.idle”:“2021-10-01T09:42:46.806039Z”,“shell.execute_reply”:“2021-10-01T09:42:46.805491Z”,“shell.execute_reply.started”:“2021-10-01T09:04:05.909260Z”}’ papermill=‘{“duration”:12.087095,“end_time”:“2021-10-01T09:42:46.806171”,“exception”:false,“start_time”:“2021-10-01T09:42:34.719076”,“status”:“completed”}’ tags=‘[]’ execution_count=4}\n\nCode\n#!yes Y|conda install -c fastai fastai=2.5.2\n!pip3 install fastai==2.5.2\n\n\nCollecting fastai==2.5.2\n  Downloading fastai-2.5.2-py3-none-any.whl (186 kB)\n     |████████████████████████████████| 186 kB 15 kB/s \nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from fastai==2.5.2) (1.7.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from fastai==2.5.2) (21.0)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from fastai==2.5.2) (5.4.1)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from fastai==2.5.2) (3.4.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from fastai==2.5.2) (2.25.1)\nRequirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (from fastai==2.5.2) (21.2.4)\nRequirement already satisfied: torchvision>=0.8.2 in /opt/conda/lib/python3.7/site-packages (from fastai==2.5.2) (0.10.0+cu111)\nRequirement already satisfied: pillow>6.0.0 in /opt/conda/lib/python3.7/site-packages (from fastai==2.5.2) (8.2.0)\nCollecting fastdownload<2,>=0.0.5\n  Downloading fastdownload-0.0.5-py3-none-any.whl (13 kB)\nRequirement already satisfied: spacy<4 in /opt/conda/lib/python3.7/site-packages (from fastai==2.5.2) (2.3.7)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from fastai==2.5.2) (0.23.2)\nRequirement already satisfied: torch<1.10,>=1.7.0 in /opt/conda/lib/python3.7/site-packages (from fastai==2.5.2) (1.9.0+cu111)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from fastai==2.5.2) (1.2.5)\nRequirement already satisfied: fastprogress>=0.2.4 in /opt/conda/lib/python3.7/site-packages (from fastai==2.5.2) (1.0.0)\nRequirement already satisfied: fastcore<1.4,>=1.3.8 in /opt/conda/lib/python3.7/site-packages (from fastai==2.5.2) (1.3.26)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from fastprogress>=0.2.4->fastai==2.5.2) (1.19.5)\nRequirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai==2.5.2) (0.7.4)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai==2.5.2) (1.0.5)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai==2.5.2) (3.0.5)\nRequirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai==2.5.2) (0.8.2)\nRequirement already satisfied: thinc<7.5.0,>=7.4.1 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai==2.5.2) (7.4.5)\nRequirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai==2.5.2) (1.1.3)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai==2.5.2) (2.0.5)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai==2.5.2) (57.4.0)\nRequirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai==2.5.2) (1.0.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai==2.5.2) (4.62.1)\nRequirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai==2.5.2) (1.0.5)\nRequirement already satisfied: importlib-metadata>=0.20 in /opt/conda/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<4->fastai==2.5.2) (3.4.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<4->fastai==2.5.2) (3.5.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<4->fastai==2.5.2) (3.7.4.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->fastai==2.5.2) (1.26.6)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->fastai==2.5.2) (4.0.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->fastai==2.5.2) (2021.5.30)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->fastai==2.5.2) (2.10)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->fastai==2.5.2) (2.4.7)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->fastai==2.5.2) (1.3.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib->fastai==2.5.2) (2.8.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->fastai==2.5.2) (0.10.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from cycler>=0.10->matplotlib->fastai==2.5.2) (1.15.0)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->fastai==2.5.2) (2021.1)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->fastai==2.5.2) (1.0.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->fastai==2.5.2) (2.2.0)\nInstalling collected packages: fastdownload, fastai\n  Attempting uninstall: fastai\n    Found existing installation: fastai 2.2.7\n    Uninstalling fastai-2.2.7:\n      Successfully uninstalled fastai-2.2.7\nSuccessfully installed fastai-2.5.2 fastdownload-0.0.5\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n\n:::\n\n\nCode\n!pip3 install timm\n\n\nCollecting timm\n  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n     |████████████████████████████████| 376 kB 607 kB/s \nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from timm) (0.10.0+cu111)\nRequirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.7/site-packages (from timm) (1.9.0+cu111)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.4->timm) (3.7.4.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision->timm) (1.19.5)\nRequirement already satisfied: pillow>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->timm) (8.2.0)\nInstalling collected packages: timm\nSuccessfully installed timm-0.4.12\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n\n\n\n\nCode\n!pip3 install nnaudio\n\n\nCollecting nnaudio\n  Downloading nnAudio-0.2.6-py3-none-any.whl (30 kB)\nInstalling collected packages: nnaudio\nSuccessfully installed nnaudio-0.2.6\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv"
  },
  {
    "objectID": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#import-all-required-modules",
    "href": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#import-all-required-modules",
    "title": "Extending-Fastai-For-Custom-Task",
    "section": "Import all required modules🖥️",
    "text": "Import all required modules🖥️\n\n\nCode\n#export\nfrom typing import Tuple\nfrom collections import namedtuple\nfrom nnAudio.Spectrogram import CQT\nfrom timm import create_model, list_models\nfrom pandas.core.frame import DataFrame\nfrom fastcore.foundation import *\nfrom fastai.vision.all import *\nfrom fastai.torch_core import show_image\nfrom fastai.vision.learner import _update_first_layer"
  },
  {
    "objectID": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#get-the-files",
    "href": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#get-the-files",
    "title": "Extending-Fastai-For-Custom-Task",
    "section": "Get the files🏗️",
    "text": "Get the files🏗️\nI will try to grab all the numpy files inside train folder\n\n\nCode\npath = Path(\"../input\")\n\n\n\nGet labels🏗️\nTraining labels are in the ‘training_labels.csv’ file.\n\n\nCode\ndf = pd.read_csv(path/'g2net-gravitational-wave-detection/training_labels.csv')\n\n\n\n\nCode\ndf.head(1)\n\n\n\n\n\n\n  \n    \n      \n      id\n      target\n    \n  \n  \n    \n      0\n      00000e74ad\n      1"
  },
  {
    "objectID": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#getfilespath-path-ext",
    "href": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#getfilespath-path-ext",
    "title": "Extending-Fastai-For-Custom-Task",
    "section": "getfiles(path: Path, ext)",
    "text": "getfiles(path: Path, ext)\nGet numpy files in path recursively, only in folders, if specified.\n\nThe “#export” in the function below and all the rest of the functions/code are there to help me use nbdev to export the required code into a library later.\n\n\n\nCode\n#export\ndef getfiles(path: Path, ext) -> L:\n    \"Get numpy files in `path` recursively, only in `folders`, if specified.\"\n    return L(path.glob(f'**/*.{ext}'))\n\n\nI am using the previous function to get all the files under the train folder.\n\n\nCode\ntrain_path = path/'g2net-gravitational-wave-detection/train'\n\n\n\n\nCode\n%%time\ntrain_files = getfiles(train_path, \"npy\")\n\n\nCPU times: user 5.74 s, sys: 2.23 s, total: 7.97 s\nWall time: 2min 1s\n\n\nJust a quick test to see if we got the correct files.\n\n\nCode\ntrain_files[:2]\n\n\n(#2) [Path('../input/g2net-gravitational-wave-detection/train/7/7/7/777d746e90.npy'),Path('../input/g2net-gravitational-wave-detection/train/7/7/7/777ecfbd65.npy')]\n\n\nPicking labels from the dataframe. We may need these labels later.\n\n\nCode\nlabels = df.target.to_list()"
  },
  {
    "objectID": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#map-path-to-labels",
    "href": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#map-path-to-labels",
    "title": "Extending-Fastai-For-Custom-Task",
    "section": "Map path to labels🗺️",
    "text": "Map path to labels🗺️\nTo make things easier I will try to map the file paths to their respective labels and create a datafrane out of it.\n\n\nCode\n#export\ndef map_path_to_labels(data: L, cols: L=None ) -> DataFrame:\n    \"\"\"maps the files to their labels\"\"\"\n    if cols is None: raise ValueError(\"You forgot to provide the columns\")\n    data = dict(zip(cols, data))\n    return pd.DataFrame.from_dict(data)\n\n\n\n\nCode\n%%time\ndf = map_path_to_labels([train_files, labels], cols=[\"id\", \"target\"])\n\n\nCPU times: user 2.07 s, sys: 0 ns, total: 2.07 s\nWall time: 2.07 s\n\n\n\n\nCode\ndf.head(1)\n\n\n\n\n\n\n  \n    \n      \n      id\n      target\n    \n  \n  \n    \n      0\n      ../input/g2net-gravitational-wave-detection/train/7/7/7/777d746e90.npy\n      1"
  },
  {
    "objectID": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#mapxydf",
    "href": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#mapxydf",
    "title": "Extending-Fastai-For-Custom-Task",
    "section": "mapxy(df)",
    "text": "mapxy(df)\n\n\nCode\n#export\ndef mapxy(df: DataFrame):\n    \"\"\"Create a dictionary of file path and the label from a dataframe\"\"\"\n    return dict(df.values)\n\n\n\n\nCode\n%%time\nfiles = mapxy(df)\n\n\nCPU times: user 872 ms, sys: 0 ns, total: 872 ms\nWall time: 872 ms\n\n\n\n\nCode\n%%time\nfiles[train_files[7]]\n\n\nCPU times: user 11 µs, sys: 7 µs, total: 18 µs\nWall time: 21.5 µs\n\n\n1"
  },
  {
    "objectID": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#get_labelf-path",
    "href": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#get_labelf-path",
    "title": "Extending-Fastai-For-Custom-Task",
    "section": "get_label(f: Path)",
    "text": "get_label(f: Path)\n\n\nCode\ndef get_label(f: Path):\n    \"\"\"get the label belonging to a file\"\"\"\n    label = None\n    if f in df.values:\n        label = df[df['id'] == f]['target'].values[0]\n    return label\n\n\n\n\nCode\nget_label(train_files[0])\n\n\n1\n\n\n\n\nCode\nget_label(Path(\"/ff/ff.npy\"))"
  },
  {
    "objectID": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#q-transform-using-nnaudio",
    "href": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#q-transform-using-nnaudio",
    "title": "Extending-Fastai-For-Custom-Task",
    "section": "Q transform using nnaudio⚗️",
    "text": "Q transform using nnaudio⚗️\nWe will design a function that would get the q transform of the time series on the fly using nnaudio. The result will be similar to converting the time series data into images.\nCode taken from notebook shared by Y.Nakama\n\n\nCode\n#export\ndef qtfm():\n    \"\"\"convert waves to images\"\"\"\n    cqt = CQT(sr= 2048, fmin= 20, fmax= 1024, hop_length= 32, bins_per_octave=8,verbose=False)\n    return cqt\n\n\nNOTE\nRemember to set verbose False if you don’t want all the string output to be displayed everytime dataloader loads the data.\nQuick test to see if this works.\n\n\nCode\nwaves = np.load(train_files[0])\nwaves = np.hstack(waves)\nwaves = waves / np.max(waves)\nwaves = torch.from_numpy(waves).float()\n\n\n\n\nCode\n%%time\nqtfm()(waves)\n\n\nCPU times: user 20.5 ms, sys: 957 µs, total: 21.5 ms\nWall time: 26.3 ms\n\n\n/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored\n  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)\n\n\ntensor([[[0.3868, 0.3814, 0.3670,  ..., 0.1262, 0.1258, 0.1258],\n         [0.3012, 0.2917, 0.2653,  ..., 0.1504, 0.1565, 0.1585],\n         [0.2445, 0.2453, 0.2446,  ..., 0.0945, 0.0961, 0.0965],\n         ...,\n         [0.0027, 0.0040, 0.0072,  ..., 0.0018, 0.0076, 0.0014],\n         [0.0027, 0.0040, 0.0120,  ..., 0.0012, 0.0061, 0.0126],\n         [0.0039, 0.0140, 0.0172,  ..., 0.0010, 0.0064, 0.0160]]])\n\n\n\n\nCode\n%%time\nfor i in range(5):\n    waves = np.load(df.iloc[i].id)\n    waves = np.hstack(waves)\n    waves = waves / np.max(waves)\n    waves = torch.from_numpy(waves).float()\n    image = qtfm()(waves)\n    target = get_label(train_files[i])\n    plt.imshow(image[0])\n    plt.title(f\"target: {target}\")\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCPU times: user 5.37 s, sys: 391 ms, total: 5.77 s\nWall time: 5.23 s\n\n\nCool! so we are able to plot the images now. IT is fast too."
  },
  {
    "objectID": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#creating-the-dataset",
    "href": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#creating-the-dataset",
    "title": "Extending-Fastai-For-Custom-Task",
    "section": "Creating the dataset🖫",
    "text": "Creating the dataset🖫\nIf you want to use fastai’s learner to train your model on the transfomed spectograms, you can do so by creating a custom Dataset in pytorch and then feeding that dataset with a dataloader to fastai’s learner. However, if you create a pipeline using fastai’s internals then you get to use some cool functionalities out-of-box. We will see that in a while.\nAll the code below are very heavily insipired by the original inspiration of this notebook (see the very first section), this post by Wayde Gilliam and the fastai siamese tutorial."
  },
  {
    "objectID": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#spectogram",
    "href": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#spectogram",
    "title": "Extending-Fastai-For-Custom-Task",
    "section": "Spectogram",
    "text": "Spectogram\n\n\nCode\n#export\ndef get_waves(f):\n    \"\"\"read numpy file, stack the timeseries and convert those into a tensor\"\"\"\n    waves = np.load(f)\n    waves = np.hstack(waves)\n    waves = waves / np.max(waves)\n    waves = torch.from_numpy(waves).float()\n    return waves\n\ndef create_spectrogram(x: Path):\n    \"\"\"Create an AudioSpectrogram from a torch tensor\"\"\"\n    waves = get_waves(x)\n    return qtfm()(waves)"
  },
  {
    "objectID": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#nnaudioimagefastuple",
    "href": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#nnaudioimagefastuple",
    "title": "Extending-Fastai-For-Custom-Task",
    "section": "NNAudioImage(fastuple)",
    "text": "NNAudioImage(fastuple)\nFirst of all, we are going to create an “Image type” for our transformed object (it’s the numpy data transformed into spectrogram).\nWe have to do this because our data is not an image data from get-go. Rather it’s a signal data which we are transforming into an Image. So, to tell fastai that this is a custom Image type which we are dealing with and ho we should be displaying it, we have to create an Image type.\n\n\nCode\n#export\nclass AudioImage(fastuple):\n    \"\"\"Custom Image for nnAudio transformed signals\"\"\"\n    def show(self, figsize=None, ctx=None, **kwargs):\n        if len(self) > 1:\n            img,label = self\n        else:\n            img = self\n            label = ''\n    \n        if figsize is None: figsize=(10,10)\n        return show_image(img, \n                          title=label, figsize=figsize, ctx=ctx)\n\n\n\n\nCode\nimage = create_spectrogram(train_files[0])\n\n\n\n\nCode\ntype(image), image.shape\n\n\n(torch.Tensor, torch.Size([1, 46, 385]))\n\n\n\n\nCode\nimage\n\n\ntensor([[[0.3868, 0.3814, 0.3670,  ..., 0.1262, 0.1258, 0.1258],\n         [0.3012, 0.2917, 0.2653,  ..., 0.1504, 0.1565, 0.1585],\n         [0.2445, 0.2453, 0.2446,  ..., 0.0945, 0.0961, 0.0965],\n         ...,\n         [0.0027, 0.0040, 0.0072,  ..., 0.0018, 0.0076, 0.0014],\n         [0.0027, 0.0040, 0.0120,  ..., 0.0012, 0.0061, 0.0126],\n         [0.0039, 0.0140, 0.0172,  ..., 0.0010, 0.0064, 0.0160]]])\n\n\n\n\nCode\nget_label(train_files[0])\n\n\n1\n\n\n\n\nCode\ns = AudioImage(image, get_label(train_files[0]))\ns.show()\n\n\n<AxesSubplot:title={'center':'1'}>"
  },
  {
    "objectID": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#nnaudiodataset",
    "href": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#nnaudiodataset",
    "title": "Extending-Fastai-For-Custom-Task",
    "section": "NNAudioDataset",
    "text": "NNAudioDataset\nYou can create a Dataset in fastai by creating a custom Transform . Creating a Transform has come advantages as compared to a pytorch Dataset. For example, you don’t need to have a len component or a get_item component.\nOn a very high level a Transform has an encodes, decodes and setup methods. For our purpose having an encodes methods only would suffice. This is the place where we would be transforming the numpy data into spectograms.\nTo know more about Tranforms refer these –> * data block nirvana * Siamese tutorial * Fastbook chapter-11 * Albumentation tutorial\n\n\nCode\n%%time\nvals = df.target.to_list()\n\n\nCPU times: user 9.73 ms, sys: 0 ns, total: 9.73 ms\nWall time: 9.74 ms\n\n\n\n\nCode\n%%time\nvocab,o2i = uniqueify(vals, sort=True, bidir=True)\n\n\nCPU times: user 12.7 ms, sys: 0 ns, total: 12.7 ms\nWall time: 12.8 ms\n\n\n\n\nCode\nlbl2path = mapxy(df)\n\n\n\n\nCode\n%%time\nlbl2path[train_files[5]]\n\n\nCPU times: user 13 µs, sys: 5 µs, total: 18 µs\nWall time: 21.5 µs\n\n\n1\n\n\ncombining all above steps"
  },
  {
    "objectID": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#nnaudiotransformitemtransform",
    "href": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#nnaudiotransformitemtransform",
    "title": "Extending-Fastai-For-Custom-Task",
    "section": "NNAudioTransform(ItemTransform)",
    "text": "NNAudioTransform(ItemTransform)\n\n\nCode\n#export\n#ItemTransform let's you work with tuple elements\nclass NNAudioTransform(ItemTransform):\n    \"\"\"Custom Transform which uses nnAudio transforms\n    to extract spectogram on the fly\"\"\"\n    def __init__(self, df: DataFrame, col: str = 'target'):\n        self.lbl2files = mapxy(df)\n        vals = df[col].to_list()\n        self.vocab,self.o2i = uniqueify(vals, sort=True, bidir=True)\n        \n    def encodes(self, o): return (create_spectrogram(o), self.o2i.get(self.lbl2files.get(o)))\n    def decodes(self, x): return AudioImage(x[0],self.vocab[x[1]])\n\n\nLet’s walk through the code.\nIf you inherit from the Transform class, the resulting transform is applied to the item as a whole but when you inherit from the ItemTransform class then the resulting transform is applied to each element of the input.\nFor example, if you have a transform that is inherited from the Transform class and you have an input which is a tuple (\"a\", 1) then the transform would consider the tuple as a single element. But, when your transform is an ItemTransform then the transform is applied to “a” as well as “1” separately.\nThe init method sets up our mapxy method as a class property. It then converts the target column values into a list and creates a vocab of our targets and a dictionary mapping our targets to indices.\nThe encodes method is where the magic occurs. Here, we return a tuple with our spectogram and the label related to our input.\nThe decodes method returns an AudioImage type which knows how to show itself whenever a show method is invoked.\nYou might notice that I have used a dataframe to create a list of our inputs and a dictionary of our labels. This was an engineering choice which I made because creating a list of labels from the input list of filenames was too slow. Doing it this was by using a dataframe made things faster.\n\nIn deep learning a majority chunk of the speed boost comes from good engineering practices rather than having the best SOTA architectures or a faster computer.\n\nWe will also use a ‘splitter’ which tells fastai the way we want to split our data. For now we will use RandomSplitter to do this job. Additionally we will also instantiate the NNAudioTransform object.\nWe will take a few samples only to make our experiment quicker.\n\n\nCode\nsubset_for_dsets = train_files[:20000]\n\n\n\n\nCode\nsplits = TrainTestSplitter()(subset_for_dsets)\ntfm = NNAudioTransform(df)\n\n\nThe tfm is a transform is would be applied to the input files to generate the spectogram. The second list has the transform which will be applied to our targets.\nNext, we have to tell fastai to take our ‘sample’ and apply the transform and the splitter to it.\n\n\nCode\n%%time\ntls = TfmdLists(subset_for_dsets, tfm, splits=splits)\n\n\nCPU times: user 85 ms, sys: 278 µs, total: 85.3 ms\nWall time: 105 ms\n\n\nTfmdLists is a low-level API which creates a pipeline for us. It creates a pipeline that takes in our samples–>splits it –> applies our transform to the items.\nMore information on a TfmdLists can be found in this tutorial fromt he official documentation.\n\n\nCode\ntls.vocab\n\n\n[0, 1]\n\n\n\n\nCode\nshow_at(tls.train, 1)\n\n\n<AxesSubplot:title={'center':'0'}>"
  },
  {
    "objectID": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#create-the-dataloader",
    "href": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#create-the-dataloader",
    "title": "Extending-Fastai-For-Custom-Task",
    "section": "Create the Dataloader🖨️",
    "text": "Create the Dataloader🖨️\nWe can use the TfmdLists to create a dataloader by calling dataloaders(). Here, we can’t apply item_tfms or batch_tfms but we can get the hooks to different point of the pipeline and can put our transforms there.\nFor example, once items are grabbed then that moment is known as “after_item”. We can use this hook to apply our transforms once items are grabbed.\n\n\nCode\ndls = tls.dataloaders(after_item=[ToTensor()], after_batch=[IntToFloatTensor(), Normalize.from_stats(*imagenet_stats)])\n\n\nOne more thing that we need to do is to make the show_batch method aware of the type of our Image. This can be easily done by using typedispatch to dispatch our show_batch (the one which we will override with our image type).\n\n\nCode\n#export\n@typedispatch\ndef show_batch(x:AudioImage, y, samples, ctxs=None, max_n=6, nrows=None, ncols=3, figsize=None, **kwargs):\n    if figsize is None: figsize = (ncols*6, max_n//ncols * 3)\n    if ctxs is None: ctxs = get_grid(min(x[0].shape[0], max_n), nrows=None, ncols=ncols, figsize=figsize)\n    for i,ctx in enumerate(ctxs):\n        AudioImage(x[0][i], ['0','1'][x[1][i].item()]).show(ctx=ctx)\n\n\ntypedispatch does something similar to multi-dispatch. So, that whenever we call the show_batch on our image type then fastai will call our version of show_batch after recognizing our image type.\nHere we go\n\n\nCode\n%%time\ndls.show_batch()\n\n\nCPU times: user 1.05 s, sys: 15.8 ms, total: 1.07 s\nWall time: 1.62 s"
  },
  {
    "objectID": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#modularity",
    "href": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#modularity",
    "title": "Extending-Fastai-For-Custom-Task",
    "section": "Modularity 🧩",
    "text": "Modularity 🧩\nThe way that we have created the above transform works well for a specific type of task. There are somethings which could not be answered by the above transform.\n\nWhat is the categories are other than 0 and 1.\nWhat if it’s a multicategory problem.\nHow to handle the lack of targets during inference?\n\nThis could be handled by having a setups method inside the transform and have it accept list of filenames. This could work well when data is small but for huge data mapping the labelling function to all the filenames in order to create a vocab and label maps would take lots of time. In short it doesn’t scale well.\n\n\nSo what do we do?\nThe solution is to create a custom datablock for our type of task which can then be plugged into a Datablock like this–>\nDataBlock(blocks=(NNAudioBlock, MultiCategoryBlock),\n                   splitter=ColSplitter(),\n                   get_x=lambda x:pascal_source/\"train\"/f'{x[0]}',\n                   get_y=lambda x:x[1].split(' '),\n                   item_tfms=Resize(224),\n                   batch_tfms=aug_transforms())\nLet’s create a type to represent our spectrogram\n\n\nCode\nclass Spectrogram(TensorImageBase):\n    \"\"\"Type to represent a spectogram which knows show itself\"\"\"\n    @classmethod\n    def create(cls, o):\n        waves = get_waves(o)\n        return cls(qtfm()(waves))\n    \n    def show(self, figsize=None, ctx=None, **kwargs): \n        t = self\n        if not isinstance(t, Tensor): return ctx\n        if figsize is None: figsize=(10,10)\n        return show_image(t, figsize=figsize, ctx=ctx)\n\n\nIn the above class we use the functions get_waves and qtfm() defined in the previous sections to create a spectrogram. The show method is also similar to the show method which we had used in the previous section. The only difference is that in this show method we are not taking the label into account because the Spectogram is just a type of a file converted to a spectrogram.\nbut does it work? let’s test it.\n\n\nCode\nspectrogram = Spectrogram.create(train_files[0])\ntype(spectrogram)\n\n\n__main__.Spectrogram\n\n\n\n\nCode\nspectrogram.show()\n\n\n<AxesSubplot:>\n\n\n\n\n\nVoila! it knows how to show itself.\nNow, we can create a custom block for our data. A block is a set of default transforms which is supposed to be applied to your data in order to tell fastai about the type of your data.\nIn our custom block we will tell fastai how create a Spectrogram block and then apply IntToFloatTensor transform.\nThe source code an ImageBlock is like this–>\n\n\nCode\nImageBlock??\n\n\nWe will use the source code for ImageBlock to create our custom block.\n\n\nCode\ndef SpectrogramBlock(cls=Spectrogram) : \n    \"A `TransformBlock` for spectograms of `cls`\"\n    return TransformBlock(type_tfms=cls.create, batch_tfms=IntToFloatTensor)\n\n\nNow that we have our custom block ready, we can test if a DataBlock can now be created.\n\n\nCode\ng2net = DataBlock(blocks=(SpectrogramBlock, CategoryBlock),\n                   splitter=RandomSplitter(),\n                   get_x=ColReader(0),\n                   get_y=ColReader(1),\n                   batch_tfms=aug_transforms())\n\n\nNext, we create the dataloader.\n\n\nCode\ndls = g2net.dataloaders(df.iloc[:2000])\n\n\n/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored\n  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)\n\n\n\n\nCode\n%%time\ndls.show_batch()\n\n\nCPU times: user 1.19 s, sys: 5.58 ms, total: 1.2 s\nWall time: 1.7 s\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere we go. Now we have a custom block and we can create a DataBlock as well as dataloaders and then display it."
  },
  {
    "objectID": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#make-your-own-model",
    "href": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#make-your-own-model",
    "title": "Extending-Fastai-For-Custom-Task",
    "section": "Make your own model🍕",
    "text": "Make your own model🍕\nWe are going to use the timm library as the source of our model. To weave it into fastai, we will create a custom fastai model.\nAll the code below is heavily inspired by–>\n\nAyushman’s notebook.\nfastai siamese tutorial.\n\nWe will also take into account the structure of fastai’s create_cnn_model class. The code for which is as follows\n\n\nCode\ncreate_cnn_model??\n\n\nLet’s build our own.\nWe will cut off the head of a timm pretrained model using create_body and take the encoder only as this would be the portion of the pretrained model which I would like to use. Then I will top it off with a custom fastai head using create_head that we would need to train on our target data.\nTo know more about this flow have a look into the fastai siamese tutorial.\nBut first we will create our custom create_body and create_head functions. the reason for this is that fastai in it’s current state is not integrated with the timm library. So, creating custom versions of create_body and create_head makes the weaving of timm into fastai re-usable.\nThe insipration for this is the post in ‘walk with fastai’. Once again the code and the approach is based on this post.\n\nI am recreating this again here instead of using the ‘walk with fastai’ library is to drill down into the concept and for my personal learning."
  },
  {
    "objectID": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#create_timm_bodyarch-n_in3-pretrainedtrue-cutnone",
    "href": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#create_timm_bodyarch-n_in3-pretrainedtrue-cutnone",
    "title": "Extending-Fastai-For-Custom-Task",
    "section": "create_timm_body(arch, n_in=3, pretrained=True, cut=None)",
    "text": "create_timm_body(arch, n_in=3, pretrained=True, cut=None)\n\n\nCode\n#export\ndef create_timm_body(arch, n_in=3, pretrained=True, cut=None):\n    \"Cut off the body of a typically pretrained timm library `arch` as determined by `cut`\"\n    model = create_model(arch, pretrained=pretrained, num_classes=0, in_chans=1,global_pool='')\n    _update_first_layer(model, n_in, pretrained)\n    #cut = ifnone(cut, cnn_config(arch)['cut'])\n    if cut is None:\n        ll = list(enumerate(model.children()))\n        cut = next(i for i,o in reversed(ll) if has_pool_type(o))\n    if   isinstance(cut, int):      return nn.Sequential(*list(model.children())[:cut])\n    elif callable(cut): return cut(model)\n    else:  raise NamedError(\"cut must be either integer or a function\")\n\n\nNow that we have a way to create a body, we will use the code from create_cnn_model to build our custom create_timm_model.\nThe code for create_timm_model is as follows.\n\n\nCode\ncreate_cnn_model??"
  },
  {
    "objectID": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#create_timm_model",
    "href": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#create_timm_model",
    "title": "Extending-Fastai-For-Custom-Task",
    "section": "create_timm_model",
    "text": "create_timm_model\n\n\nCode\ncreate_head?\n\n\n\n\nCode\n#export\n@delegates(create_head)\ndef create_timm_model(arch, n_out, pretrained=True, cut=None, n_in=3, init=nn.init.kaiming_normal_, custom_head=None,\n                     concat_pool=True, in_chans=1, **kwargs):\n    \"Create custom architecture from the timm library\"\n    body = create_timm_body(arch, n_in, pretrained, None)\n    if custom_head is None:\n        nf = num_features_model(nn.Sequential(*body.children()))\n        head = create_head(nf, n_out, concat_pool=concat_pool, **kwargs)\n    else: head = custom_head\n    model = nn.Sequential(body, head)\n    if init is not None: apply_init(model[1], init)\n    return model\n\n\nThe @delegate macro tells fastai to show the parameters of any **kwargs (which we would be using in the create_body) during function introspection.\nLet’s do a quick test to check if our custom model works.\n\n\nCode\n# num of classes\nn_out = 2\n\n\n\n\nCode\nmodel = create_timm_model(\"efficientnet_b3a\", n_out)\n\n\nDownloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_b3_ra2-cf984f9c.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b3_ra2-cf984f9c.pth\n\n\n\n\nCode\nL(model.children())\n\n\n(#2) [Sequential(\n  (0): Conv2d(1, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n  (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (2): SiLU(inplace=True)\n  (3): Sequential(\n    (0): Sequential(\n      (0): DepthwiseSeparableConv(\n        (conv_dw): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n        (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): SiLU(inplace=True)\n        (se): SqueezeExcite(\n          (conv_reduce): Conv2d(40, 10, kernel_size=(1, 1), stride=(1, 1))\n          (act1): SiLU(inplace=True)\n          (conv_expand): Conv2d(10, 40, kernel_size=(1, 1), stride=(1, 1))\n          (gate): Sigmoid()\n        )\n        (conv_pw): Conv2d(40, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): Identity()\n      )\n      (1): DepthwiseSeparableConv(\n        (conv_dw): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n        (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): SiLU(inplace=True)\n        (se): SqueezeExcite(\n          (conv_reduce): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1))\n          (act1): SiLU(inplace=True)\n          (conv_expand): Conv2d(6, 24, kernel_size=(1, 1), stride=(1, 1))\n          (gate): Sigmoid()\n        )\n        (conv_pw): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): Identity()\n      )\n    )\n    (1): Sequential(\n      (0): InvertedResidual(\n        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): SiLU(inplace=True)\n        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): SiLU(inplace=True)\n        (se): SqueezeExcite(\n          (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n          (act1): SiLU(inplace=True)\n          (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n          (gate): Sigmoid()\n        )\n        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): InvertedResidual(\n        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): SiLU(inplace=True)\n        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): SiLU(inplace=True)\n        (se): SqueezeExcite(\n          (conv_reduce): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n          (act1): SiLU(inplace=True)\n          (conv_expand): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n          (gate): Sigmoid()\n        )\n        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): InvertedResidual(\n        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): SiLU(inplace=True)\n        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): SiLU(inplace=True)\n        (se): SqueezeExcite(\n          (conv_reduce): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n          (act1): SiLU(inplace=True)\n          (conv_expand): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n          (gate): Sigmoid()\n        )\n        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (2): Sequential(\n      (0): InvertedResidual(\n        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): SiLU(inplace=True)\n        (conv_dw): Conv2d(192, 192, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=192, bias=False)\n        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): SiLU(inplace=True)\n        (se): SqueezeExcite(\n          (conv_reduce): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n          (act1): SiLU(inplace=True)\n          (conv_expand): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n          (gate): Sigmoid()\n        )\n        (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): InvertedResidual(\n        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): SiLU(inplace=True)\n        (conv_dw): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)\n        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): SiLU(inplace=True)\n        (se): SqueezeExcite(\n          (conv_reduce): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n          (act1): SiLU(inplace=True)\n          (conv_expand): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n          (gate): Sigmoid()\n        )\n        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): InvertedResidual(\n        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): SiLU(inplace=True)\n        (conv_dw): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)\n        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): SiLU(inplace=True)\n        (se): SqueezeExcite(\n          (conv_reduce): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n          (act1): SiLU(inplace=True)\n          (conv_expand): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n          (gate): Sigmoid()\n        )\n        (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (3): Sequential(\n      (0): InvertedResidual(\n        (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): SiLU(inplace=True)\n        (conv_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)\n        (bn2): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): SiLU(inplace=True)\n        (se): SqueezeExcite(\n          (conv_reduce): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n          (act1): SiLU(inplace=True)\n          (conv_expand): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n          (gate): Sigmoid()\n        )\n        (conv_pwl): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): InvertedResidual(\n        (conv_pw): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): SiLU(inplace=True)\n        (conv_dw): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n        (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): SiLU(inplace=True)\n        (se): SqueezeExcite(\n          (conv_reduce): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n          (act1): SiLU(inplace=True)\n          (conv_expand): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n          (gate): Sigmoid()\n        )\n        (conv_pwl): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): InvertedResidual(\n        (conv_pw): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): SiLU(inplace=True)\n        (conv_dw): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n        (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): SiLU(inplace=True)\n        (se): SqueezeExcite(\n          (conv_reduce): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n          (act1): SiLU(inplace=True)\n          (conv_expand): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n          (gate): Sigmoid()\n        )\n        (conv_pwl): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (3): InvertedResidual(\n        (conv_pw): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): SiLU(inplace=True)\n        (conv_dw): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n        (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): SiLU(inplace=True)\n        (se): SqueezeExcite(\n          (conv_reduce): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n          (act1): SiLU(inplace=True)\n          (conv_expand): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n          (gate): Sigmoid()\n        )\n        (conv_pwl): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (4): InvertedResidual(\n        (conv_pw): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): SiLU(inplace=True)\n        (conv_dw): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n        (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): SiLU(inplace=True)\n        (se): SqueezeExcite(\n          (conv_reduce): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n          (act1): SiLU(inplace=True)\n          (conv_expand): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n          (gate): Sigmoid()\n        )\n        (conv_pwl): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (4): Sequential(\n      (0): InvertedResidual(\n        (conv_pw): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): SiLU(inplace=True)\n        (conv_dw): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n        (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): SiLU(inplace=True)\n        (se): SqueezeExcite(\n          (conv_reduce): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n          (act1): SiLU(inplace=True)\n          (conv_expand): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n          (gate): Sigmoid()\n        )\n        (conv_pwl): Conv2d(576, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): InvertedResidual(\n        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): SiLU(inplace=True)\n        (conv_dw): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): SiLU(inplace=True)\n        (se): SqueezeExcite(\n          (conv_reduce): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n          (act1): SiLU(inplace=True)\n          (conv_expand): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n          (gate): Sigmoid()\n        )\n        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): InvertedResidual(\n        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): SiLU(inplace=True)\n        (conv_dw): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): SiLU(inplace=True)\n        (se): SqueezeExcite(\n          (conv_reduce): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n          (act1): SiLU(inplace=True)\n          (conv_expand): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n          (gate): Sigmoid()\n        )\n        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (3): InvertedResidual(\n        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): SiLU(inplace=True)\n        (conv_dw): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): SiLU(inplace=True)\n        (se): SqueezeExcite(\n          (conv_reduce): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n          (act1): SiLU(inplace=True)\n          (conv_expand): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n          (gate): Sigmoid()\n        )\n        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (4): InvertedResidual(\n        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): SiLU(inplace=True)\n        (conv_dw): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): SiLU(inplace=True)\n        (se): SqueezeExcite(\n          (conv_reduce): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n          (act1): SiLU(inplace=True)\n          (conv_expand): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n          (gate): Sigmoid()\n        )\n        (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (5): Sequential(\n      (0): InvertedResidual(\n        (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): SiLU(inplace=True)\n        (conv_dw): Conv2d(816, 816, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=816, bias=False)\n        (bn2): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): SiLU(inplace=True)\n        (se): SqueezeExcite(\n          (conv_reduce): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n          (act1): SiLU(inplace=True)\n          (conv_expand): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n          (gate): Sigmoid()\n        )\n        (conv_pwl): Conv2d(816, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): InvertedResidual(\n        (conv_pw): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): SiLU(inplace=True)\n        (conv_dw): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n        (bn2): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): SiLU(inplace=True)\n        (se): SqueezeExcite(\n          (conv_reduce): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n          (act1): SiLU(inplace=True)\n          (conv_expand): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n          (gate): Sigmoid()\n        )\n        (conv_pwl): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): InvertedResidual(\n        (conv_pw): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): SiLU(inplace=True)\n        (conv_dw): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n        (bn2): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): SiLU(inplace=True)\n        (se): SqueezeExcite(\n          (conv_reduce): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n          (act1): SiLU(inplace=True)\n          (conv_expand): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n          (gate): Sigmoid()\n        )\n        (conv_pwl): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (3): InvertedResidual(\n        (conv_pw): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): SiLU(inplace=True)\n        (conv_dw): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n        (bn2): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): SiLU(inplace=True)\n        (se): SqueezeExcite(\n          (conv_reduce): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n          (act1): SiLU(inplace=True)\n          (conv_expand): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n          (gate): Sigmoid()\n        )\n        (conv_pwl): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (4): InvertedResidual(\n        (conv_pw): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): SiLU(inplace=True)\n        (conv_dw): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n        (bn2): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): SiLU(inplace=True)\n        (se): SqueezeExcite(\n          (conv_reduce): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n          (act1): SiLU(inplace=True)\n          (conv_expand): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n          (gate): Sigmoid()\n        )\n        (conv_pwl): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (5): InvertedResidual(\n        (conv_pw): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): SiLU(inplace=True)\n        (conv_dw): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n        (bn2): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): SiLU(inplace=True)\n        (se): SqueezeExcite(\n          (conv_reduce): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n          (act1): SiLU(inplace=True)\n          (conv_expand): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n          (gate): Sigmoid()\n        )\n        (conv_pwl): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (6): Sequential(\n      (0): InvertedResidual(\n        (conv_pw): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): SiLU(inplace=True)\n        (conv_dw): Conv2d(1392, 1392, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1392, bias=False)\n        (bn2): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): SiLU(inplace=True)\n        (se): SqueezeExcite(\n          (conv_reduce): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n          (act1): SiLU(inplace=True)\n          (conv_expand): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n          (gate): Sigmoid()\n        )\n        (conv_pwl): Conv2d(1392, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): InvertedResidual(\n        (conv_pw): Conv2d(384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(2304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): SiLU(inplace=True)\n        (conv_dw): Conv2d(2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)\n        (bn2): BatchNorm2d(2304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): SiLU(inplace=True)\n        (se): SqueezeExcite(\n          (conv_reduce): Conv2d(2304, 96, kernel_size=(1, 1), stride=(1, 1))\n          (act1): SiLU(inplace=True)\n          (conv_expand): Conv2d(96, 2304, kernel_size=(1, 1), stride=(1, 1))\n          (gate): Sigmoid()\n        )\n        (conv_pwl): Conv2d(2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (4): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  (5): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (6): SiLU(inplace=True)\n),Sequential(\n  (0): AdaptiveConcatPool2d(\n    (ap): AdaptiveAvgPool2d(output_size=1)\n    (mp): AdaptiveMaxPool2d(output_size=1)\n  )\n  (1): Flatten(full=False)\n  (2): BatchNorm1d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (3): Dropout(p=0.25, inplace=False)\n  (4): Linear(in_features=3072, out_features=512, bias=False)\n  (5): ReLU(inplace=True)\n  (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (7): Dropout(p=0.5, inplace=False)\n  (8): Linear(in_features=512, out_features=2, bias=False)\n)]\n\n\ncool! so it works.\nNow, we will build a learner which would enable us to do transfer learning with timm models. Once again we will port cnn_learner for our use and like before let’s quickly take a look into the cnn_learner code\n\n\nCode\ncnn_learner??\n\n\n\n\nCode\n#export\n@delegates(create_timm_model)\ndef timm_learner(dls, arch, n_out=None, pretrained=True,\n                # learner args\n                loss_func=None, opt_func=Adam, lr=defaults.lr, splitter=None, cbs=None, metrics=None, path=None,\n                model_dir='models', wd=None, wd_bn_bias=False, train_bn=True, moms=(0.95,0.85,0.95),\n                # other model args\n                **kwargs):\n    \"Build a convnet style learner from `dls` and `timm arch`\"\n\n    kwargs = {**kwargs}\n    if n_out is None: n_out = get_c(dls)\n    assert n_out, \"`n_out` is not defined, and could not be inferred from data, set `dls.c` or pass `n_out`\"\n    model = create_timm_model(arch, n_out, default_split, pretrained, **kwargs)\n\n    learn = Learner(dls=dls, model=model, loss_func=loss_func, opt_func=opt_func, lr=lr, splitter=default_split, cbs=cbs,\n                   metrics=metrics, path=path, model_dir=model_dir, wd=wd, wd_bn_bias=wd_bn_bias, train_bn=train_bn,\n                   moms=moms)\n    if pretrained: learn.freeze()\n    # keep track of args for loggers\n    store_attr('arch,n_out,pretrained', self=learn, **kwargs)\n    return learn\n\n\nHere we go. We have managed to get a port of the learner code which looks the part. Does it work?\nLet me find out.\n\nTo find the list of models available in the timm library use list_models\n\n\n\nCode\nlist_models(\"efficient*\")\n\n\n['efficientnet_b0',\n 'efficientnet_b1',\n 'efficientnet_b1_pruned',\n 'efficientnet_b2',\n 'efficientnet_b2_pruned',\n 'efficientnet_b2a',\n 'efficientnet_b3',\n 'efficientnet_b3_pruned',\n 'efficientnet_b3a',\n 'efficientnet_b4',\n 'efficientnet_b5',\n 'efficientnet_b6',\n 'efficientnet_b7',\n 'efficientnet_b8',\n 'efficientnet_cc_b0_4e',\n 'efficientnet_cc_b0_8e',\n 'efficientnet_cc_b1_8e',\n 'efficientnet_el',\n 'efficientnet_el_pruned',\n 'efficientnet_em',\n 'efficientnet_es',\n 'efficientnet_es_pruned',\n 'efficientnet_l2',\n 'efficientnet_lite0',\n 'efficientnet_lite1',\n 'efficientnet_lite2',\n 'efficientnet_lite3',\n 'efficientnet_lite4',\n 'efficientnetv2_l',\n 'efficientnetv2_m',\n 'efficientnetv2_rw_m',\n 'efficientnetv2_rw_s',\n 'efficientnetv2_s']"
  },
  {
    "objectID": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#the-learner",
    "href": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#the-learner",
    "title": "Extending-Fastai-For-Custom-Task",
    "section": "The learner👩‍🏫",
    "text": "The learner👩‍🏫\nNow that we have the model in place, we can go ahead and create the learner the usual way. We have kept the batch size to default.\nThere is one little thing that I would like to do before creating a learner. I will create a helper function which can help me to get the suggested learning rate quickly.\n\n\nCode\n#export\ndef show_me_lrs(learn, num_it:int= 10):\n    Suggested_lrs = namedtuple('Suggested_lrs', [\"min\", \"steep\",\n                                            \"valley\", \"slide\"])\n    lrs = learn.lr_find(suggest_funcs=(minimum, steep,valley, slide))\n    suggested_lrs = Suggested_lrs(lrs[0], lrs[1], lrs[2], lrs[3])\n    \n    print(f\"Minimum/10:\\t{lrs[0]:.2e}\\\n          \\nSteepest point:\\t{lrs[1]:.2e}\\\n          \\nLongest valley:\\t{lrs[2]:.2e}\\\n          \\nSlide interval:\\t{lrs[3]:.2e}\")\n    \n    return suggested_lrs\n\n\n\n\nCode\nlearn = timm_learner(dls, 'efficientnet_b7', loss_func=CrossEntropyLossFlat(), metrics=[RocAucBinary(axis=0)], n_out=2).to_fp16()\n\n\nFit one epoch to see how it behaves\n\n\nCode\nlearn.fit_one_cycle(1, 3e-3)\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      roc_auc_score\n      time\n    \n  \n  \n    \n      0\n      1.216163\n      0.700841\n      0.482039\n      00:37\n    \n  \n\n\n\n\n\nCode\n#to recover gpu ram\nlearn.save('epoch1')\nlearn.load('epoch1')\n\n\n<fastai.learner.Learner at 0x7fdc87c14890>\n\n\nUsing the learning rate finder to get the learning rate\n\n\nCode\nimport gc; gc.collect()\n\n\n66411\n\n\n\n\nCode\nsuggested_lrs = show_me_lrs(learn)\n#learn.lr_find(suggest_funcs=(minimum, steep,valley, slide))\n\n\n\n\n\n/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored\n  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)\n/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored\n  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)\n/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored\n  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)\n/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored\n  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)\n/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored\n  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)\n/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored\n  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)\n/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored\n  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)\n/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored\n  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)\n/opt/conda/lib/python3.7/site-packages/fastai/callback/schedule.py:269: UserWarning: color is redundantly defined by the 'color' keyword argument and the fmt string \"ro\" (-> color='r'). The keyword argument will take precedence.\n  ax.plot(val, idx, 'ro', label=nm, c=color)\n\n\nMinimum/10: 1.00e-06          \nSteepest point: 1.10e-06          \nLongest valley: 6.92e-06          \nSlide interval: 4.37e-03\n\n\n\n\n\nI will use the slide algorithm here to get the optimal learning rate.\n\n\nCode\nlearn.unfreeze()\nlearn.fit_one_cycle(3, suggested_lrs.slide)\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      roc_auc_score\n      time\n    \n  \n  \n    \n      0\n      1.114650\n      1.825437\n      0.465078\n      00:36\n    \n    \n      1\n      1.011949\n      0.750606\n      0.491870\n      00:37\n    \n    \n      2\n      0.917761\n      0.708508\n      0.516685\n      00:37\n    \n  \n\n\n\n/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored\n  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)\n/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored\n  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)\n/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored\n  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)\n/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored\n  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)\n/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored\n  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)\n/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored\n  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)\n/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored\n  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)\n/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored\n  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)\n/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored\n  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)\n/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored\n  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)\n/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored\n  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)\n/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored\n  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)\n\n\nOk! The performance is not that great but the goal of this exercise was not to have a SOTA model but rather to learn how to create a custom code base by using Fastai internals.\nHowever, with proper data augmentation and more data the performance can be much better.\n\n\nCode\nlearn.export(\"./final\")"
  },
  {
    "objectID": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#inference",
    "href": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#inference",
    "title": "Extending-Fastai-For-Custom-Task",
    "section": "Inference🧐",
    "text": "Inference🧐\nFor inference you will need to use the previous dataloader to create a test dataloader by passing the test files to it.\n\n\nCode\n%%time\ntest_path = path/'g2net-gravitational-wave-detection/test'\ntest_files = getfiles(test_path, \"npy\")\n\n\nCPU times: user 2.65 s, sys: 1.22 s, total: 3.86 s\nWall time: 1min 5s"
  },
  {
    "objectID": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#inference-1",
    "href": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#inference-1",
    "title": "Extending-Fastai-For-Custom-Task",
    "section": "Inference",
    "text": "Inference\nFor inference we first load the learner\n\n\nCode\nlearn = load_learner('./final', cpu=False)\n\n\nCreate a test dataloader. This will take in the test files and apply the transforms that we had created during trainign timebut on the inference data and give you a dataloader.\n\n\nCode\ntest_dls = learn.dls.test_dl(test_files[:100])\n\n\ncheck the batch\n\n\nCode\ntest_dls.show_batch()\n\n\n/opt/conda/lib/python3.7/site-packages/nnAudio/utils.py:326: SyntaxWarning: If fmax is given, n_bins will be ignored\n  warnings.warn('If fmax is given, n_bins will be ignored',SyntaxWarning)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse get_preds to get predictions in batches.\n\n\nCode\npreds = learn.get_preds(dl=test_dls)\n\n\n\n\n\nHave a look at your predictions.\n\n\nCode\npreds\n\n\n(tensor([[0.5247, 0.4753],\n         [0.5247, 0.4753],\n         [0.5248, 0.4752],\n         [0.5247, 0.4753],\n         [0.5252, 0.4748],\n         [0.5249, 0.4751],\n         [0.4635, 0.5365],\n         [0.5260, 0.4740],\n         [0.5246, 0.4754],\n         [0.5247, 0.4753],\n         [0.4650, 0.5350],\n         [0.5247, 0.4753],\n         [0.5270, 0.4730],\n         [0.5247, 0.4753],\n         [0.5247, 0.4753],\n         [0.4974, 0.5026],\n         [0.5246, 0.4754],\n         [0.5246, 0.4754],\n         [0.5247, 0.4753],\n         [0.4956, 0.5044],\n         [0.4867, 0.5133],\n         [0.4694, 0.5306],\n         [0.5264, 0.4736],\n         [0.5246, 0.4754],\n         [0.5247, 0.4753],\n         [0.5247, 0.4753],\n         [0.4893, 0.5107],\n         [0.4874, 0.5126],\n         [0.4887, 0.5113],\n         [0.5246, 0.4754],\n         [0.5247, 0.4753],\n         [0.5246, 0.4754],\n         [0.5248, 0.4752],\n         [0.5247, 0.4753],\n         [0.4840, 0.5160],\n         [0.5248, 0.4752],\n         [0.5247, 0.4753],\n         [0.4933, 0.5067],\n         [0.5247, 0.4753],\n         [0.5247, 0.4753],\n         [0.4694, 0.5306],\n         [0.4978, 0.5022],\n         [0.4897, 0.5103],\n         [0.5247, 0.4753],\n         [0.4885, 0.5115],\n         [0.4894, 0.5106],\n         [0.5246, 0.4754],\n         [0.5247, 0.4753],\n         [0.4868, 0.5132],\n         [0.5247, 0.4753],\n         [0.4903, 0.5097],\n         [0.5247, 0.4753],\n         [0.5247, 0.4753],\n         [0.4875, 0.5125],\n         [0.5250, 0.4750],\n         [0.4724, 0.5276],\n         [0.4901, 0.5099],\n         [0.5250, 0.4750],\n         [0.5247, 0.4753],\n         [0.4883, 0.5117],\n         [0.4836, 0.5164],\n         [0.4875, 0.5125],\n         [0.5246, 0.4754],\n         [0.4853, 0.5147],\n         [0.4876, 0.5124],\n         [0.5247, 0.4753],\n         [0.4884, 0.5116],\n         [0.4890, 0.5110],\n         [0.5247, 0.4753],\n         [0.4846, 0.5154],\n         [0.5247, 0.4753],\n         [0.5246, 0.4754],\n         [0.5247, 0.4753],\n         [0.4892, 0.5108],\n         [0.4853, 0.5147],\n         [0.4899, 0.5101],\n         [0.4841, 0.5159],\n         [0.5247, 0.4753],\n         [0.4905, 0.5095],\n         [0.4673, 0.5327],\n         [0.5246, 0.4754],\n         [0.5677, 0.4323],\n         [0.4856, 0.5144],\n         [0.5247, 0.4753],\n         [0.4878, 0.5122],\n         [0.5247, 0.4753],\n         [0.5259, 0.4741],\n         [0.4909, 0.5091],\n         [0.5004, 0.4996],\n         [0.4859, 0.5141],\n         [0.5247, 0.4753],\n         [0.5247, 0.4753],\n         [0.5247, 0.4753],\n         [0.5246, 0.4754],\n         [0.5246, 0.4754],\n         [0.5171, 0.4829],\n         [0.5247, 0.4753],\n         [0.5275, 0.4725],\n         [0.4877, 0.5123],\n         [0.4890, 0.5110]]),\n None)"
  },
  {
    "objectID": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#conclusion",
    "href": "posts/Extending-Fastai-For-Custom-Task/Extending-Fastai-For-Custom-Task.html#conclusion",
    "title": "Extending-Fastai-For-Custom-Task",
    "section": "Conclusion",
    "text": "Conclusion\nI prepared this post as part of my experimentation for the g2net-gravitational-wave-detection competition. My goal for preparing this notebook was to design an end-to-end flow to learn about extending fastai for a custom new task and how to extend the library to work well with other libraries.\nIt took quite a long time to get my head around the low-level and mid level API in fastai.\nPart of the reason being that I couldn’t spend much time on this competition and the other part was that there are very few resources available at this moment which provide good detail about creating custom bits using fastai’s mid-level and low-level APIs.\nI would like to say that the effort that it took to complete this post was worth it and I came to know how powerful the modular structure of fastai is.\nI would like to create an extension library using the code that I have developed for this post but at this moment I can’t say how soon I would be able to do it and when but stay tuned as I would keep posting my progress on this."
  },
  {
    "objectID": "posts/Can-we-use-neural-network-to-understand-data/Can-we-use-neural-network-to-understand-data.html",
    "href": "posts/Can-we-use-neural-network-to-understand-data/Can-we-use-neural-network-to-understand-data.html",
    "title": "Can-we-use-neural-network-to-understand-data",
    "section": "",
    "text": "This notebook can also available in kaggle\nGet the code used in this note book on github.\nThe data used can be found here"
  },
  {
    "objectID": "posts/Can-we-use-neural-network-to-understand-data/Can-we-use-neural-network-to-understand-data.html#what-is-this",
    "href": "posts/Can-we-use-neural-network-to-understand-data/Can-we-use-neural-network-to-understand-data.html#what-is-this",
    "title": "Can-we-use-neural-network-to-understand-data",
    "section": "What is this ?",
    "text": "What is this ?\n\nA starter notebook for folks new to machine learning.\nA vanilla CNN created for whale species recognition competition using fastai.\nAim is to observe the behavior of a simple CNN trained to recognize the whale species.\nHere I am trying to show that you can use a simple CNN to know what areas of an image does the CNN “sees”. You can use this to analyze what needs to be improved or engineered in the data to create a better model."
  },
  {
    "objectID": "posts/Can-we-use-neural-network-to-understand-data/Can-we-use-neural-network-to-understand-data.html#import-required-things.",
    "href": "posts/Can-we-use-neural-network-to-understand-data/Can-we-use-neural-network-to-understand-data.html#import-required-things.",
    "title": "Can-we-use-neural-network-to-understand-data",
    "section": "Import required things.",
    "text": "Import required things.\n\n\nCode\nfrom fastai.vision.all import *\n\n\n\nRequired paths\n\n\nCode\nroot_path = Path(\"../input\")\ntrain_csv_path = root_path/'happy-whale-and-dolphin/train.csv'\ntrain_img_path = root_path/\"jpeg-happywhale-128x128/train_images-128-128/train_images-128-128\""
  },
  {
    "objectID": "posts/Can-we-use-neural-network-to-understand-data/Can-we-use-neural-network-to-understand-data.html#get-a-glimpse-of-the-files",
    "href": "posts/Can-we-use-neural-network-to-understand-data/Can-we-use-neural-network-to-understand-data.html#get-a-glimpse-of-the-files",
    "title": "Can-we-use-neural-network-to-understand-data",
    "section": "Get a glimpse of the files",
    "text": "Get a glimpse of the files\nget_image_files is a convenience function that goes through different folders and subfolders and gathers all the image file path.\n\n\nCode\ntrain_imgs = get_image_files(train_img_path)\ntrain_imgs[:10]\n\n\n(#10) [Path('../input/jpeg-happywhale-128x128/train_images-128-128/train_images-128-128/80b5373b87942b.jpg'),Path('../input/jpeg-happywhale-128x128/train_images-128-128/train_images-128-128/e113b51585c677.jpg'),Path('../input/jpeg-happywhale-128x128/train_images-128-128/train_images-128-128/94eb976e25416c.jpg'),Path('../input/jpeg-happywhale-128x128/train_images-128-128/train_images-128-128/19a45862ab99cd.jpg'),Path('../input/jpeg-happywhale-128x128/train_images-128-128/train_images-128-128/be9645065510e9.jpg'),Path('../input/jpeg-happywhale-128x128/train_images-128-128/train_images-128-128/76d25044120d3c.jpg'),Path('../input/jpeg-happywhale-128x128/train_images-128-128/train_images-128-128/0c64a705ba5d31.jpg'),Path('../input/jpeg-happywhale-128x128/train_images-128-128/train_images-128-128/c1fe278bdbd837.jpg'),Path('../input/jpeg-happywhale-128x128/train_images-128-128/train_images-128-128/6f94f30ac500a0.jpg'),Path('../input/jpeg-happywhale-128x128/train_images-128-128/train_images-128-128/15f295322e5b54.jpg')]\n\n\n\n\nCode\nImage.open(train_imgs[1])\n\n\n\n\n\nlet’s see what is there in the csv file.\n\n\nCode\ntrain_path_df = pd.read_csv(train_csv_path)\n\n\n\n\nCode\ntrain_path_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      image\n      species\n      individual_id\n    \n  \n  \n    \n      0\n      00021adfb725ed.jpg\n      melon_headed_whale\n      cadddb1636b9\n    \n    \n      1\n      000562241d384d.jpg\n      humpback_whale\n      1a71fbb72250\n    \n    \n      2\n      0007c33415ce37.jpg\n      false_killer_whale\n      60008f293a2b\n    \n    \n      3\n      0007d9bca26a99.jpg\n      bottlenose_dolphin\n      4b00fe572063\n    \n    \n      4\n      00087baf5cef7a.jpg\n      humpback_whale\n      8e5253662392\n    \n  \n\n\n\n\nIf I can catch hold of the image filename from the dataframe and attach this to the train_img_path then I can get the full image path.\nReplacing filnames with the filepath\n\n\nCode\ntrain_path_df['image'] = train_path_df['image'].apply(lambda x:train_img_path/x)\n\n\n\n\nCode\ntrain_path_df.head(2)\n\n\n\n\n\n\n  \n    \n      \n      image\n      species\n      individual_id\n    \n  \n  \n    \n      0\n      ../input/jpeg-happywhale-128x128/train_images-128-128/train_images-128-128/00021adfb725ed.jpg\n      melon_headed_whale\n      cadddb1636b9\n    \n    \n      1\n      ../input/jpeg-happywhale-128x128/train_images-128-128/train_images-128-128/000562241d384d.jpg\n      humpback_whale\n      1a71fbb72250\n    \n  \n\n\n\n\n\n\nCode\nImage.open(train_path_df.image[0])"
  },
  {
    "objectID": "posts/Can-we-use-neural-network-to-understand-data/Can-we-use-neural-network-to-understand-data.html#datablock",
    "href": "posts/Can-we-use-neural-network-to-understand-data/Can-we-use-neural-network-to-understand-data.html#datablock",
    "title": "Can-we-use-neural-network-to-understand-data",
    "section": "Datablock",
    "text": "Datablock\nA DataBlock is like a blueprint that tells fastai–> * What kind of data are we dealing with. Is it image, text etc. * What kind of labels we have. For example categorical labels, continuous labels etc. * From where to get the inputs. * From where to get the targets. * How to split the data into train and test. * The transforms that you want to do.\n\n\nCode\ndata = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                splitter=TrainTestSplitter(),\n                 get_x = ColReader(0),\n                 get_y = ColReader(1),\n                 item_tfms=Resize(224),\n                 batch_tfms=aug_transforms(size=128))\n\n\nIn the above code–> * The ImageBlock tells fastai that we are dealing with image data. * CategoryBlock means our targets are categorical in nature. * The get_x and get_y tells fastai to get the x i.e. the image path from column 0 of dataframe and the target from column 1 of the dataframe. * splitter is the way in which train and test data are split. check here for more details. * The next two lines are not that important to understand now but they are needed when you are working on image augmentation. Refer to the fastbook to understand more about this.\n\n\nCode\n# this is not necessary. I was just testing if my datablocks are correct or not\n#data.summary(train_path_df)"
  },
  {
    "objectID": "posts/Can-we-use-neural-network-to-understand-data/Can-we-use-neural-network-to-understand-data.html#dataloader",
    "href": "posts/Can-we-use-neural-network-to-understand-data/Can-we-use-neural-network-to-understand-data.html#dataloader",
    "title": "Can-we-use-neural-network-to-understand-data",
    "section": "Dataloader",
    "text": "Dataloader\nThink of a dataloader as a mechanism that picks up your images, does all the things that you had described in the datablock and then loads your data to the device(cpu or gpu).\nAt the minimum, you need to provide the source of data, the train_path_df in our case. Here we give the bs i.e. the batch size. For example if we say that the batch size is 8 then it means that the dataloader will load 8 images at a time onto the device.\n\n\nCode\ndls = data.dataloaders(train_path_df, bs=128)\n\n\n/root/.local/lib/python3.7/site-packages/torch/_tensor.py:1023: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release.\ntorch.linalg.solve has its arguments reversed and does not return the LU factorization.\nTo get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack.\nX = torch.solve(B, A).solution\nshould be replaced with\nX = torch.linalg.solve(A, B) (Triggered internally at  /pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:760.)\n  ret = func(*args, **kwargs)\n\n\n\n\nCode\ndls.show_batch()"
  },
  {
    "objectID": "posts/Can-we-use-neural-network-to-understand-data/Can-we-use-neural-network-to-understand-data.html#the-model",
    "href": "posts/Can-we-use-neural-network-to-understand-data/Can-we-use-neural-network-to-understand-data.html#the-model",
    "title": "Can-we-use-neural-network-to-understand-data",
    "section": "The model",
    "text": "The model\nSimple CNN to classify whale/dolphin species. The intuition is the following–> * a model which knows to recognize whale/dolphin species should also be able to learn the features to distinguish the different species. * such a model can be queried to learn what all portions of an image is taken into account. For example, if somehow the background is tricking the model such that the model considers the background as an area of interest then such data needs to be further engineered.\n\nsuch model can then be used as a pre-trained model to recognize individuals ."
  },
  {
    "objectID": "posts/Can-we-use-neural-network-to-understand-data/Can-we-use-neural-network-to-understand-data.html#creating-a-simple-cnn-learner",
    "href": "posts/Can-we-use-neural-network-to-understand-data/Can-we-use-neural-network-to-understand-data.html#creating-a-simple-cnn-learner",
    "title": "Can-we-use-neural-network-to-understand-data",
    "section": "Creating a simple CNN learner",
    "text": "Creating a simple CNN learner\ncnn_learner creates a convolutional neural network for you. At the least you provide it the dataloader, a model architecture and a loss function.\nmodels.resnet18 is a “pre-trained” neural network. It’s a network that was already trained by some good folks (folks with huge computational hardware) on a very large number of images. This particular network knows how to recognize different images. So, we take this network and then ask fastai to train this network on our image data (whale, dolphins).\nThe intuition is that since the pre-trained network already knows how to distinguish between different images, we have to spend less time and computation to make it understand how to recognize different whales and dolphins. This is Transfer learning\n\n\nCode\nlearn = cnn_learner(dls, models.resnet18, loss_func=CrossEntropyLossFlat(), ps=0.25)\n\n\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n\n\n\n\n\n/root/.local/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n\n\nFor now understand that fine_tune means “take my network and make it look at each image a specified number of times. This”number of times” is the number that is provided as the first argument.\nLet’s not dive into the second argument at this moment in time.\n\nFor the curious mind, the second argument is known as the learning rate. It’s a hyperparameter of a neural network. More information can be found here\n\n\n\nCode\nlearn.fine_tune(2, 3e-3)\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.905516\n      0.673760\n      03:58\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.422199\n      0.337718\n      02:24\n    \n    \n      1\n      0.231187\n      0.232147\n      02:24\n    \n  \n\n\n\nWe can see what the model predicted vs the actual target.\n\n\nCode\nlearn.show_results()\n\n\n\n\n\n\n\n\n\n\nCode\ninterp = Interpretation.from_learner(learn)\ninterp.plot_top_losses(3, figsize=(15,10))"
  },
  {
    "objectID": "posts/Can-we-use-neural-network-to-understand-data/Can-we-use-neural-network-to-understand-data.html#why-the-neural-network-makes-the-decision",
    "href": "posts/Can-we-use-neural-network-to-understand-data/Can-we-use-neural-network-to-understand-data.html#why-the-neural-network-makes-the-decision",
    "title": "Can-we-use-neural-network-to-understand-data",
    "section": "Why the neural network makes the decision?",
    "text": "Why the neural network makes the decision?\nWe can use class activation maps (CAM) to see which regions of images are of interest to the neural network. Class activation map (CAM) was first introduced by Bolei Zhou et al. in “Learning Deep Features for Discriminative Localization”. Cam uses the output of the last convolutional layer with the predictions to give a heatmap of the regions of interest of the network in an image.\nThe intuition behind CAM is that if we do the dot product of the activations of the final layer with the final weights, for each location on our feature map then we can get the score of the feature that was used to make a decision.\nPytorch provides hooks to hook onto the any layer of a network. We can attach a hook to any layer of a neural network and it will be executed during the forward pass i.e. the moment when the output is computed or during the backward pass i.e. the moment when the weights are being re-adjusted.\nI would highly recommend to read more about CAM in this chapter of fastbook before proceeding on with the code.\n\n\nCode\nImage.open(train_path_df.image[6])\n\n\n\n\n\nLet’s grab a batch.\n\n\nCode\nimg = PILImage.create(train_path_df.image[6])\n# grab a batch\nx, = first(dls.test_dl([img]))\n\n\nWe hook into the network . A forward hook takes into three things –> the model , its input and it’s output. The fourth line in the below code is the hook function.\n\n\nCode\nclass Hook():\n    def __init__(self, m):\n        self.hook = m.register_forward_hook(self.hook_func)   \n    def hook_func(self, m, i, o): self.stored = o.detach().clone()\n    def __enter__(self, *args): return self\n    def __exit__(self, *args): self.hook.remove()\n\n\nNow we do the dot product of our weight matrix with the activations.\nThe code below takes the model, hooks into the last layer of our model using our hook function and stores the activations in the act variable. Then we do the dot product of the stored activations with the weights using torch.einsum.\n\n\nCode\nwith Hook(learn.model[0]) as hook:\n    with torch.no_grad(): \n        output = learn.model.eval()(x.cuda())\n        act = hook.stored[0]\n    cam_map = torch.einsum('ck,kij->cij', learn.model[1][-1].weight, act)\n    cam_map.shape\n\n\nFor each image in our batch, and for each class, we get a 7×7 feature map that tells us where the activations were higher and where they were lower. This will let us see which areas of the pictures influenced the model’s decision.\n\n\nCode\nx_dec = TensorImage(dls.train.decode((x,))[0][0])\n_,ax = plt.subplots()\nx_dec.show(ctx=ax)\nax.imshow(cam_map[1].detach().cpu(), alpha=0.6, extent=(0,128,128,0),\n              interpolation='bilinear', cmap='magma');\n\n\n\n\n\nThe bright areas in the activations are somehow spilling over to the ocean. In my opinion this should not be the case. The activations should be more around the of the dorsal fins and the surface of the fin. In layman terms the network should focus more on the object i.e. the fin features and not on the surrounding water.\nAn important takeaway from my perspective is that if we can restrict the region of interest to the whale fin and not the surrounding water then the network would be able to learn better."
  },
  {
    "objectID": "posts/Can-we-use-neural-network-to-understand-data/Can-we-use-neural-network-to-understand-data.html#conclusion",
    "href": "posts/Can-we-use-neural-network-to-understand-data/Can-we-use-neural-network-to-understand-data.html#conclusion",
    "title": "Can-we-use-neural-network-to-understand-data",
    "section": "Conclusion",
    "text": "Conclusion\nBesides tasks like classifying images or detecting objects, a neural networks can be used for analyzing the data to device strategies for pre-processing, data engineering and data cleaning.\nHere I have bounced off one idea about how a simple CNN can be used to understand the regions in an image that the neural network focuses to make it’s decisions and then analyzing these things we can observer if the network is focusing on areas of images that it needs to focus and if not, then we can think of the steps that we can take to help the network to focus to generalize better.\nI have just scratched the surface of what’s possible here and there can be many more ways in which deep learning can be used to take better decisions on data. So, I would emphasize you to expand on this idea and think of other ways in which you can use simple networks to analyze the data before building an architecture for the actual task at hand."
  },
  {
    "objectID": "posts/image-restoration-series/chapter1-deblur.html",
    "href": "posts/image-restoration-series/chapter1-deblur.html",
    "title": "Chapter 1 - Deblur",
    "section": "",
    "text": "Note\n\n\n\nExecute this notebook on kaggle by clicking here\nPhotography is one of my hobbies and a good percentage of my photography workflow is spent on correcting various artifacts in images. I always wondered if there is I could efficiently automate the process of image correction/restoration but I didn’t pay much attention to image restoration using deep learning. So, for the past couple of months I started playing around with different techniques in the image restoration side of deep learning in the hopes of creating a tool that would assist me in image restoration part of my photography workflow and whatever I have learnt till now, I am putting into a series of articles.\nThis is first in a series of such articles where we will try to build a image correction tool by implementing some cool generative imaging techniques using Fastai and pytorch. Infact this entire series is inspired by the fastai course from the year 2018.\nAt the end of the article I will also post links to different resources from which I got to know a lot about GANs and related techniques.\nAt the begining of this notebook we will install all the required packages.\nThen we will import the required modules."
  },
  {
    "objectID": "posts/image-restoration-series/chapter1-deblur.html#downloading-the-data",
    "href": "posts/image-restoration-series/chapter1-deblur.html#downloading-the-data",
    "title": "Chapter 1 - Deblur",
    "section": "Downloading the data",
    "text": "Downloading the data\nI have created a tiny library “fastaibreadcrumbs”. It provides some convinience functions that extends a few of the functionalities available in the “fastkaggle” library.\n“fastaibreadcrumbs” provides a setup_data using which you can pull any kaggle non-competition dataset onto your machine. Just provide the username from the kaggle dataset page and the dataset name.\n\n\n\n\n\n\nNote\n\n\n\nIf you are trying to pull a competition dataset then fastkaggle has an equivalent function setup_comp.\n\n\nWe will pull the first dataset (the one I collected from pexels.com)\n\n\nCode\npath1 = setup_data('sapal6', 'superresolution')\n\n\nWarning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n\n\n\n\nCode\npath1.ls()\n\n\n(#11) [Path('superresolution/trains'),Path('superresolution/nature'),Path('superresolution/kids-playing'),Path('superresolution/fireworks'),Path('superresolution/busystreet'),Path('superresolution/dogs-running'),Path('superresolution/sports'),Path('superresolution/underwater'),Path('superresolution/dance'),Path('superresolution/wildlife')...]\n\n\nThen we will pull the other dataset from kaggle which I will combine with my dataset.\n\nSometimes more data helps. While experimenting for this tutorial, I found that the amount of data that I had in my dataset was not enough and adding a few more samples expanded the variation in the input. That’s why I augmented my dataset with some external data (which in this case was another dataset from kaggle).\n\n\n\n\n\n\n\nTip\n\n\n\nIf your model’s results are not at par then you should first try to bring in more training data (if it’s possible because in many domains more data is simply hard to come by). If addign more data doesn’t improve your model then you should look to other techniques.\n\n\n\n\nCode\npath2 = setup_data('thaihoa1476050', 'df2k-ost')\n\n\n\n\nCode\npath2.ls()\n\n\n(#2) [Path('df2k-ost/train'),Path('df2k-ost/test')]\n\n\nThe next thing that I am going to do is to create a config sort of thing that would help me to store frequently used values.\n\n\n\n\n\n\nTip\n\n\n\nIn real world a better way is to create a config file in your project.\n\n\n\n\nCode\nconfig = {'root':Path(\".\")}"
  },
  {
    "objectID": "posts/image-restoration-series/chapter1-deblur.html#crappifying",
    "href": "posts/image-restoration-series/chapter1-deblur.html#crappifying",
    "title": "Chapter 1 - Deblur",
    "section": "Crappifying",
    "text": "Crappifying\nIn order to get the target images which in our case are the images having “camera shake” in them, we need to simulate motion blur. “fastaibreadcrumbs” provides Crappifier which takes in images and creates another version of the same image that has motion blur in it.\nFirst let’s get the source and destination paths.\n\n\nCode\nconfig['path_hr1'] = path1\nconfig['path_hr2'] = path2\n\n\n\n\nCode\nconfig['path_crappy'] = Path('crappy')\n\n\nfastai provides get_image_files which fetches all teh image files from a given path. We will use this get the images from our first dataset.\n\n\nCode\nfiles_hr1 = get_image_files(config['path_hr1'])\n\n\n\n\nCode\nfiles_hr1[:10]\n\n\n(#10) [Path('superresolution/trains/0055.jpg'),Path('superresolution/trains/0002.jpg'),Path('superresolution/trains/0067.jpg'),Path('superresolution/trains/0022.jpg'),Path('superresolution/trains/0012.jpg'),Path('superresolution/trains/0019.jpg'),Path('superresolution/trains/0009.jpg'),Path('superresolution/trains/0051.jpg'),Path('superresolution/trains/0047.jpg'),Path('superresolution/trains/0072.jpg')]\n\n\nsimilarly we will get the images from teh second dataset path.\n\n\nCode\nfiles_hr2 = get_image_files(config['path_hr2'])\n\n\nLet’s crappify a single image to check if things work.\n\n\nCode\nCrappifier(config[\"path_crappy\"])(files_hr1[0])\n\n\nLet’s grab our crappified image.\n\n\nCode\nfiles_crappy1 = get_image_files(config['path_crappy'])\n\n\n\n\nCode\nfiles_crappy1[0]\n\n\nPath('crappy/path_860/trains/0055.jpg')\n\n\n“fastaibreadcrumbs” provides the convinience function show_plot which let’s you display two images side by side.\n\n\n\n\n\n\nNote\n\n\n\nIf you look into the source code of show_plot it’s only a few lines of code. Under the hood it’s only matplotlib functions. So, if you don’t want to use show_plot then you can create your own plotting function.\n\n\n\n\nCode\nshow_plot(files_hr1[0], files_crappy1[0], 1, (10, 10))\n\n\n\n\n\n\n\nCode\n#cleaning up the crappy directory.\nshutil.rmtree(config['path_crappy'])\n\n\nWhile feeding data into a neural network, all teh images needs to be in same size. So, let’s first check the sizes of our training images.\nThere is a neat trick which I learned from Jeremy Howard’s notebook to get the image size. I am using the same code here.\n\n\nCode\nfrom fastcore.parallel import *\n\n\n\n\nCode\ndef f(o): return PILImage.create(o).size\nsizes = parallel(f, files_hr1+files_hr2, n_workers=4)\npd.Series(sizes).value_counts()\n\n\n/usr/local/lib/python3.9/dist-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/usr/local/lib/python3.9/dist-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n\n\n(1880, 1253)    4147\n(867, 1300)     2608\n(2040, 1356)    1435\n(496, 368)       637\n(1733, 1300)     623\n                ... \n(464, 416)         1\n(1803, 1300)       1\n(1756, 1300)       1\n(1235, 1300)       1\n(2040, 1740)       1\nLength: 2790, dtype: int64\n\n\nNow, there are a variety of image sizes and orientations but most of the images have there sizes in the range of a minimum of 800ish(shortest side) and 1300is(longest side). So, I will randomly pick a proportion which allows me to resize an image having teh longest side as 860. I could also have picked up a higher size but then the compute required would have been higher.\nFastai can resize images while loading data but all those computation would take time and the training speed might get affected on low power devices (for example my laptop which has a humble gpu). We may end up resizing the images to still lower sizes during the creation of dataloader but an initial resizing really helps to save some computing during the training time.\n\nThese resizing tricks to save computing and speeding up training is what I got to know from Jeremy Howard’s notebook and from my own experiments that I did on my local machine and my experiments done on kaggle kernel. Your experience might vary depending upon the computing power that you have access to. So, again feel free to experiment.\n\nI will create a config for the path where I am goign to store my resized images.\n\n\nCode\nconfig['path_860'] = Path('path_860')\n\n\nfastai provides resize_images which resizes your source images to the desired size. The below function resz_imgs is just a wrapper on top of fastai’s resize_images. Pass on the source and destination paths and pass the maximum size that you want your images to be resized to. resize_images will then resize the images to the new proportions while keeping the longest side to the given max_sz and accordingly modify the shortest side.\n\n\nCode\ndef resz_imgs(source: Path, dest: Path, max_sz: int):\n    resize_images(source, dest=dest, max_size=max_sz, recurse=True)\n\n\nLet’s resize the first dataset.\n\n\nCode\n#for first set\nresz_imgs(config['path_hr1'], config['path_860'], max_sz=860)\n\n\nThen the second dataset.\n\n\nCode\n#for second set\nresz_imgs(config['path_hr2'], config['path_860'], max_sz=860)\n\n\nNext, we are going to crappify our resized images and we are going to do that in parallel. Do do things in parallel we will use parallel provided by fastcore. It’s similar to python standard library’s parallel function but much more convenient.\n\n\n\n\n\n\n📒 Side Note\n\n\n\nfastcore is a underrated python library which supercharges python.\n\n\nFor parallel crappification, I will use crappify_imgs provided by “fastaibreadcrumbs” which let’s you provide the high resolution image path and the destination path, strength of the blur (as sz).\n\n\nCode\ncrappify_imgs(config['path_860'], config['path_crappy'], n_workers=8)"
  },
  {
    "objectID": "posts/image-restoration-series/chapter1-deblur.html#grabbing-the-data",
    "href": "posts/image-restoration-series/chapter1-deblur.html#grabbing-the-data",
    "title": "Chapter 1 - Deblur",
    "section": "Grabbing the data",
    "text": "Grabbing the data\nNow that crappification is done, let’s grab our input and target data. We can do this by using fastai Datablock API. Datablock let’s you customize the way you want to grab your input and output.\n\n\nCode\nfiles_crappy = get_image_files(config['path_crappy'])\nfiles_crappy[:4]\n\n\n(#4) [Path('crappy/path_860/trains/0055.jpg'),Path('crappy/path_860/trains/0002.jpg'),Path('crappy/path_860/trains/0067.jpg'),Path('crappy/path_860/trains/0022.jpg')]\n\n\n\n\nCode\nfiles_crappy[0].relative_to(config['path_crappy'])\n\n\nPath('path_860/trains/0055.jpg')\n\n\n\n\nCode\ndblock = DataBlock(blocks=(ImageBlock, ImageBlock),\n                       get_y = lambda x: x.relative_to(config['path_crappy']),\n                       splitter=RandomSplitter(),\n                       item_tfms=Resize(480),\n                       batch_tfms=[*aug_transforms(max_zoom=2.), Normalize.from_stats(*imagenet_stats)])\n\n\nIn the first line we describe outr blocks which are the x and y. Here, our x and y both are images and thus we provide Imageblock as the blocks of choice. Then we use lambda function to tell fastai that we are expecting the crappy images as our y, whose filename match with the filename of the input images. After defining our labels (y) we tell fastai to randomly split our images into training and test data. Then we call the Resize since we want all our images to be of same size. Lastly we do some augmentations.\n\n\nCode\ndls = dblock.dataloaders(files_crappy, bs=4)\n\n\n\n\nCode\ndls.show_batch()\n\n\n\n\n\n\nFeature loss\nIn the paper “Perceptual Losses for Real-Time Style Transfer and Super-Resolution the authors proposed a kind of loss known as perceptual loss which would compare different feature maps of the target image with the feature map of the generated image to see if teh generated image’s features are the same as the actual image. This can otherwise be known as “feature loss” as described in the Fastai course 2019 lesson 7.\nThe below doodle gives a very simple overview of the method used by the authors of the “Perceptual Losses for Real-Time Style Transfer and Super-Resolution paper\n\n\n\nIn the proposed method, the hourglass shaped figure is a pre-cursor to “Unet” which is made up of a encoder for learning the features of an input image and a decoder which would reconstruct the image while upscaling the image.\n\nThis is similar to super-resolution where an U-net is used to scale up the resolution of an image. Such a network can also be used for tasks like improving the quality of a crappy image.\n\nThe colorful rectangle in the above figure is a pre-trained image model like vgg-16 which feeds on teh image generated by the U-net. While the generated image passes through the pre-trained image model, the feature map of the generated images are grabbed from the intermidiate layers of the vgg-16 model. The target image (actual data) is alos passed through this vgg-16 model it’s feature map is also extracted from teh intermidiate layers. After this is done, the feature map of the generated image is compared with the feature map of the target image. This is done by the feature loss function which is then used to re-train the U-net in order to make the generated model appear as close as possible to the actual thing."
  },
  {
    "objectID": "posts/image-restoration-series/chapter1-deblur.html#extracting-features",
    "href": "posts/image-restoration-series/chapter1-deblur.html#extracting-features",
    "title": "Chapter 1 - Deblur",
    "section": "Extracting features",
    "text": "Extracting features\nIf recall the diagram from previous section, you will notice that we need to extract the features from our input image as well as the target image which we can compare later. To extract the features, we will use a simple pre-trained network like “vgg16”.\nThis below is a representation of the pre-trained network which we would be using to extract the features from our images. The sections in this image represents different layers of the network and the downward arrows represent the extraction of the features.\n\n\n\nThe features are grabbed just before the grid size changes and the maxpooling layer in network is where the grid size change occurs."
  },
  {
    "objectID": "posts/image-restoration-series/chapter1-deblur.html#feature-loss-1",
    "href": "posts/image-restoration-series/chapter1-deblur.html#feature-loss-1",
    "title": "Chapter 1 - Deblur",
    "section": "Feature Loss",
    "text": "Feature Loss\nTaking all the previous things into account, we will use a pre-trained vgg16 to create a loss function that will help the network to compare the pixels of the target and the input image and check if the two images are the same.\n“fastaibreadcrumbs” provides calc_ft_loss which does the heavy lifting of initiating a vgg16 pre-trained model and calculating the feature loss for you.\n\nIf you wan to know more about what happens under the hood then I feel free to read through the fastai course(2018) notebook here and watch the lecture here\n\n\n\nCode\nfeat_loss = calc_ft_loss()\n\n\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and will be removed in 0.15. Please use keyword parameter(s) instead.\n  warnings.warn(\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_BN_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\" to /root/.cache/torch/hub/checkpoints/vgg16_bn-6c64b313.pth"
  },
  {
    "objectID": "posts/image-restoration-series/chapter1-deblur.html#grabbing-data",
    "href": "posts/image-restoration-series/chapter1-deblur.html#grabbing-data",
    "title": "Chapter 1 - Deblur",
    "section": "Grabbing data",
    "text": "Grabbing data\nLike before we have to create a dataloader which would load adata onto the device. We can use fastai’s dataloader for this purpose but for convenience I will use the get_unet_dls from “fastaibreadcrumbs” which is simply a wrapper over a fastai datablock with two Imageblocks (one Imageblock for input and one Imageblock for target) and a dataloader.\n\n\nCode\nconfig\n\n\n{'root': Path('.'),\n 'path_hr1': Path('superresolution'),\n 'path_hr2': Path('df2k-ost'),\n 'path_crappy': Path('crappy'),\n 'path_860': Path('path_860')}\n\n\n…setting some parameters like weight decay, nymber of epochs etc…\n\n\nCode\n# hyperparameters\nconfig['epoch'] = 4\nconfig['wd'] = 1e-3\n\n\n…then we will set our item transformations. These are the transformations which will be applied to each item when they are loaded. Here, we will reduce the size of images to a smaller size. We will use Resize for this. Then we will setup our “batch transformations”, these are the transformations which will be applied to the images once they are grouped into batches. In our case we will apply image augmentations to our batches which will do things like rotate the images, brighten up the images, darken the images, flip the images etc. After that we will setup the method by the way of which we want to grab our target images. Here, we will use a path relative to our crappy images because the directory structure and the image name of the crappy images matches with that of our input images.\n\nTraining on a smaller image size will make our training faster and this will enabel use to iterate faster in case we want to try a lots of things to find the best settings fro our training. There is one more reason due to which we may want to start with a smaller image size. We are going to use something known as “progressive resizing” , whereby we start training with small images and then use the model trained on these smaller images as a pre-trained model for another training setup where we use slightly bigger images. This makes the trainign a lot faster then starting with big images from the get go. Remember that our goal is to make our training as fast as possible.\n\n\n\nCode\n#transformations\nitem_tfms = Resize(128)\nbatch_tfms = [*aug_transforms(max_zoom=2.), Normalize.from_stats(*imagenet_stats)]\nget_y = lambda x: x.relative_to(config['path_crappy'])\n\n\nAfter all these are done, we will pass the following to the get_unet_dls function –>\n\nbatch size\nsource i.e. the crappy file names\nget_y i.e. the way to fetch our targets\nsplitter i.e. how to split our data into trainign and validation sets.\nitem_tfms i.e. item transformations.\nbatch_tfms i.e. batch transformations.\n\n\n\nCode\ndls= get_unet_dls(8, source = files_crappy, get_y = get_y, \n                     splitter = RandomSplitter(), item_tfms = item_tfms,\n                     batch_tfms = batch_tfms)\n\n\nlet’s see our samples.\n\n\nCode\ndls.show_batch()"
  },
  {
    "objectID": "posts/image-restoration-series/chapter1-deblur.html#unet",
    "href": "posts/image-restoration-series/chapter1-deblur.html#unet",
    "title": "Chapter 1 - Deblur",
    "section": "Unet",
    "text": "Unet\nNow that we have our feature loss and data ready, let’s create a unet.\nCreating a basic unet is a one liner affair in fastai. Ofcourse, you can do all sorts of customization to your unet but I will use the minimalist way here and take advantage of the high level API in fastai.\nWe will use the unet_learner parameters recommended here.\n\n\nCode\nunet_learn = unet_learner(dls, models.resnet34, loss_func=feat_loss,\n                          metrics=LossMetrics(feat_loss.metric_names),\n                          blur=True, norm_type=NormType.Weight)\n\ngc.collect();\n\n\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n\n\n\n\n\nNotice that I have passed something like LossMetrics(feat_loss.metric_names) to the unet. This is our metric which will tell us how our unet is progressing. Where does this comes from?\nWell if you look into the source code of the Feature_loss-\nclass FeatureLoss(Module):\n    \"\"\"Class to calculate feature loss\"\"\"\n    def __init__(self, m_feat, layer_ids, layer_wgts):\n        self.m_feat = m_feat\n        self.loss_features = [self.m_feat[i] for i in layer_ids]\n        self.hooks = hook_outputs(self.loss_features, detach=False)\n        self.wgts = layer_wgts\n        self.metric_names = ['pixel',] + [f'feat_{i}' for i in range(len(layer_ids))\n              ] + [f'gram_{i}' for i in range(len(layer_ids))]\n\n    def make_features(self, x, clone=False):\n        self.m_feat(x)\n        return [(o.clone() if clone else o) for o in self.hooks.stored]\n    \n    def forward(self, input, target, reduction='mean'):\n        out_feat = self.make_features(target, clone=True)\n        in_feat = self.make_features(input)\n        self.feat_losses = [pixel_loss(input,target,reduction=reduction)]\n        self.feat_losses += [pixel_loss(f_in, f_out,reduction=reduction)*w\n                             for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]\n        self.feat_losses += [pixel_loss(gram_matrix(f_in), gram_matrix(f_out),reduction=reduction)*w**2 * 5e3\n                             for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]\n        if reduction=='none': \n            self.feat_losses = [f.mean(dim=[1,2,3]) for f in self.feat_losses[:4]] + [f.mean(dim=[1,2]) for f in self.feat_losses[4:]]\n        for n,l in zip(self.metric_names, self.feat_losses): setattr(self, n, l)\n        return sum(self.feat_losses)\n    \n    def __del__(self): self.hooks.remove()\n…notice that we have something called metric_names there. This thing is the collection of the pixel loss and gram loss which can then be used as a metric by us.\nfastai provides the lr_find function which we will use to find a suitable learning rate.\n\n\nCode\nlr = unet_learn.lr_find(suggest_funcs=(valley, slide))\n\n\n\n\n\n\n\n\n\n\n\n\nlr_find has a few different methods of finding the learning rate but the “valley” and “slide” functions provide suitable learning rate most of the time.\nThe lr_find suggests very optimistic values of learning rates so that we don’t screw up our training but another thing that we can do is to look at the learning rate suggested by the “valley” and “slide” functions and then take a value in between these two.\n\n\nCode\n(lr.valley, lr.slide)\n\n\n(0.0003311311302240938, 0.0006918309954926372)\n\n\nWe will train the unet using the “one cycle policy”. From 50,000 feet, the one cycle policy can be explained like this “A one cycle policy trains the model with large and oscillating (changing between different values) learning rate in order to make the training faster and more accurate”. A more detailed explanation can be found in this cool article.\nThere is one more trick that I am going to use in my unet_learner. I have experimented with quite a different versions of this notebook with variety of data (variations both in nterms of number of images and amount of data) and I have found that with data variations it becomes difficult for me to predict that a particular choice of “epoch numbers” will work everytime. So, I will need to tell fastai that when we need to stop training if my loss is not improving and save my modell whenever a “good” enough loss state is reached. This can be done by the EarlyStoppingCallback and SaveModelCallback. The former stops the model trainign if the loss doesn’t improve further and the latter will save the best possible model. I have provided links to the official documentation of these two functions if you wan tot know more about them.\nBut hey! what’s a callback? Simply put a callback is a function that is passed as an argument to another function. It provides a way for the programmer to modify an existing piece of code without changing the structure of code. With callbacks you can modify the behaviour of your piece of code, say a function without changing the body of your current function. Here is an article that explains callbacks with examples.\n\n\nCode\nunet_learn.fit_one_cycle(config['epoch'], (lr.valley+lr.slide)/2, pct_start=0.9, wd=config['wd'], \n                         cbs=[EarlyStoppingCallback(patience=2), SaveModelCallback(fname='model_128')])\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      pixel\n      feat_0\n      feat_1\n      feat_2\n      gram_0\n      gram_1\n      gram_2\n      time\n    \n  \n  \n    \n      0\n      2.209806\n      2.456991\n      0.185003\n      0.207987\n      0.262833\n      0.114802\n      0.530690\n      0.872372\n      0.283306\n      06:00\n    \n    \n      1\n      2.042598\n      2.254473\n      0.171976\n      0.196098\n      0.245231\n      0.106519\n      0.463559\n      0.806346\n      0.264745\n      06:03\n    \n    \n      2\n      1.886767\n      2.064314\n      0.150376\n      0.181493\n      0.225683\n      0.098219\n      0.433128\n      0.729233\n      0.246182\n      06:01\n    \n    \n      3\n      1.694850\n      1.856245\n      0.135413\n      0.170341\n      0.208545\n      0.090624\n      0.365436\n      0.657052\n      0.228834\n      06:04\n    \n  \n\n\n\nBetter model found at epoch 0 with valid_loss value: 2.45699143409729.\nBetter model found at epoch 1 with valid_loss value: 2.2544734477996826.\nBetter model found at epoch 2 with valid_loss value: 2.0643138885498047.\nBetter model found at epoch 3 with valid_loss value: 1.8562445640563965.\n\n\nwe can view the predictions on the validation set using show_results\n\n\nCode\nunet_learn.show_results()"
  },
  {
    "objectID": "posts/image-restoration-series/chapter1-deblur.html#progressive-resizing",
    "href": "posts/image-restoration-series/chapter1-deblur.html#progressive-resizing",
    "title": "Chapter 1 - Deblur",
    "section": "Progressive resizing",
    "text": "Progressive resizing\nEven with an image size of 128px our model is doing a good job of removing the motion blur and generating the blur free images. Now, let’s use this model as a pre-trained model fro our next training phase where we will increase the image size to 256px.\n\n\nCode\n#transformations\nitem_tfms = Resize(256)\nbatch_tfms = [*aug_transforms(max_zoom=2.), Normalize.from_stats(*imagenet_stats)]\nget_y = lambda x: x.relative_to(config['path_crappy'])\n\n\n\n\nCode\ndls= get_unet_dls(8, source = files_crappy, get_y = get_y, \n                     splitter = RandomSplitter(), item_tfms = item_tfms,\n                     batch_tfms = batch_tfms)\n\n\n\n\nCode\ndls.show_batch()\n\n\n\n\n\nsince we have re-initialized our unet_learner, we would need to assign the new dataloader (the one with 256px images) to the learner.\n\n\nCode\nunet_learn.dls = dls\n\n\nLoad the model that was trained on the 128px images.\n\n\nCode\nunet_learn.load('model_128');\n\n\n/usr/local/lib/python3.9/dist-packages/fastai/learner.py:58: UserWarning: Saved filed doesn't contain an optimizer state.\n  elif with_opt: warn(\"Saved filed doesn't contain an optimizer state.\")\n\n\nRemember that our input image size have changed now and so has the parameters of our model. Due to this we will have to find new learning rate.\n\n\nCode\nlr = unet_learn.lr_find(suggest_funcs=(valley, slide))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n(lr.valley, lr.slide)\n\n\n(9.120108734350652e-05, 0.0006918309954926372)\n\n\nNext, we will fine tune our model on the new set of data. With fine_tune we ask fastai to freeze the model, train it for a epoch, unfreeze it and then train for some more epochs.\nIn case you wan to understand more about fine_tune, then here is the source code and here is an old thread in the fastai forum where fine_tune was discussed.\n\n\nCode\nunet_learn.fine_tune(5, (lr.valley+lr.slide)/2, pct_start=0.9, wd=config['wd'], \n                         cbs=[EarlyStoppingCallback(patience=2), SaveModelCallback(fname='model_256')])\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      pixel\n      feat_0\n      feat_1\n      feat_2\n      gram_0\n      gram_1\n      gram_2\n      time\n    \n  \n  \n    \n      0\n      1.473990\n      1.519641\n      0.186348\n      0.215888\n      0.219089\n      0.080457\n      0.296886\n      0.394380\n      0.126594\n      16:53\n    \n  \n\n\n\nBetter model found at epoch 0 with valid_loss value: 1.519641399383545.\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      pixel\n      feat_0\n      feat_1\n      feat_2\n      gram_0\n      gram_1\n      gram_2\n      time\n    \n  \n  \n    \n      0\n      1.412022\n      1.459381\n      0.178994\n      0.212948\n      0.214645\n      0.078533\n      0.275205\n      0.375495\n      0.123562\n      17:30\n    \n    \n      1\n      1.368803\n      1.436245\n      0.174595\n      0.210241\n      0.212634\n      0.077483\n      0.268589\n      0.370479\n      0.122224\n      17:30\n    \n    \n      2\n      1.368343\n      1.437706\n      0.169373\n      0.206416\n      0.211114\n      0.077285\n      0.278587\n      0.373089\n      0.121841\n      17:31\n    \n    \n      3\n      1.324890\n      1.393757\n      0.164672\n      0.203151\n      0.207758\n      0.075826\n      0.261442\n      0.361697\n      0.119211\n      17:30\n    \n    \n      4\n      1.313353\n      1.355986\n      0.158461\n      0.200111\n      0.204974\n      0.074636\n      0.248391\n      0.351761\n      0.117653\n      17:30\n    \n  \n\n\n\nBetter model found at epoch 0 with valid_loss value: 1.4593812227249146.\nBetter model found at epoch 1 with valid_loss value: 1.4362448453903198.\nBetter model found at epoch 3 with valid_loss value: 1.393756628036499.\nBetter model found at epoch 4 with valid_loss value: 1.3559863567352295.\n\n\nLet’s check the predictions on the validation set.\n\n\nCode\nunet_learn.show_results()\n\n\n\n\n\n\n\n\n\n\n\n\nSo, the results here have improved as compared to the 128px images. You see that’s how progressive resizing helps. Starting with smaller sized images makes training faster.\nFrom this point onwards you can keep on increasing the image size and keep training until the performance of your model doesn’t improve much."
  },
  {
    "objectID": "posts/image-restoration-series/chapter1-deblur.html#observations-from-my-experiments",
    "href": "posts/image-restoration-series/chapter1-deblur.html#observations-from-my-experiments",
    "title": "Chapter 1 - Deblur",
    "section": "Observations from my experiments",
    "text": "Observations from my experiments\nI have actually done many different experiments on this and I have the followign observations –>\n\nWhen sufficient data is available, training for a 4-5 epochs during each stage of progressive re-sizing is sufficient to generate “decently blur free” images.\nI have got decent results during training done on only 200 images but there were some noticeable artifacts in the generated images. An interesting thing I noticed was that the motion blur was removed.\nIn my experiments even when I didn’t continue the training after I had trained the model on 256px images, decent results were produced on bigger sized images during inference."
  },
  {
    "objectID": "posts/image-restoration-series/chapter1-deblur.html#inference",
    "href": "posts/image-restoration-series/chapter1-deblur.html#inference",
    "title": "Chapter 1 - Deblur",
    "section": "Inference",
    "text": "Inference\nLet’s test our model on unseen data. For this I have downloaded new images (which were not inlcuded in teh training data) from pexels.com.\n\n\nCode\nconfig['test_860'] = Path(\"test_860\")\nconfig['test_crappy'] = Path(\"test_crappy\")\n\n\n\n\nCode\nfiles_test = get_image_files(config['test_860'])\n\n\n\n\nCode\nfiles_test\n\n\n(#2) [Path('test_860/sports/pexels-pixabay-248547.jpg'),Path('test_860/signpost/signpost.jpg')]\n\n\n…and crappified them like before.\n\n\nCode\ncrappify_imgs(config['test_860'], config['test_crappy'], sz=80, n_workers=8)\n\n\n\n\nCode\nfiles_crappy = get_image_files(config['test_crappy'])\n\n\nWe also need to grab our dataloader with the same type of transformations as before but this time we will take a 860px image because we are trying to generate “full size” blur free image.\n\n\nCode\n#transformations\nitem_tfms = Resize(860)\nbatch_tfms = [*aug_transforms(max_zoom=2.), Normalize.from_stats(*imagenet_stats)]\nget_y = lambda x: x.relative_to(config['test_crappy'])\n\n\n\n\nCode\ndls= get_unet_dls(2, source = files_crappy, get_y = get_y, \n                     splitter = RandomSplitter(), item_tfms = item_tfms,\n                     batch_tfms = batch_tfms)\n\n\n\n\nCode\ndls.show_batch()\n\n\n\n\n\nCreate the Unet learner as before.\n\n\nCode\nunet_learn = unet_learner(dls, models.resnet34, loss_func=F.l1_loss,\n                     blur=True, norm_type=NormType.Weight).load(config['root']/'model_256')\n\n\nBefore prediction we will drop any sort of augmentation from our data.\n\n\nCode\ndl = dls.train.new(shuffle=False, drop_last=False, \n                       after_batch=[IntToFloatTensor, Normalize.from_stats(*imagenet_stats)])\n\n\n“fastaibreadcrumbs” provides save_preds, to which we pass the dataloader, learner and the path. This function will generate the predictionsa nd save them.\n\n\nCode\nsave_preds(dl, unet_learn, \"gen_imgs\")\n\n\n\n\n\n\n\n\n\n\n\nCode\npreds = [Path(\"gen_imgs\")/fn.name for fn in files_crappy]\n\n\n“fastaibreadcrumbs” provides compare_imgs which takes in the path of the original images, path of crappy images, path of generated images and then compare them side by side.\n\n\nCode\ncompare_imgs(files_test, files_crappy, preds, (100, 100))\n\n\n[[<AxesSubplot:> <AxesSubplot:> <AxesSubplot:>]\n [<AxesSubplot:> <AxesSubplot:> <AxesSubplot:>]]"
  },
  {
    "objectID": "posts/image-restoration-series/chapter1-deblur.html#conclusion",
    "href": "posts/image-restoration-series/chapter1-deblur.html#conclusion",
    "title": "Chapter 1 - Deblur",
    "section": "Conclusion",
    "text": "Conclusion\nLook! With a few epochs of training our model was able to recognize and remove motion blur from images. This is incredible as to how with some clever tricks (like progressive resizing) we can create such powerful deep learning models which runs fast on quite decent machines and with “not so crazy” amount of data. Another thing to note is that we had just used 256px images in our training but even then our model generalizes quite well when a bigger image is given to it. So, a couple of more epochswould produce even better results.\nOne quick thing to note is that you might notice the proportion difference between the original image and the generated images. This is becasue we had resized the images to 860px during training to make the inference faster. If you have access to GPUs with larger memory, then you can give the full size image a shot.\nNow that we have created and trained a model to remove camera shake from our images, in next chapter we will prototype a Graphical tool that can be used as an interface to consume this model."
  },
  {
    "objectID": "posts/image-restoration-series/chapter1-deblur.html#reading-list",
    "href": "posts/image-restoration-series/chapter1-deblur.html#reading-list",
    "title": "Chapter 1 - Deblur",
    "section": "Reading list",
    "text": "Reading list\n\nCode for this tutorial\nSuperresolution- Fastai - Lesson Notebook, Lesson video\nFastai course 2022\n“Perceptual Losses for Real-Time Style Transfer and Super-Resolution\nWalk with fastai"
  },
  {
    "objectID": "posts/image-restoration-series/chapter2-mvp.html",
    "href": "posts/image-restoration-series/chapter2-mvp.html",
    "title": "Chapter 2 - MVP",
    "section": "",
    "text": "Last time we trained a model that was able to remove motion blur from our images. In this chapter we are going to create a very basic application through which we can showcase how our users can use the model on their images."
  },
  {
    "objectID": "posts/image-restoration-series/chapter2-mvp.html#why-we-need-an-app",
    "href": "posts/image-restoration-series/chapter2-mvp.html#why-we-need-an-app",
    "title": "Chapter 2 - MVP",
    "section": "Why we need an app ?",
    "text": "Why we need an app ?\nYou may be thinking that why we need an app? Well! your target user won’t be firing up your jupyter notebook or your code everytime they need to use your model, right?\nThe user needs a visual medium through which they can consume the predictions of your model. That’s why we are goign to need an app."
  },
  {
    "objectID": "posts/image-restoration-series/chapter2-mvp.html#why-build-an-mvp",
    "href": "posts/image-restoration-series/chapter2-mvp.html#why-build-an-mvp",
    "title": "Chapter 2 - MVP",
    "section": "Why build an MVP ?",
    "text": "Why build an MVP ?\nDuring the initial stages of developing an idea into a usable product, you need to focus on speed of iteration. You need to be able to iterate quickly through many different ways to discover the right thing.\nThis is because of this reason that during the initial stage you don’t want to go all out while building a user facing interface. You don’t need a fancy GUI or you don’t need to worry about the hardcore software engineering stages. A prototype application will do fine. So, you need a minimal viable product (MVP) to present your idea to the world and to test if your idea resonates well with the need of the user.\nYou don’t need to be an expert in web designing to build a quick prototype of an application. A little bit of creativity and an open source tool like Gradio is all you need.\n\n\n\n\n\n\nNote\n\n\n\nGradio is an open source library to quickly build web apps for your machine learning model using python."
  },
  {
    "objectID": "posts/image-restoration-series/chapter2-mvp.html#the-design",
    "href": "posts/image-restoration-series/chapter2-mvp.html#the-design",
    "title": "Chapter 2 - MVP",
    "section": "The design",
    "text": "The design\nA good interface provides a good user experience. A state-of-the-art model with an user interface with below average usage experience will not provide any value to the user.\nSo, a design which provides the required ease of usage is a must even during the prototype stage. Of course, the design of the prototype can be kept simple but some basic user experience elements like ease of use should be taken into consideration.\nSo, the first thing that we are going to do is to start with a basic design.\n\n\n\n\n\n\nNote\n\n\n\nIn a real life project you may not need to create designs yourself as bigger projects/organization usually have separate experts, but in some cases you may need to wear the hat of the designer as well.\n\n\nBelow is a hand-drawn design of the prototype UI.\n\nBasically it has a browse button to upload your images (which needs deblurring) and a preview space with a comparison view of the original image and the deblurred image. I would also want to have a button to save the deblurred image.\n\n\n\n\n\n\nNote\n\n\n\nIn a real project you may wan to use tools like “vision” to create UI designs before presenting that to the stakeholders (customers, teams etc.)"
  },
  {
    "objectID": "posts/image-restoration-series/chapter2-mvp.html#importing-the-libraries",
    "href": "posts/image-restoration-series/chapter2-mvp.html#importing-the-libraries",
    "title": "Chapter 2 - MVP",
    "section": "Importing the libraries",
    "text": "Importing the libraries\nFirst let’s import the required libraries.\n\n\nCode\ntry: import gradio as gr\nexcept ModuleNotFoundError:\n    !pip install -Uq gradio\n\n\n\n\nCode\n# install fastkaggle if not available\ntry: from fastaibreadcrumbs.core import *\nexcept ModuleNotFoundError:\n    !pip install -Uq fastaibreadcrumbs\n\n\n\n\nCode\nimport gc\nfrom fastai.vision.all import *\nfrom fastai.vision.gan import *\nfrom fastkaggle import *\nfrom fastaibreadcrumbs.core import *"
  },
  {
    "objectID": "posts/image-restoration-series/chapter2-mvp.html#getting-the-prediciton",
    "href": "posts/image-restoration-series/chapter2-mvp.html#getting-the-prediciton",
    "title": "Chapter 2 - MVP",
    "section": "Getting the prediciton",
    "text": "Getting the prediciton\nWe will use the model that we had trained in chapter1. We will use the same dataloaders and learners which we used in the previous schapter.\nAs is the usual drill, I will create a config dictionary.\n\n\nCode\nconfig = {'path_crappy': Path('crappy')}\n\n\nI have created a quick function below which contains all my transformations and dataloaders from chapter1. If refer the previous chapter, you will notice that all the code used is the same. It’s just that here I have combined those into a single function.\n\n\nCode\ndef get_dls(sz:int,bs:int, src):\n    item_tfms = Resize(sz)\n    batch_tfms = [*aug_transforms(max_zoom=2.), Normalize.from_stats(*imagenet_stats)]\n    get_y = lambda x: x.relative_to(config['path_crappy'])\n    files_crappy = get_image_files(src)\n    \n    dls= get_unet_dls(bs, source = files_crappy, get_y = get_y, \n                     splitter = RandomSplitter(), item_tfms = item_tfms,\n                     batch_tfms = batch_tfms)\n    \n    return dls\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor line 4 you would need to make sure that your target images(the non-crappified images), needs to be in a path that is relative to the path where the crappified images are stored. Now if you are coming to this chpater from chapter1 then you would be having the crappified and non-crappified images in relative paths.\n\n\nI have another function here which creates a unet learner and loads the model that we trained in the previous chapter.\n\n\nCode\ndef get_inf_model(dls, model:str):\n    unet_learn = unet_learner(dls, models.resnet34, loss_func=F.l1_loss,\n                     blur=True, norm_type=NormType.Weight).load(model)\n    \n    return unet_learn\n\n\nNow, I will create the dataloaders. I will pass an image size of 860 to my get_dls functions and a batch size of 8. It’s not necessary to keep the batch size same to what you had kept during the trainign time. You can change the batch size. while training the model (in chapter1) we used a final image size of 256px. Here, during inference time I would like to “debluurify” an image bigger than that. Although, I can have the original “big” size of an image but somehow it crashes the jupyter kernel as soon as prediction is performed by the model. It might be due to limited gpu memory. So, a size of 860px works good enough for experiment.\nFeel free to play around with bigger image size and let me know your findings by posting it on twitter (@thecodingprojec).\n\n\nCode\ndls = get_dls(860, 8, config['path_crappy'])\n\n\nNext, let’s create the learner by passing the dataloaders and the model trained in chapter 1.\n\n\n\n\n\n\nNote\n\n\n\nYou will need to pass the path where you have stored the crappy files which you had used to train your model.\n\n\n\n\nCode\nlearner = get_inf_model(dls, './model_256')\n\n\nI have created a function to get the prediction and save that onto the disk.\n\n\n\n\n\n\nNote\n\n\n\nMake sure to have your final model in the “models” directory and pass the same path to the get_inf_model above.\n\n\n\n\nCode\ndef save_pred(path:str, dest:str,learner):\n    path = Path(path)\n    dest = Path(dest)\n    preds = learner.predict(path)\n    arr = preds[0].numpy().transpose(1,2,0).astype(np.uint8)\n    dest.mkdir(parents=True, exist_ok=True)\n    Image.fromarray(arr).save(dest/path.name)\n    return dest/path.name\n\n\nThe save_pred takes in the source image path, the destionation path and the learner. In line 4 the source image is passed onto the learner and the prediciton is stored in pred. pred is actually a tuple of three things out of which we need only the first item in this tuple i.e. the generated image."
  },
  {
    "objectID": "posts/image-restoration-series/chapter2-mvp.html#the-ui",
    "href": "posts/image-restoration-series/chapter2-mvp.html#the-ui",
    "title": "Chapter 2 - MVP",
    "section": "The UI",
    "text": "The UI\nFirst of all we need a function which would trigger the prediction for us, save it to the disk and then return the path. save_pred from the previous section would take care of generating and saving the prediction. We will wrap this in another function which would then return the path of the generated image.\n\n\nCode\ndef display_result(path):\n    dest = save_pred(path,\"gen_imgs\",learner)\n    return dest\n\n\nNext, we use gradio code to create our Ui components.\n\n\nCode\nwith gr.Blocks() as demo:\n    with gr.Row():\n        image_input = gr.Image(type=\"filepath\")\n        image_output = gr.Image()\n    deblur_btn = gr.Button(\"Deblurrify\")\n    deblur_btn.click(fn=display_result,\n                     inputs=image_input, outputs=image_output)\n\ndemo.launch()\n\n\n\nIn line 1 we ask gradio to initiate something known as “Blocks”. This is an API in gradio which let’s you have more flexibility while creating UI elements. In line 2 we ask gradio to create rows in which we would want to place our UI elements and then in line 3 and 4 we create Image boxes. One image box for our input image and one image box for displaying our output. After this in line 5 we create a button. In line 6 we tie in everything together. Whenever user clicks on the button, display_result function is called with the image from the first image box (defined in line 3) as input and then the generated image will be displayed in the image box define in line 4.\nNow, go ahead and test the UI by dragging on an image of you choice (which has motion blur) and then clicking on the “Deblurrify” button."
  },
  {
    "objectID": "posts/image-restoration-series/chapter2-mvp.html#conclusion",
    "href": "posts/image-restoration-series/chapter2-mvp.html#conclusion",
    "title": "Chapter 2 - MVP",
    "section": "Conclusion",
    "text": "Conclusion\nWhat we have created here is a very early stage prototype application. You will notice that the UI that we created is exactly not same as the design that we imagined but that’s okay in current context. We will iterate on it later on to bring it close to the design.\nYou will also notice that the app code and the supporting code is not suitable enough to be hosted somewhere. For example, we still need to re-create the dataloaders and learner before prediction. This is cumbersome as we would need to re-create the same directory structure of our input data and target data wherever we want to host of app. Also, moving the training data around whenever we want to host our app is not a good way to do things.\nAt the current state since we are testing out our idea, the current state of our code and app is good enough. In the next chapter we would explore some more things like testing a few edge cases to find out where our model fails, optimize our training code further so that trying out different experiments becomes more easier."
  },
  {
    "objectID": "posts/image-restoration-series/chapter2-mvp.html#references",
    "href": "posts/image-restoration-series/chapter2-mvp.html#references",
    "title": "Chapter 2 - MVP",
    "section": "References",
    "text": "References\n\nChapter 1 can be found here.\nCode for this chapter can be found here"
  },
  {
    "objectID": "posts/Nbdev.jl-And-Literate-programming/Nbdev.jl-And-Literate-programming.html",
    "href": "posts/Nbdev.jl-And-Literate-programming/Nbdev.jl-And-Literate-programming.html",
    "title": "Nbdev.jl-And-Literate-programming",
    "section": "",
    "text": "Litrerate programming is a term coined by Donald_Knuth. Under this programming paradigm, one would write human redeable code documentation writing code."
  },
  {
    "objectID": "posts/Nbdev.jl-And-Literate-programming/Nbdev.jl-And-Literate-programming.html#why-this-makes-sense",
    "href": "posts/Nbdev.jl-And-Literate-programming/Nbdev.jl-And-Literate-programming.html#why-this-makes-sense",
    "title": "Nbdev.jl-And-Literate-programming",
    "section": "Why this makes sense ?",
    "text": "Why this makes sense ?\nYou all know that whatever instructions we give to the computer, it “percieves” it in the following way\n\nOops! I got carried away.. It looks more like this\n\nSo, for a computer it’s all binary number. So, what is this that we write when we say that we are writing a computer program ?\n\nSome of you will say this is a programming language, but I ask why it’s called a “language” ? Well! if you ask me I would say that it’s a language and like every other language that exists, it’s sole purpose is to enable communication, share stories, share thoughts. Not to a machine but to another human, that’s why it’s called a programming “language”.\nUltimately this programming instructions get’s converted into machine code but if this piece of code which when I show to a fellow human then that person would know what instructions I want to give a computer. See! that’s communication…that’s what languages do.\nSo, in short we invented programming languages for communicating our thoughts to another person, our thoughts about what we want a machine to do and that’s why we want the programming or coding to be as clear and as human readable as possible."
  },
  {
    "objectID": "posts/Nbdev.jl-And-Literate-programming/Nbdev.jl-And-Literate-programming.html#writing-code-containing-documentation-vs-writing-documentation-containing-code",
    "href": "posts/Nbdev.jl-And-Literate-programming/Nbdev.jl-And-Literate-programming.html#writing-code-containing-documentation-vs-writing-documentation-containing-code",
    "title": "Nbdev.jl-And-Literate-programming",
    "section": "Writing code containing documentation vs writing documentation containing code",
    "text": "Writing code containing documentation vs writing documentation containing code\nUsually we write code the folowing way\n\n\nCode\ndef funny_func(who_is_funny):\n    print(\"I am funny\")\n\n\nOver here the function is self explanatory but to make it extra clear for your friend, you might include an elaborate comment like this\n\n\nCode\ndef funny_func(who_is_funny):\n    \"\"\"function to tell who is funny and who is not.\\\n    Takes in an argument about the name and then prints a string\"\"\"\n    print(\"I am funny\")\n\n\nYou can insert as much elaborate documentation as you may wish. All is well till now.\nBut wait! what would happen when you have thousands of lines of code? Unsurprisengly there would be thousands of lines of documentation on top of that which makes it difficult for a clear communication of your thoughts via that piece of code. Not only this, in all possibilities, you will also end up writing a separate detailed documentation explaining your code.\nThis is the problem that literate programming tries to solve. Under literate programming paradigm you write documentation containing code like this.\n\nIn the above example, you can see that the document contains an heading which is the name of the function followed by a description of what the function is all about then that is followed by the code itself.\nIf I were to show this code document to you then you could easily understand what the code is all about and at teh same time you would also appreciate the “lengthy document free” code. This in short is what a literate programming style would look like."
  },
  {
    "objectID": "posts/Nbdev.jl-And-Literate-programming/Nbdev.jl-And-Literate-programming.html#literate-programming-tool",
    "href": "posts/Nbdev.jl-And-Literate-programming/Nbdev.jl-And-Literate-programming.html#literate-programming-tool",
    "title": "Nbdev.jl-And-Literate-programming",
    "section": "Literate programming tool",
    "text": "Literate programming tool\nTo implement a literate programming style you would need special tools. These tools come under the umbrella of literate programming tools, which read through your document, separate out the code and the documentations and put the code in a source code file and the documentation in a text file.\nWhile talking about literate programming tools, one can not ignore the role of “notebooks” in making it easier for developers to write literate programs. Notebooks are web based interactive computing platforms that let’s you write text and code in a web based editor and let’s you execute your code interactively.\n\nTwo of the notable notebooks are Jupyter notebooks and Pluto.jl. Both of these projects differ in their behaviour and the extent of languages they support but both of these projects have a common goal, that is to provide developers with a web based literate programming and exploratory platform. With these projects you can write your documentation and code in the same notebook, interactively execute your code and also share that across as a reproducible environment.\nDue to the very nature of these notebooks projects, they provide a very good foundation of a literate programming environment and using this foundation, Jeremy howard announced the creation of Nbdev. At a very high level Nbdev is a system that let’s you write your python code and documentation in jupyter notebook and then generates source code files and the related documentation for your project among other things. However, this is a very lazy explanation of Nbdev. So, I suggest you to read the original launch blog and the documentation to truly appreciate the capabilities of Nbdev.\nThe original Nbdev supports the creation of Python projects from a jupyter notebook and it does really superb job at that but there is another popular language ecosystem and notebook environment which the original nbdev doesn’t support. That language is Julia and the notebook environment is Pluto.jl.\nPluto is a reactive notebook environment and differs from jupyter notebook environment in many ways. I would suggest you to explore the project Readme to get an idea about teh awsomeness of Pluto.\nPluto is a great exploratory developement platform and really love using it for my Julia project. I love Julia and python equally 😉 and I am big fan of Nbdev (python version) and use it extensively when I am developing projects in python. I always wanted a literate programming tool similar to Nbdev for Pluto/julia and while searching through the Julia forums found this discussion –>\n\nScrolling down the discussion thread I could see that lot many folks were also in search of something similar. This is when I decided to do something in this direction and started working on porting Nbdev to Julia."
  },
  {
    "objectID": "posts/Nbdev.jl-And-Literate-programming/Nbdev.jl-And-Literate-programming.html#nbdev.jl",
    "href": "posts/Nbdev.jl-And-Literate-programming/Nbdev.jl-And-Literate-programming.html#nbdev.jl",
    "title": "Nbdev.jl-And-Literate-programming",
    "section": "Nbdev.jl",
    "text": "Nbdev.jl\nLast month I released the alpha version of Nbdev.jl. With Nbdev.jl I hope to work towards building a developer tool which would do justice to the awesome user experience that is provided by the original Nbdev.\nThis is just the begining and there are lots of things that I need to fix and many things which are yet to come to Nbdev.jl. However, if you would like to take Nbdev.jl for a spin then follow the getting started tutorial [here])(https://sapal6.github.io/Nbdev.jl/tutorial/).\nIf you want to take part in the twitter discussion then hope onto this thread –>\n\n\nI am happy to announce that I have released the Alpha version of Nbdev.jl (port of @fastdotai 's Nbdev to #julialang ) . Follow the tutorial - https://t.co/BwLlUPw9KQ to start experimenting with Nbdev. A short 🧵 on my thoughts 👇.\n\n— Satyabrata pal (@TheCodingProjec) December 27, 2021"
  },
  {
    "objectID": "posts/Nbdev.jl-And-Literate-programming/Nbdev.jl-And-Literate-programming.html#further-reads",
    "href": "posts/Nbdev.jl-And-Literate-programming/Nbdev.jl-And-Literate-programming.html#further-reads",
    "title": "Nbdev.jl-And-Literate-programming",
    "section": "Further reads",
    "text": "Further reads\n\nhttp://ross.net/funnelweb/tutorial/intro_what.html\nhttps://www.fast.ai/2019/12/02/nbdev/\nhttps://nbdev.fast.ai/tutorial.html"
  },
  {
    "objectID": "posts/How-to-workout-for-longevity/How-to-workout-for-longevity.html",
    "href": "posts/How-to-workout-for-longevity/How-to-workout-for-longevity.html",
    "title": "How To Workout For Longevity",
    "section": "",
    "text": "This article contains my notes to summerize the article “The Eternal Warrior Plan” by Christian Thibaudeau.\nI have been a long time reader of T-nation especially articles by Christian Thibaudeau. This man is so full of knowledge that’s practicle for experts, beginner, pros and all the rest.\nDuring the month of October there was an awsome article named “The Eternal Warrior Plan” wherein Christian shared some very insightful thoughts about how to train for longetivity and in this article I have tried to summerize a few points and I hope that this summary would help you to get a gist of the original article in case you are in a hurry and you want to distill the article down to the essential points."
  },
  {
    "objectID": "posts/How-to-workout-for-longevity/How-to-workout-for-longevity.html#what-the-research-says",
    "href": "posts/How-to-workout-for-longevity/How-to-workout-for-longevity.html#what-the-research-says",
    "title": "How To Workout For Longevity",
    "section": "What the research says?👩‍🔬",
    "text": "What the research says?👩‍🔬\n\nCarb surplus and Caloric surplus will favor muscle growth, but they can reduce longevity. Carb deficit and caloric deficit can increase longevity, but it’ll make it harder to build muscle.\nResistance training favours muscle growth and endurance training slows cellular ageing.\nHigher body mass increases ageing. Lower body mass slows down cellular age."
  },
  {
    "objectID": "posts/How-to-workout-for-longevity/How-to-workout-for-longevity.html#how-to-use-this-knowledge-to-build-muscularity-strength-longevity",
    "href": "posts/How-to-workout-for-longevity/How-to-workout-for-longevity.html#how-to-use-this-knowledge-to-build-muscularity-strength-longevity",
    "title": "How To Workout For Longevity",
    "section": "How to use this knowledge to build MUSCULARITY, STRENGTH, LONGEVITY? 🏋️‍♂️",
    "text": "How to use this knowledge to build MUSCULARITY, STRENGTH, LONGEVITY? 🏋️‍♂️\n\nGet lean and stay lean. Geeting lean makes you look more muscular and a lean body doesn’t put strain on your cardiovascular system becasue your body now has to carry around less mass.\nTrain you cardiovascular system. This is important to carry all the muscles that you would build.\nWork towards increasing your work capacity. Unless you are a pro aiming for Mr. olympia you are better off in increasing your capacity to do more exercises in less amount of time.\nWalk more. Walking is a great, low strain form of cardio and is good for your mind and body.\nGive loaded carries priority. Farmer’s walk, suitcass walk etc. challenges your entire musculature as well as your cardiovascular syste. This also transfers to your real life like carrying a heavy bag of grocery even when you are 60+ years of age.\nDon’t chase the idea of getting “as big as possible”. This mindset won’t serve you well. To quote from the original article.\n\n\nHow many 280-pound, 70-year olds do you see walking around?"
  },
  {
    "objectID": "posts/How-to-workout-for-longevity/How-to-workout-for-longevity.html#conclusion",
    "href": "posts/How-to-workout-for-longevity/How-to-workout-for-longevity.html#conclusion",
    "title": "How To Workout For Longevity",
    "section": "Conclusion",
    "text": "Conclusion\nI have tried to distill some of the bigger points in this article from the original article. However, I strongly recommend you to read the original article as there are many more thing (include a bonus workout template) which I didn’t cover here. However, this article is kind of a quick note for me and you which we can use to design our next workout template or just keep these things in mind if the goal is to keep lifting as long as possible."
  },
  {
    "objectID": "posts/How-to-workout-for-longevity/How-to-workout-for-longevity.html#further-reading",
    "href": "posts/How-to-workout-for-longevity/How-to-workout-for-longevity.html#further-reading",
    "title": "How To Workout For Longevity",
    "section": "Further reading",
    "text": "Further reading\nRead the original article here 👉 The Eternal Warrior Plan"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Satyabrata pal",
    "section": "",
    "text": "Machine learning architect during the week and on weekends I write about Deep learning👨‍🏫, Fitness🏋️‍♂️ and Photography📷."
  },
  {
    "objectID": "series.html",
    "href": "series.html",
    "title": "Satyabrata pal",
    "section": "",
    "text": "This series contains a collection of step-by-step chapters that will help you to build image restoration tool."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Satyabrata pal",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning\n\n\n\n\nUsing Gradio to quickly build a deep learning tool\n\n\n\n\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning\n\n\n\n\nUsing deep learning to restore and correct images\n\n\n\n\n\n\nSep 26, 2022\n\n\n19 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning\n\n\n\n\nHow to use neural networks to analyze and understand data\n\n\n\n\n\n\nFeb 23, 2022\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProgramming\n\n\n\n\nWhy, what and how of Nbdev.jl\n\n\n\n\n\n\nJan 5, 2022\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\n\n\nOct 12, 2021\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFitness\n\n\n\n\nMy quick notes on an article by Christian Thibaudeau\n\n\n\n\n\n\nOct 12, 2021\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "image-restoration-series.html",
    "href": "image-restoration-series.html",
    "title": "Series: Image restoration",
    "section": "",
    "text": "Using Gradio to quickly build a deep learning tool\n\n\n\n\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nUsing deep learning to restore and correct images\n\n\n\n\n\n\nSep 26, 2022\n\n\n19 min\n\n\n\n\n\n\nNo matching items"
  }
]