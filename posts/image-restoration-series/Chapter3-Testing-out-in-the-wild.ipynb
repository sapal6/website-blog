{
 "cells": [
  {
   "cell_type": "raw",
   "id": "7d5d0732",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Chapter 3 - Testing out in the wild\"\n",
    "description: \"Why should we test our model with random real-world data\"\n",
    "author: \"Satyabrata pal\"\n",
    "date: \"\"\n",
    "categories: ['Deep Learning']\n",
    "format: \n",
    "  html:\n",
    "    code-fold: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccc91bc",
   "metadata": {},
   "source": [
    "## Why to test machine learning model?\n",
    "Why do we need to test our machine learning model? Didn't we test it during the training time using the validation set? Well! we did. Kind of, but that validation set was created by us from the data that we had created. Even when you do test your model on a test set, it might still have the same biases as the validation set and training set. To make sure that your model behaves correctly, we will have to test the trained model on a variety of data taken from the wild."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6bf3a8",
   "metadata": {},
   "source": [
    "## Re-creating the model\n",
    "I will have to re-create the datablock and the model code here as well. Let's do that. We will use the functions from my library [fastaibreadcrumbs](https://sapal6.github.io/fastaibreadcrumbs/) and some fucntions that we used in [chapter 2](satyabratapal.xyz/posts/image-restoration-series/chapter2-mvp.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2448e19c",
   "metadata": {},
   "source": [
    "## Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3840f9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: import gradio as gr\n",
    "except ModuleNotFoundError:\n",
    "    !pip install -Uq gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4eaadcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install fastkaggle if not available\n",
    "try: from fastaibreadcrumbs.core import *\n",
    "except ModuleNotFoundError:\n",
    "    !pip install -Uq fastaibreadcrumbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b52e702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from fastai.vision.all import *\n",
    "from fastai.vision.gan import *\n",
    "from fastkaggle import *\n",
    "from fastaibreadcrumbs.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4364c2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'path_orig':Path('test_860'), 'path_crappy': Path('crappy')}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b792f9c8",
   "metadata": {},
   "source": [
    "## Prepare the test data\n",
    "I will use a picture taken from my mobile phone for this. The reason is that the target users may use a deblurring app to deblurify the pictures they take on their mobile phones. One reason for this assumption is that people would want to deblurify images which are blurred, right and most of the times we take images with our mobile deives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ea67b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['test_path'] = Path('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a4a2db",
   "metadata": {},
   "source": [
    "for your test you can click any picture from your phone and put in the above \"test\" path or replace the \"test\" path in the above code with your path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2b15083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#4) [Path('test/IMG20230128074559.jpg'),Path('test/IMG20230128074749.jpg'),Path('test/IMG20230128074758.jpg'),Path('test/IMG20230128074733.jpg')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img = get_image_files(config['test_path'])\n",
    "test_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcdfa61",
   "metadata": {},
   "source": [
    "## ReCreating the learner and dataloader\n",
    "Just like [chapter2](https://www.satyabratapal.xyz/posts/image-restoration-series/chapter2-mvp.html), we will recreate our datalaoders and learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e268ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dls(sz:int,bs:int, src):\n",
    "    item_tfms = Resize(sz)\n",
    "    batch_tfms = [Normalize.from_stats(*imagenet_stats)]\n",
    "    get_y = lambda x: x.relative_to(config['path_crappy'])\n",
    "    files_crappy = get_image_files(src)\n",
    "    \n",
    "    dls= get_unet_dls(bs, source = files_crappy, get_y = get_y, \n",
    "                     splitter = RandomSplitter(), item_tfms = item_tfms,\n",
    "                     batch_tfms = batch_tfms)\n",
    "    \n",
    "    return dls\n",
    "\n",
    "def get_inf_model(dls, model:str):\n",
    "    unet_learn = unet_learner(dls, models.resnet34, loss_func=F.l1_loss,\n",
    "                     blur=True, norm_type=NormType.Weight).load(model)\n",
    "    \n",
    "    return unet_learn\n",
    "\n",
    "def save_pred(path:str, dest:str,learner):\n",
    "    path = Path(path)\n",
    "    dest = Path(dest)\n",
    "    preds = learner.predict(path)\n",
    "    arr = preds[0].numpy().transpose(1,2,0).astype(np.uint8)\n",
    "    dest.mkdir(parents=True, exist_ok=True)\n",
    "    Image.fromarray(arr).save(dest/path.name)\n",
    "    return dest/path.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3645d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = get_dls(480, 8, config['path_crappy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67d0b9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/satya/anaconda3/envs/py310/lib/python3.10/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and will be removed in 0.15. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n",
      "/home/satya/anaconda3/envs/py310/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/satya/anaconda3/envs/py310/lib/python3.10/site-packages/fastai/learner.py:59: UserWarning: Saved filed doesn't contain an optimizer state.\n",
      "  elif with_opt: warn(\"Saved filed doesn't contain an optimizer state.\")\n"
     ]
    }
   ],
   "source": [
    "#| output: false\n",
    "learner = get_inf_model(dls, './model_256')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b30356",
   "metadata": {},
   "source": [
    "Like last time let's also create the fucntion to save our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4454a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pred(path:str, dest:str,learner):\n",
    "    path = Path(path)\n",
    "    dest = Path(dest)\n",
    "    preds = learner.predict(path)\n",
    "    arr = preds[0].numpy().transpose(1,2,0).astype(np.uint8)\n",
    "    dest.mkdir(parents=True, exist_ok=True)\n",
    "    Image.fromarray(arr).save(dest/path.name)\n",
    "    return dest/path.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "051e405f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_result(path):\n",
    "    dest = save_pred(path,\"gen_imgs\",learner)\n",
    "    return dest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc6b4b9",
   "metadata": {},
   "source": [
    "## Creating the gradio UI\n",
    "We will create a gradio Ui to visually test our results. This is the same UI as chapter 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ad4158e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"900\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<gradio.routes.App at 0x7f524c236740>, 'http://127.0.0.1:7860/', None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| output: false\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        image_input = gr.Image(type=\"filepath\")\n",
    "        image_output = gr.Image()\n",
    "    deblur_btn = gr.Button(\"Deblurrify\")\n",
    "    deblur_btn.click(fn=display_result,\n",
    "                     inputs=image_input, outputs=image_output)\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c77dcda",
   "metadata": {},
   "source": [
    "![](ui.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f861108e",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "## Observations\n",
    "* The blur is not removed.\n",
    "* The noise in the image is also amplified.\n",
    "\n",
    "## Reasons for the bad performance\n",
    "This is a typical example of model performing well on training data but not performing good on real world data. \n",
    "\n",
    "* I trained the model on data where the motion blur was simulated only in a single direction but in real world camera shake/motion blur in an image can be in any direction (horizontal, vertical etc.). So while training the model, this thing should be kept in mind. \n",
    "\n",
    "* Secondly, my training data only contained high quality images and thus there was no sample which would tell the model what todo in case noise is available in the image (this is something that I can look into in later iterations.).\n",
    "\n",
    "* The model that I am using is trained on 256*256 px images. I think I should train it on a bit higher size images as well. \n",
    "\n",
    "## Next steps\n",
    "I will go back to my training and then train it with these corrections in training data and I will take a look into some new architectures which can predict the noise and motion blurs better then the present architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ff8e77",
   "metadata": {},
   "source": [
    "## Reference\n",
    "* Chapter 3 code can be found here.\n",
    "* I have created a [dataset](https://www.kaggle.com/datasets/sapal6/superresolution) of high resolution images that I collected from the free stock photography website [pexels.com](https://www.pexels.com/). During the training cycle of my model I combined my data with another [dataset](https://www.kaggle.com/datasets/thaihoa1476050/df2k-ost) which also had some more high resolution images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed663ca9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "69d38cecdba947ab064d7c73aceac6482aa3d921b1a7186092828f96164a872a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
